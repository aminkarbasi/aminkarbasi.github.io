<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>IID Group</title>
    <link>/</link>
    <description>Recent content on IID Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 03 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Data-driven mapping between functional connectomes using optimal transport</title>
      <link>/publications/2021/javid-2021/</link>
      <pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/javid-2021/</guid>
      <description>Abstract Functional connectomes derived from functional magnetic resonance imaging have long been used to understand the functional organization of the brain. Nevertheless, a connectome is intrinsically linked to the atlas used to create it. In other words, a connectome generated from one atlas is different in scale and resolution compared to a connectome generated from another atlas. Being able to map connectomes and derived results between different atlases without additional pre-processing is a crucial step in improving interpretation and generalization between studies that use different atlases.</description>
    </item>
    
    <item>
      <title>Batched Neural Bandits</title>
      <link>/publications/2021/gu-2021/</link>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/gu-2021/</guid>
      <description>Abstract In many sequential decision-making problems, the individuals are split into several batches and the decision-maker is only allowed to change her policy at the end of batches. These batch problems have a large number of applications, ranging from clinical trials to crowdsourcing. Motivated by this, we study the stochastic contextual bandit problem for general reward distributions under the batched setting. We propose the BatchNeuralUCB algorithm which combines neural networks with optimism to address the exploration-exploitation tradeoff while keeping the total number of batches limited.</description>
    </item>
    
    <item>
      <title>Safe Learning under Uncertain Objectives and Constraints</title>
      <link>/publications/2020/karbasi-2020/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/karbasi-2020/</guid>
      <description>Abstract In this paper, we consider non-convex optimization problems under\textit {unknown} yet safety-critical constraints. Such problems naturally arise in a variety of domains including robotics, manufacturing, and medical procedures, where it is infeasible to know or identify all the constraints. Therefore, the parameter space should be explored in a conservative way to ensure that none of the constraints are violated during the optimization process once we start from a safe initialization point.</description>
    </item>
    
    <item>
      <title>Streaming Submodular Maximization under a k-Set System Constraint. </title>
      <link>/info/info/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/info/info/</guid>
      <description>Welcome to the I.I.D. group, led by Amin Karbasi. The research in our group is at the intersection of learning theory[1,2], optimization[3,4], and information processing[5,6]. We develop data-driven algorithms, with strong theoretical guarantees, that can automatically acquire and reason about highly uncertain information. For more information, please visit our research directions.</description>
    </item>
    
    <item>
      <title>Trustworthy ML &amp; Robust Statistics</title>
      <link>/research/trust.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/research/trust.md/</guid>
      <description>As machine learning systems are pervasively deployed in many scientific fields with increasingly sensitive tasks, it has become paramount to develop algorithms that are robust against the numerous sources of uncertainty inherent in those applications including noise in the data, malicious exploitation of vulnerabilities, outliers, variability of the true objective, privacy, and fairness. While current research in machine learning has led to fundamental breakthroughs, there is still a large gap between the theory and the limitations of the existing algorithms used by practitioners in the real world.</description>
    </item>
    
    <item>
      <title>Batched Multi-Armed Bandits with Optimal Regret</title>
      <link>/publications/2019/esfandiari-2019a/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/esfandiari-2019a/</guid>
      <description>Abstract In this paper, we propose three online algorithms for submodular maximization. The first one, Mono-Frank-Wolfe, reduces the number of per-function gradient evaluations from $T^{1/2}$ [Chen2018Online] and $T^{3/2}$ [chen2018projection] to 1, and achieves a (1‚àí1/e)-regret bound of $O(T^{4/5})$. The second one, Bandit-Frank-Wolfe, is the first bandit algorithm for continuous DR-submodular maximization, which achieves a (1‚àí1/e)-regret bound of $O(T^{8/9})$. Finally, we extend Bandit-Frank-Wolfe to a bandit algorithm for discrete submodular maximization, Responsive-Frank-Wolfe, which attains a (1‚àí1/e)-regret bound of $O(T^{8/9})$ in the responsive bandit setting.</description>
    </item>
    
    <item>
      <title>Minimax Regret of Switching-Constrained Online Convex Optimization: No Phase Transition</title>
      <link>/publications/2019/lin-2019d/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/lin-2019d/</guid>
      <description>Abstract We study the problem of switching-constrained online convex optimization (OCO), where the player has a limited number of opportunities to change her action. While the discrete analog of this online learning task has been studied extensively, previous work in the continuous setting has neither established the minimax rate nor algorithmically achieved it. We here show that T-round switching-constrained OCO with fewer than K switches has a minimax regret of $Œò(\frac{T}{\sqrt{K}})$.</description>
    </item>
    
    <item>
      <title>Eliminating Latent Discrimination: Train Then Mask.</title>
      <link>/publications/2018/ghili-18a/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/ghili-18a/</guid>
      <description>Abstract How can we control for latent discrimination in predictive models? How can we provably remove it? Such questions are at the heart of algorithmic fairness and its impacts on society. In this paper, we define a new operational fairness criteria, inspired by the well-understood notion of omitted variable-bias in statistics and econometrics. Our notion of fairness effectively controls for sensitive features and provides diagnostics for deviations from fair decision making.</description>
    </item>
    
    <item>
      <title>Deletion-Robust Submodular Maximization at Scale.</title>
      <link>/publications/2017/kazemi-17a/</link>
      <pubDate>Tue, 03 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/kazemi-17a/</guid>
      <description>Abstract Can we efficiently extract useful information from a large user-generated dataset while protecting the privacy of the users and/or ensuring fairness in representation. We cast this problem as an instance of a deletion-robust submodular maximization where part of the data may be deleted due to privacy concerns or fairness criteria. We propose the first memory-efficient centralized, streaming, and distributed methods with constant-factor approximation guarantees against any number of adversarial deletions.</description>
    </item>
    
    <item>
      <title>Near-Optimal Active Learning of Halfspaces via Query Synthesis in the Noisy Setting</title>
      <link>/publications/2016/lin-16d/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/lin-16d/</guid>
      <description>Abstract In this paper, we consider the problem of actively learning a linear classifier through query synthesis where the learner can construct artificial queries in order to estimate the true decision boundaries. This problem has recently gained a lot of interest in automated science and adversarial reverse engineering for which only heuristic algorithms are known. In such applications, queries can be constructed de novo to elicit information (e.g., automated science) or to evade detection with minimal cost (e.</description>
    </item>
    
    <item>
      <title>Lazier Than Lazy Greedy</title>
      <link>/icml/publications/baharan-15a/</link>
      <pubDate>Sat, 03 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/baharan-15a/</guid>
      <description>Abstract Is it possible to maximize a monotone submodular function faster than the widely used lazy greedy algorithm (also known as accelerated greedy), both in theory and practice? In this paper, we develop the first linear-time algorithm for maximizing a general monotone submodular function subject to a cardinality constraint. We show that our randomized algorithm, STOCHASTIC-GREEDY, can achieve a (1 ‚àí 1/e ‚àí Œµ) approximation guarantee, in expectation, to the optimum solution in time linear in the size of the data and independent of the cardinality constraint.</description>
    </item>
    
    <item>
      <title>Adaptive Content Search Through Comparisons</title>
      <link>/publications/2011/karbasi-11c/</link>
      <pubDate>Mon, 03 Jan 2011 00:00:00 +0000</pubDate>
      
      <guid>/publications/2011/karbasi-11c/</guid>
      <description>Abstract The problem of content search through comparisons has recently received considerable attention. In short, a user searching for a target object navigates through a database in the following manner: the user is asked to select the object most similar to her target from a small list of objects. A new object list is then presented to the user based on her earlier selection. This process is repeated until the target is included in the list presented, at which point the search terminates.</description>
    </item>
    
    <item>
      <title>An Estimation Theoretic Approach for Sparsity Pattern Recovery in the Noisy Setting</title>
      <link>/publications/2009/hormati-09a/</link>
      <pubDate>Sat, 03 Jan 2009 00:00:00 +0000</pubDate>
      
      <guid>/publications/2009/hormati-09a/</guid>
      <description>Abstract Compressed sensing deals with the reconstruction of sparse signals using a small number of linear measurements. One of the main challenges in compressed sensing is to find the support of a sparse signal. In the literature, several bounds on the scaling law of the number of measurements for successful support recovery have been derived where the main focus is on random Gaussian measurement matrices. In this paper, we investigate the noisy support recovery problem from an estimation theoretic point of view, where no specific assumption is made on the underlying measurement matrix.</description>
    </item>
    
    <item>
      <title>Black-Box Generalization</title>
      <link>/publications/2022/neurips-4/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/neurips-4/</guid>
      <description>Abstract We provide the first generalization error analysis for black-box learning through derivative-free optimization. Under the assumption of a Lipschitz and smooth unknown loss, we consider the Zeroth-order Stochastic Search (ZoSS) algorithm, that updates a d-dimensional model by replacing stochastic gradient directions with stochastic differences of K+1 perturbed loss evaluations per dataset (example) query. For both unbounded and bounded possibly nonconvex losses, we present the first generalization bounds for the ZoSS algorithm.</description>
    </item>
    
    <item>
      <title>Fast Neural Kernel Embeddings for General Activations</title>
      <link>/publications/2022/neurips-6/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/neurips-6/</guid>
      <description>Abstract Infinite width limit has shed light on generalization and optimization aspects of deep learning by establishing connections between neural networks and kernel methods. Despite their importance, the utility of these kernel methods was limited in large-scale learning settings due to their (super-)quadratic runtime and memory complexities. Moreover, most prior works on neural kernels have focused on the ReLU activation, mainly due to its popularity but also due to the difficulty of computing such kernels for general activations.</description>
    </item>
    
    <item>
      <title>Multiclass Learnability Beyond the PAC Framework: Universal Rates and Partial Concept Classes </title>
      <link>/publications/2022/neurips-3/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/neurips-3/</guid>
      <description>Abstract </description>
    </item>
    
    <item>
      <title>On Optimal Learning Under Targeted Data Poisoning </title>
      <link>/publications/2022/neurips-5/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/neurips-5/</guid>
      <description>Abstract </description>
    </item>
    
    <item>
      <title>Submodular Maximization in Clean Linear Time</title>
      <link>/publications/2022/li-2022/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/li-2022/</guid>
      <description>Abstract In this paper, we provide the first deterministic algorithm that achieves the tight 1‚àí1/e approximation guarantee for submodular maximization under a cardinality (size) constraint while making a number of queries that scales only linearly with the size of the ground set n. To complement our result, we also show strong information-theoretic lower bounds. More specifically, we show that when the maximum cardinality allowed for a solution is constant, no algorithm making a sub-linear number of function evaluations can guarantee any constant approximation ratio.</description>
    </item>
    
    <item>
      <title>The Best of Both Worlds: Reinforcement Learning with Logarithmic Regret and Policy Switches</title>
      <link>/publications/2022/icml2-2022/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/icml2-2022/</guid>
      <description>Abstract In this paper, we study the problem of regret minimization for episodic Reinforcement Learning (RL) both in the model-free and the model-based setting. We focus on learning with general function classes and general model classes, and we derive results that scale with the eluder dimension of these classes. In contrast to the existing body of work that mainly establishes instance-independent regret guarantees, we focus on the instance-dependent setting and show that the regret scales logarithmically with the horizon T, provided that there is a gap between the best and the second best action in every state.</description>
    </item>
    
    <item>
      <title>Universal Rates for Interactive Learning</title>
      <link>/publications/2022/neurips-7/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/neurips-7/</guid>
      <description>Abstract </description>
    </item>
    
    <item>
      <title>Self-Consistency of the Fokker-Planck Equation</title>
      <link>/publications/2022/karbasi-2022b/</link>
      <pubDate>Fri, 10 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/karbasi-2022b/</guid>
      <description>Abstract The Fokker-Planck equation (FPE) is the partial differential equation that governs the density evolution of the It√¥ process and is of great importance to the literature of statistical physics and machine learning. The FPE can be regarded as a continuity equation where the change of the density is completely determined by a time varying velocity field. Importantly, this velocity field also depends on the current density function. As a result, the ground-truth velocity field can be shown to be the solution of a fixed-point equation, a property that we call self-consistency.</description>
    </item>
    
    <item>
      <title>Scalable MCMC Sampling for Nonsymmetric Determinantal Point Processes</title>
      <link>/publications/2022/icml3-2022/</link>
      <pubDate>Thu, 09 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/icml3-2022/</guid>
      <description>Abstract A determinantal point process (DPP) is an elegant model that assigns a probability to every subset of a collection of items. While conventionally a DPP is parameterized by a symmetric kernel matrix, removing this symmetry constraint, resulting in nonsymmetric DPPs (NDPPs), leads to significant improvements in modeling power and predictive performance. Recent work has studied an approximate Markov chain Monte Carlo (MCMC) sampling algorithm for NDPPs restricted to size- subsets (called -NDPPs).</description>
    </item>
    
    <item>
      <title>Combining multiple atlases to estimate data-driven mappings between functional connectomes using optimal transport</title>
      <link>/publications/2022/miccai-2022/</link>
      <pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/miccai-2022/</guid>
      <description>Abstract Connectomics is a popular approach for understanding the brain with neuroimaging data. Yet, a connectome generated from one atlas is different in size, topology, and scale compared to a connectome generated from another atlas. These differences hinder interpreting, generalizing, and combining connectomes and downstream results from different atlases. Recently, it was proposed that a mapping between atlases can be estimated such that connectomes from one atlas (\textit{i.e.}, source atlas) can be reconstructed into a connectome from a different atlas (\textit{i.</description>
    </item>
    
    <item>
      <title>Learning Distributionally Robust Models at Scale via Composite Optimization</title>
      <link>/publications/2022/iclar-2022a/</link>
      <pubDate>Mon, 17 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/iclar-2022a/</guid>
      <description>Abstract To train machine learning models that are robust to distribution shifts in the data, distributionally robust optimization (DRO) has been proven very effective. However, the existing approaches to learning a distributionally robust model either require solving complex optimization problems such as semidefinite programming or a first-order method whose convergence scales linearly with the number of data samples‚Äì which hinders their scalability to large datasets. In this paper, we show how different variants of DRO are simply instances of a finite-sum composite optimization for which we provide scalable methods.</description>
    </item>
    
    <item>
      <title>Scalable Sampling for Nonsymmetric Determinantal Point Processes</title>
      <link>/publications/2022/iclar-2022b/</link>
      <pubDate>Mon, 17 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/iclar-2022b/</guid>
      <description>Abstract A determinantal point process (DPP) on a collection of M items is a model, parameterized by a symmetric kernel matrix, that assigns a probability to every subset of those items. Recent work shows that removing the kernel symmetry constraint, yielding nonsymmetric DPPs (NDPPs), can lead to significant predictive performance gains for machine learning applications. However, existing work leaves open the question of scalable NDPP sampling. There is only one known DPP sampling algorithm, based on Cholesky decomposition, that can directly apply to NDPPs as well.</description>
    </item>
    
    <item>
      <title>Federated Functional Gradient Boosting</title>
      <link>/publications/2022/karbasi-2022a/</link>
      <pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/karbasi-2022a/</guid>
      <description>Abstract In this paper, we initiate a study of functional minimization in Federated Learning. First, in the semi-heterogeneous setting, when the marginal distributions of the feature vectors on client machines are identical, we develop the federated functional gradient boosting (FFGB) method that provably converges to the global minimum. Subsequently, we extend our results to the fully-heterogeneous setting (where marginal distributions of feature vectors may differ) by designing an efficient variant of FFGB called FFGB.</description>
    </item>
    
    <item>
      <title>An Exponential Improvement on the Memorization Capacity of Deep Threshold Networks</title>
      <link>/publications/2021/neurips3/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/neurips3/</guid>
      <description>Abstract It is well known that modern deep neural networks are powerful enough to memorize datasets even when the labels have been randomized. Recently, Vershynin (2020) settled a long standing question by Baum (1988), proving that \emph{deep threshold} networks can memorize n points in d dimensions using ÓàªÀú(e1/Œ¥2+n‚Äæ‚àö) neurons and ÓàªÀú(e1/Œ¥2(d+n‚Äæ‚àö)+n) weights, where Œ¥ is the minimum distance between the points. In this work, we improve the dependence on Œ¥ from exponential to almost linear, proving that ÓàªÀú(1Œ¥+n‚Äæ‚àö) neurons and ÓàªÀú(dŒ¥+n) weights are sufficient.</description>
    </item>
    
    <item>
      <title>Multiple Descent: Design Your Own Generalization Curve</title>
      <link>/publications/2021/neurips2/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/neurips2/</guid>
      <description>Abstract This paper explores the generalization loss of linear regression in variably parameter- ized families of models, both under-parameterized and over-parameterized. We show that the generalization curve can have an arbitrary number of peaks, and moreover, locations of those peaks can be explicitly controlled. Our results highlight the fact that both classical U-shaped generalization curve and the recently observed double descent curve are not intrinsic properties of the model family. Instead, their emergence is due to the interaction between the properties of the data and the inductive biases of learning algorithms.</description>
    </item>
    
    <item>
      <title>Parallelizing Thompson Sampling</title>
      <link>/publications/2021/neurips4/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/neurips4/</guid>
      <description>Abstract How can we make use of information parallelism in online decision making problems while efficiently balancing the exploration-exploitation trade-off? In this paper, we introduce a batch Thompson Sampling framework for two canonical online decision making problems, namely, stochastic multi-arm bandit and linear contextual bandit with finitely many arms. Over a time horizon T , our batch Thompson Sampling policy achieves the same (asymptotic) regret bound of a fully sequential one while carrying out only O(log T ) batch queries.</description>
    </item>
    
    <item>
      <title>Submodular &#43; Concave</title>
      <link>/publications/2021/neurips1/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/neurips1/</guid>
      <description>Abstract It has been well established that first order optimization methods can converge to the maximal objective value of concave functions and provide constant factor approximation guarantees for (non-convex/non-concave) continuous submodular functions. In this work, we initiate the study of the maximization of functions of the form F (x) = G(x) + C (x) over a solvable convex body P, where G is a smooth DR-submodular function and C is a smooth concave function.</description>
    </item>
    
    <item>
      <title>Regularized Submodular Maximization at Scale</title>
      <link>/publications/2021/karbasi-2020j/</link>
      <pubDate>Mon, 03 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/karbasi-2020j/</guid>
      <description>Abstract In this paper, we propose scalable methods for maximizing a regularized submodular function f(¬∑) = g(¬∑) ‚àí l(¬∑) expressed as the difference between a monotone submodular function g and a modular function l. Indeed, submodularity is inherently related to the notions of diversity, coverage, and representativeness. In particular, finding the mode (i.e., the most likely configuration) of many popular probabilistic models of diversity, such as determinantal point processes, submodular probabilistic models, and strongly log-concave distributions, involves maximization of (regularized) submodular functions.</description>
    </item>
    
    <item>
      <title>Adaptivity in Adaptive Submodularity</title>
      <link>/publications/2021/esfandiari-2019b/</link>
      <pubDate>Sat, 03 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/esfandiari-2019b/</guid>
      <description>Abstract Adaptive sequential decision making is one of the central challenges in machine learning and artificial intelligence. In such problems, the goal is to design an interactive policy that plans for an action to take, from a finite set of n actions, given some partial observations. It has been shown that in many applications such as active learning, robotics, sequential experimental design, and active detection, the utility function satisfies adaptive submodularity, a notion that generalizes the notion of diminishing returns to policies.</description>
    </item>
    
    <item>
      <title>Learning and Certification under Instance-targeted Poisoning</title>
      <link>/publications/2021/gao-21/</link>
      <pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/gao-21/</guid>
      <description>Abstract In this paper, we study PAC learnability and certification under instance-targeted poisoning attacks, where the adversary may change a fraction of the training set with the goal of fooling the learner at a specific target instance. Our first contribution is to formalize the problem in various settings, and explicitly discussing subtle aspects such as learner&amp;rsquo;s randomness and whether (or not) adversary&amp;rsquo;s attack can depend on it. We show that when the budget of the adversary scales sublinearly with the sample complexity, PAC learnability and certification are achievable.</description>
    </item>
    
    <item>
      <title>The curious case of adversarially robust models: More data can help, double descend, or hurt generalization</title>
      <link>/publications/2021/karbasi-2020g/</link>
      <pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/karbasi-2020g/</guid>
      <description>Abstract Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classification problems.</description>
    </item>
    
    <item>
      <title>Meta Learning in the Continuous Time Limit</title>
      <link>/publications/2021/karbasi-2020c/</link>
      <pubDate>Wed, 03 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/karbasi-2020c/</guid>
      <description>Abstract In this paper, we establish the ordinary differential equation (ode) that underlies the train- ing dynamics of Model-Agnostic Meta-Learning (maml). Our continuous-time limit view of the process eliminates the influence of the manually chosen step size of gradient descent and includes the existing gradient descent training algorithm as a special case that results from a specific discretization. We show that the maml ode enjoys a linear convergence rate to an approximate stationary point of the maml loss function for strongly convex task losses, even when the corresponding maml loss is non-convex.</description>
    </item>
    
    <item>
      <title>Regret Bounds for Batched Bandits</title>
      <link>/publications/2021/esfandiari-2021/</link>
      <pubDate>Wed, 03 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/esfandiari-2021/</guid>
      <description>Abstract We present simple and efficient algorithms for the batched stochastic multi-armed bandit and batched stochastic linear bandit problems. We prove bounds for their expected regrets that improve over the best known regret bounds for any number of batches. In particular, our algorithms in both settings achieve the optimal expected regrets by using only a logarithmic number of batches. We also study the batched adversarial multi-armed bandit problem for the first time and find the optimal regret, up to logarithmic factors, of any algorithm with predetermined batch sizes.</description>
    </item>
    
    <item>
      <title>Continuous Submodular Maximization: Beyond DR-Submodularity</title>
      <link>/publications/2020/karbasi-2020b/</link>
      <pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/karbasi-2020b/</guid>
      <description>Abstract n this paper, we propose the first continuous optimization algorithms that achieve a constant factor approximation guarantee for the problem of monotone continuous submodular maximization subject to a linear constraint. We first prove that a simple variant of the vanilla coordinate ascent, called Coordinate-Ascent+, achieves $\frac{e-1}{2e-1}-\epsilon$ a -approximation guarantee while performing $O(n/\epsilon)$ iterations, where the computational complexity of each iteration is roughly $O(n/\sqrt(\epsilon)+n\log n)$(here $n$, denotes the dimension of the optimization problem).</description>
    </item>
    
    <item>
      <title>Minimax Regret of Switching-Constrained Online Convex Optimization: No Phase Transition</title>
      <link>/publications/2020/lin-2019d/</link>
      <pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/lin-2019d/</guid>
      <description>Abstract We study the problem of switching-constrained online convex optimization (OCO), where the player has a limited number of opportunities to change her action. While the discrete analog of this online learning task has been studied extensively, previous work in the continuous setting has neither established the minimax rate nor algorithmically achieved it. We here show that T-round switching-constrained OCO with fewer than K switches has a minimax regret of $Œò(\frac{T}{\sqrt{K}})$.</description>
    </item>
    
    <item>
      <title>Online MAP Inference of Determinantal Point Processes</title>
      <link>/publications/2020/bhaskara-2020/</link>
      <pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/bhaskara-2020/</guid>
      <description>Abstract In this paper, we provide an efficient approximation algorithm for finding the most likelihood configuration (MAP) of size k for Determinantal Point Processes (DPP) in the online setting where the data points arrive in an arbitrary order and the algorithm cannot discard the selected elements from its local memory. Given a tolerance additive error $\eta$, our onlinealgorithm achieves a $k^{O(k)}$ multiplicative approximation guarantee with an additive error $\eta$, using a memory footprint independent of the size of the data stream.</description>
    </item>
    
    <item>
      <title>Submodular Maximization Through Barrier Functions</title>
      <link>/publications/2020/karbasi-2020i/</link>
      <pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/karbasi-2020i/</guid>
      <description>Abstract In this paper, we introduce a novel technique for constrained submodular maximization, inspired by barrier functions in continuous optimization. This connection not only improves the running time for constrained submodular maximization but also provides the state of the art guarantee. More precisely, for maximizing a monotone submodular function subject to the combination of a k-matchoid and l-knapsack constraint (for l ‚â§ k), we propose a potential function that can be approximately minimized.</description>
    </item>
    
    <item>
      <title>More data can expand the generalization gap between adversarially robust and standard models</title>
      <link>/publications/2020/karbasi-2020h/</link>
      <pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/karbasi-2020h/</guid>
      <description>Abstract Despite remarkable success in practice, modern machine learning models have been found to be susceptible to adversarial attacks that make human-imperceptible perturbations to the data, but result in serious and potentially dangerous prediction errors. To address this issue, practitioners often use adversarial training to learn models that are robust against such attacks at the cost of higher generalization error on unperturbed test sets. The conventional wisdom is that more training data should shrink the gap between the generalization error of adversarially-trained models and standard models.</description>
    </item>
    
    <item>
      <title>Streaming Submodular Maximization under a k-Set System Constraint. </title>
      <link>/publications/2020/haba-2020/</link>
      <pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/haba-2020/</guid>
      <description>Abstract In this paper, we propose a novel framework that converts streaming algorithms for monotone submodular maximization into streaming algorithms for non-monotone submodular maximization. This reduction readily leads to the currently tightest deterministic approximation ratio for submodular maximization subject to a k-matchoid constraint. Moreover, we propose the first streaming algorithm for monotone submodular maximization subject to k-extendible and k-set system constraints. Together with our proposed reduction, we obtain O(klogk) and O(k2logk) approximation ratio for submodular maximization subject to the above constraints, respectively.</description>
    </item>
    
    <item>
      <title>Black Box Submodular Maximization: Discrete and Continuous Settings</title>
      <link>/publications/2020/lin-2020a/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/lin-2020a/</guid>
      <description>Abstract In this paper, we consider the problem of black box continuous submodular maximization where we only have access to the function values and no information about the derivatives is provided. For a monotone and continuous DR-submodular function, and subject to a bounded convex body constraint, we propose Black-box Continuous Greedy, a derivative-free algorithm that provably achieves the tight [(1‚àí1/e)OPT‚àíœµ] approximation guarantee with $O(d/œµ^3)$ function evaluations. We then extend our result to the stochastic setting where function values are subject to stochastic zero-mean noise.</description>
    </item>
    
    <item>
      <title>One Sample Stochastic Frank-Wolfe</title>
      <link>/publications/2020/mingrui-2020c/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/mingrui-2020c/</guid>
      <description>Abstract One of the beauties of the projected gradient descent method lies in its rather simple mechanism and yet stable behavior with inexact, stochastic gradients, which has led to its wide-spread use in many machine learning applications. However, once we replace the projection operator with a simpler linear program, as is done in the Frank-Wolfe method, both simplicity and stability take a serious hit. The aim of this paper is to bring them back without sacrificing the efficiency.</description>
    </item>
    
    <item>
      <title>Quantized frank-wolfe: Faster optimization, lower communication, and projection free</title>
      <link>/publications/2020/karbasi-2020f/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/karbasi-2020f/</guid>
      <description>Abstract How can we efficiently mitigate the overhead of gradient communications in distributed optimization? This problem is at the heart of training scalable machine learning models and has been mainly studied in the unconstrained setting. In this paper, we propose Quantised Frank-Wolfe (QFW), the first projection free and communication-efficient algorithm for solving constrained optimization problems at scale. We consider both convex and non-convex objective functions, expressed as a finite-sum or more generally a stochastic optimization problem, and provide strong theoretical guarantees on the convergence rate of QFW.</description>
    </item>
    
    <item>
      <title>Adaptive Sequence Submodularity</title>
      <link>/publications/2019/marko-2019a/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/marko-2019a/</guid>
      <description>Abstract In many machine learning applications, one needs to interactively select a sequence of items (e.g., recommending movies based on a user&amp;rsquo;s feedback) or make sequential decisions in a certain order (e.g., guiding an agent through a series of states). Not only do sequences already pose a dauntingly large search space, but we must also take into account past observations, as well as the uncertainty of future outcomes. Without further structure, finding an optimal sequence is notoriously challenging, if not completely intractable.</description>
    </item>
    
    <item>
      <title>Online Continuous Submodular Maximization: From Full-Information to Bandit Feedback</title>
      <link>/publications/2019/minguri-2019a/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/minguri-2019a/</guid>
      <description>Abstract In this paper, we propose three online algorithms for submodular maximization. The first one, Mono-Frank-Wolfe, reduces the number of per-function gradient evaluations from $T^{1/2}$ [Chen2018Online] and $T^{3/2}$ [chen2018projection] to 1, and achieves a (1‚àí1/e)-regret bound of $O(T^{4/5})$. The second one, Bandit-Frank-Wolfe, is the first bandit algorithm for continuous DR-submodular maximization, which achieves a (1‚àí1/e)-regret bound of $O(T^{8/9})$. Finally, we extend Bandit-Frank-Wolfe to a bandit algorithm for discrete submodular maximization, Responsive-Frank-Wolfe, which attains a (1‚àí1/e)-regret bound of $O(T^{8/9})$ in the responsive bandit setting.</description>
    </item>
    
    <item>
      <title>Stochastic Continuous Greedy &#43;&#43;: When Upper and Lower Bounds Match</title>
      <link>/publications/2019/karbasi-2019a/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/karbasi-2019a/</guid>
      <description>Abstract In this paper, we develop $\scg~(\text{SCG}{++})$, the first efficient variant of a conditional gradient method for maximizing a continuous submodular function subject to a convex constraint. Concretely, for a monotone and continuous DR-submodular function, \SCGPP achieves a tight [(1‚àí1/e)\OPT‚àíœµ] solution while using $O(1/œµ^2)$ stochastic gradients and O(1/œµ) calls to the linear optimization oracle. The best previously known algorithms either achieve a suboptimal $[(1/2)\OPT‚àíœµ]$ solution with $O(1/œµ^2)$ stochastic gradients or the tight $[(1‚àí1/e)\OPT‚àíœµ]$ solution with suboptimal $O(1/œµ^3)$ stochastic gradients.</description>
    </item>
    
    <item>
      <title>Submodular Maximization beyond Non-negativity: Guarantees, Fast Algorithms, and Applications</title>
      <link>/publications/2019/harshaw-2019a/</link>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/harshaw-2019a/</guid>
      <description>Abstract It is generally believed that submodular functions‚Äìand the more general class of Œ≥ -weakly submodular functions‚Äìmay only be optimized under the non-negativity assumption f(S)‚â•0 . In this paper, we show that once the function is expressed as the difference f=g‚àíc , where g is monotone, non-negative, and Œ≥ -weakly submodular and c is non-negative modular, then strong approximation guarantees may be obtained. We present an algorithm for maximizing g‚àíc under a k -cardinality constraint which produces a random feasible set S such that $ùîº[g(S)‚àíc(S)]\geq (1‚àíe‚àíŒ≥‚àí\epsilon)g(OPT)‚àíc(OPT)$ , whose running time is $O(\frac{n}{\epsilon}log2\frac{1}{\epsilon})$ , independent of k .</description>
    </item>
    
    <item>
      <title>Submodular Streaming in All Its Glory: Tight Approximation, Minimum Memory and Low Adaptive Complexity. </title>
      <link>/publications/2019/kazemi-2019a/</link>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/kazemi-2019a/</guid>
      <description>Abstract Streaming algorithms are generally judged by the quality of their solution, memory footprint, and computational complexity. In this paper, we study the problem of maximizing a monotone submodular function in the streaming setting with a cardinality constraint k . We first propose SIEVE-STREAMING++, which requires just one pass over the data, keeps only O(k) elements and achieves the tight $\frac{1}{2}$ -approximation guarantee. The best previously known streaming algorithms either achieve a suboptimal $\frac{1}{4}$ -approximation with Œò(k) memory or the optimal 12 -approximation with O(klogk) memory.</description>
    </item>
    
    <item>
      <title>Projection-Free Bandit Convex Optimization</title>
      <link>/publications/2019/lin-2019a/</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/lin-2019a/</guid>
      <description>Abstract In this paper, we propose the first computationally efficient projection-free algorithm for bandit convex optimization (BCO) with a general convex constraint. We show that our algorithm achieves a sublinear regret of $O(nT^{4/5})$ (where T is the horizon and n is the dimension) for any bounded convex functions with uniformly bounded gradients. We also evaluate the performance of our algorithm against baselines on both synthetic and real data sets for quadratic programming, portfolio selection and matrix completion problems.</description>
    </item>
    
    <item>
      <title>Eliminating Latent Discrimination: Train Then Mask</title>
      <link>/publications/2019/ghili-2019/</link>
      <pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/ghili-2019/</guid>
      <description>Abstract How can we control for latent discrimination in predictive models? How can we provably remove it? Such questions are at the heart of algorithmic fairness and its impacts on society. In this paper, we define a new operational fairness criteria, inspired by the well-understood notion of omitted variable-bias in statistics and econometrics. Our notion of fairness effectively controls for sensitive features and provides diagnostics for deviations from fair decision making.</description>
    </item>
    
    <item>
      <title>Unconstrained submodular maximization with constant adaptive complexity</title>
      <link>/publications/2019/lin-2019b/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/lin-2019b/</guid>
      <description>Abstract In this paper, we consider the unconstrained submodular maximization problem. We propose the first algorithm for this problem that achieves a tight $(1/2‚àíŒµ)$-approximation guarantee using $O(Œµ^{‚àí1})$ adaptive rounds and a linear number of function evaluations. No previously known algorithm for this problem achieves an approximation ratio better than 1/3 using less than Œ©(n) rounds of adaptivity, where n is the size of the ground set. Moreover, our algorithm easily extends to the maximization of a non-negative continuous DR-submodular function subject to a box constraint, and achieves a tight (1/2‚àíŒµ)-approximation guarantee for this problem while keeping the same adaptive and query complexities.</description>
    </item>
    
    <item>
      <title>Do Less, Get More: Streaming Submodular Maximization with Subsampling</title>
      <link>/publications/2018/feldman-18a/</link>
      <pubDate>Sat, 03 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/feldman-18a/</guid>
      <description>Abstract In this paper, we develop the first one-pass streaming algorithm for submodular maximization that does not evaluate the entire stream even once. By carefully subsampling each element of the data stream, our algorithm enjoys the tightest approximation guarantees in various settings while having the smallest memory footprint and requiring the lowest number of function evaluations. More specifically, for a monotone submodular function and a p-matchoid constraint, our randomized algorithm achieves a 4p approximation ratio (in expectation) with O(k) memory and O(km/p) queries per element (k is the size of the largest feasible solution and m is the number of matroids used to define the constraint).</description>
    </item>
    
    <item>
      <title>Data Summarization at Scale: A Two-Stage Submodular Approach</title>
      <link>/publications/2018/marko-18b/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/marko-18b/</guid>
      <description>Abstract The sheer scale of modern datasets has resulted in a dire need for summarization techniques that can identify representative elements in a dataset. Fortunately, the vast majority of data summariza- tion tasks satisfy an intuitive diminishing returns condition known as submodularity, which allows us to find nearly-optimal solutions in linear time. We focus on a two-stage submodular framework where the goal is to use some given training func- tions to reduce the ground set so that optimizing new functions (drawn from the same distribution) over the reduced set provides almost as much value as optimizing them over the entire ground set.</description>
    </item>
    
    <item>
      <title>Decentralized Submodular Maximization: Bridging Discrete and Continuous Settings.</title>
      <link>/publications/2018/mokhtari-18b/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/mokhtari-18b/</guid>
      <description>Abstract In this paper, we showcase the interplay between discrete and continuous optimization in network-structured settings. We propose the first fully decentralized optimization method for a wide class of non-convex objective functions that possess a diminishing returns property. More specifically, given an arbitrary connected network and a global continuous submodular function, formed by a sum of local functions, we develop Decentralized Continuous Greedy (DCG), a message passing algorithm that converges to the tight (1‚àí1/e) approximation factor of the optimum global solution using only local computation and communication.</description>
    </item>
    
    <item>
      <title>Projection-Free Online Optimization with Stochastic Gradient: From Convexity to Submodularity.</title>
      <link>/publications/2018/lin-18c/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/lin-18c/</guid>
      <description>Abstract Online optimization has been a successful framework for solving large-scale problems under computational constraints and partial information. Current methods for online convex optimization require either a projection or exact gradient computation at each step, both of which can be prohibitively expensive for large-scale applications. At the same time, there is a growing trend of non-convex optimization in machine learning community and a need for online methods. Continuous DR-submodular functions, which exhibit a natural diminishing returns condition, have recently been proposed as a broad class of non-convex functions which may be efficiently optimized.</description>
    </item>
    
    <item>
      <title>Scalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints.</title>
      <link>/publications/2018/kazemi-2018b/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/kazemi-2018b/</guid>
      <description>Abstract Can we efficiently extract useful information from a large user-generated dataset while protecting the privacy of the users and/or ensuring fairness in representation? We cast this problem as an instance of a deletion-robust submodular maximization where part of the data may be deleted or masked due to privacy concerns or fairness criteria. We propose the first memory-efficient centralized, streaming, and distributed methods with constant-factor approximation guarantees against any number of adversarial deletions.</description>
    </item>
    
    <item>
      <title>Weakly Submodular Maximization Beyond Cardinality Constraints: Does Randomization Help Greedy?</title>
      <link>/publications/2018/lin-18b/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/lin-18b/</guid>
      <description>Abstract Submodular functions are a broad class of set functions that naturally arise in many machine learning applications. Due to their combinatorial structures, there has been a myriad of algorithms for maximizing such functions under various constraints. Unfortunately, once a function deviates from submodularity (even slightly), the known algorithms may perform arbitrarily poorly. Amending this issue, by obtaining approximation results for functions obeying properties that generalize submodularity, has been the focus of several recent works.</description>
    </item>
    
    <item>
      <title>Comparison Based Learning from Weak Oracles</title>
      <link>/publications/2018/kazemi-2018a/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/kazemi-2018a/</guid>
      <description>Abstract There is increasing interest in learning algorithms that involve interaction between hu- man and machine. Comparison-based queries are among the most natural ways to get feed- back from humans. A challenge in designing comparison-based interactive learning algorithms is coping with noisy answers. The most common fix is to submit a query several times, but this is not applicable in many situations due to its prohibitive cost and due to the unrealistic assumption of independent noise in different repetitions of the same query.</description>
    </item>
    
    <item>
      <title>Conditional Gradient Method for Stochastic Submodular Maximization: Closing the Gap</title>
      <link>/publications/2018/mokhtari-18a/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/mokhtari-18a/</guid>
      <description>Abstract In this paper, we study the problem of constrained and stochastic continuous submodular maximization. Even though the objective function is not concave (nor convex) and is defined in terms of an expectation, we develop a variant of the conditional gradient method, called Stochastic Continuous Greedy, which achieves a tight approximation guarantee. More precisely, for a monotone and continuous DR-submodular function and subject to a general convex body constraint, we prove that Stochastic Continuous Greedy achieves a $[(1‚àí1/e)OPT‚àí\eps]$ guarantee (in expectation) with $O(1/\eps^3)$ stochastic gradient computations.</description>
    </item>
    
    <item>
      <title>Online Continuous Submodular Maximization</title>
      <link>/publications/2018/lin-18a/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/lin-18a/</guid>
      <description>Abstract In this paper, we consider an online optimization process, where the objective functions are not convex (nor concave) but instead belong to a broad class of continuous submodular functions. We first propose a variant of the Frank-Wolfe algorithm that has access to the full gradient of the objective functions. We show that it achieves a regret bound of $O(\sqrt(T))$ (where T is the horizon of the online optimization problem) against a (1‚àí1/e) -approximation to the best feasible solution in hindsight.</description>
    </item>
    
    <item>
      <title>Submodularity on Hypergraphs: From Sets to Sequences</title>
      <link>/publications/2018/marko-18a/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/marko-18a/</guid>
      <description>Abstract The sheer scale of modern datasets has resulted in a dire need for summarization techniques that can identify representative elements in a dataset. Fortunately, the vast majority of data summarization tasks satisfy an intuitive diminishing returns condition known as submodularity, which allows us to find nearly-optimal solutions in linear time. We focus on a two-stage submodular framework where the goal is to use some given training functions to reduce the ground set so that optimizing new functions (drawn from the same distribution) over the reduced set provides almost as much value as optimizing them over the entire ground set.</description>
    </item>
    
    <item>
      <title>Gradient Methods for Submodular Maximization</title>
      <link>/publications/2017/hassani-17a/</link>
      <pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/hassani-17a/</guid>
      <description>Abstract In this paper, we study the problem of maximizing continuous submodular functions that naturally arise in many learning applications such as those involving utility functions in active learning and sensing, matrix approximations and network inference. Despite the apparent lack of convexity in such functions, we prove that stochastic projected gradient methods can provide strong approximation guarantees for maximizing continuous submodular functions with convex constraints. More specifically, we prove that for monotone continuous DR-submodular functions, all fixed points of projected gradient ascent provide a factor 1/2 approximation to the global maxima.</description>
    </item>
    
    <item>
      <title>Interactive Submodular Bandit.</title>
      <link>/publications/2017/lin-17b/</link>
      <pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/lin-17b/</guid>
      <description>Abstract In many machine learning applications, submodular functions have been used as a model for evaluating the utility or payoff of a set such as news items to recommend, sensors to deploy in a terrain, nodes to influence in a social network, to name a few. At the heart of all these applications is the assumption that the underlying utility/payoff function is known a priori, hence maximizing it is in principle possible.</description>
    </item>
    
    <item>
      <title>Streaming Weak Submodularity: Interpreting Neural Networks on the Fly.</title>
      <link>/publications/2017/elenberg-17a/</link>
      <pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/elenberg-17a/</guid>
      <description>Abstract In many machine learning applications, it is important to explain the predictions of a black-box classifier. For example, why does a deep neural network assign an image to a particular class? We cast interpretability of black-box classifiers as a combinatorial maximization problem and propose an efficient streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of random stream order and a weakly submodular objective function.</description>
    </item>
    
    <item>
      <title>Differentially Private Submodular Maximization: Data Summarization in Disguise.</title>
      <link>/publications/2017/marko-17a/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/marko-17a/</guid>
      <description>Abstract Many data summarization applications are captured by the general framework of submodular maximization. As a consequence, a wide range of efficient approximation algorithms have been developed. However, when such applications involve sensitive data about individuals, their privacy concerns are not automatically addressed. To remedy this problem, we propose a general and systematic study of differentially private submodular maximization. We present privacy-preserving algorithms for both monotone and non-monotone submodular maximization under cardinality, matroid, and p-extendible system constraints, with guarantees that are competitive with optimal.</description>
    </item>
    
    <item>
      <title>Greed Is Good: Near-Optimal Submodular Maximization via Greedy Optimization.</title>
      <link>/publications/2017/baharan-17a/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/baharan-17a/</guid>
      <description>Abstract How can we summarize a dynamic data stream when elements selected for the summary can be deleted at any time? This is an important challenge in online services, where the users generating the data may decide to exercise their right to restrict the service provider from using (part of) their data due to privacy concerns. Motivated by this challenge, we introduce the dynamic deletion-robust submodular maximization problem. We develop the first resilient streaming algorithm, called ROBUST-STREAMING, with a constant factor approximation guarantee to the optimum solution.</description>
    </item>
    
    <item>
      <title>Probabilistic Submodular Maximization in Sub-Linear Time.</title>
      <link>/publications/2017/stan-17a/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/stan-17a/</guid>
      <description>Abstract In this paper, we consider optimizing submodular functions that are drawn from some unknown distribution. This setting arises, e.g., in recommender systems, where the utility of a subset of items may depend on a user-specific submodular utility function. In modern applications, the ground set of items is often so large that even the widely used (lazy) greedy algorithm is not efficient enough. As a remedy, we introduce the problem of sublinear time probabilistic submodular maximization: Given training examples of functions (e.</description>
    </item>
    
    <item>
      <title>Near-Optimal Active Learning of Halfspaces via Query Synthesis in the Noisy Setting.</title>
      <link>/publications/2017/lin-17a/</link>
      <pubDate>Thu, 03 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/lin-17a/</guid>
      <description>Abstract In this paper, we consider the problem of actively learning a linear classifier through query synthesis where the learner can construct artificial queries in order to estimate the true decision boundaries. This problem has recently gained a lot of interest in automated science and adversarial reverse engineering for which only heuristic algorithms are known. In such applications, queries can be constructed de novo to elicit information (e.g., automated science) or to evade detection with minimal cost (e.</description>
    </item>
    
    <item>
      <title>Greed Is Good: Near-Optimal Submodular Maximization via Greedy Optimization.</title>
      <link>/publications/2017/moran-17a/</link>
      <pubDate>Mon, 03 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/moran-17a/</guid>
      <description>Abstract It is known that greedy methods perform well for maximizing \textitmonotone submodular functions. At the same time, such methods perform poorly in the face of non-monotonicity. In this paper, we show‚Äîarguably, surprisingly‚Äîthat invoking the classical greedy algorithm $O(\sqrt(k))$ -times leads to the (currently) fastest deterministic algorithm, called RepeatedGreedy, for maximizing a general submodular function subject to k -independent system constraints. RepeatedGreedy achieves (1+O(1/$\sqrt(k)$))k approximation using $O(nr\sqrt{k})$ function evaluations (here, n and r denote the size of the ground set and the maximum size of a feasible solution, respectively).</description>
    </item>
    
    <item>
      <title>A Submodular Approach to Create Individualized Parcellations of the Human Brain</title>
      <link>/publications/2017/mehraveh-17a/</link>
      <pubDate>Sat, 03 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/mehraveh-17a/</guid>
      <description>Abstract Recent studies on functional neuroimaging (e.g. fMRI) attempt to model the brain as a network. A conventional functional connectivity approach for defining nodes in the network is grouping similar voxels together, a method known as functional parcellation. The majority of previous work on human brain parcellation employs a group-level analysis by collapsing data from the entire population. However, these methods ignore the large amount of inter-individual variability and uniqueness in connectivity.</description>
    </item>
    
    <item>
      <title>Estimating the Size of a Large Network and its Communities from a Random Sample</title>
      <link>/publications/2016/lin-16c/</link>
      <pubDate>Thu, 03 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/lin-16c/</guid>
      <description>Abstract Most real-world networks are too large to be measured or studied directly and there is substantial interest in estimating global network properties from smaller sub-samples. One of the most important global properties is the number of vertices/nodes in the network. Estimating the number of vertices in a large network is a major challenge in computer science, epidemiology, demography, and intelligence analysis. In this paper we consider a population random graph G = (V;E) from the stochastic block model (SBM) with K communities/blocks.</description>
    </item>
    
    <item>
      <title>Fast Constrained Submodular Maximization: Personalized Data Summarization.</title>
      <link>/publications/2016/lin-16b/</link>
      <pubDate>Mon, 03 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/lin-16b/</guid>
      <description>Abstract Can we summarize multi-category data based on user preferences in a scalable manner? Many utility functions used for data summarization satisfy submodularity, a natural diminishing returns property. We cast personalized data summarization as an instance of a general submodular maximization problem subject to multiple constraints. We develop the first practical and FAst coNsTrained submOdular Maximization algorithm, FANTOM, with strong theoretical guarantees. FANTOM maximizes a submodular function (not necessarily monotone) subject to intersection of a p-system and l knapsacks constrains.</description>
    </item>
    
    <item>
      <title>Seeing the Unseen Network: Inferring Hidden Social Ties from Respondent-Driven Sampling.</title>
      <link>/publications/2016/lin-16a/</link>
      <pubDate>Wed, 03 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/lin-16a/</guid>
      <description>Abstract Learning about the social structure of hidden and hard-to-reach populations ‚Äî such as drug users and sex workers ‚Äî is a major goal of epidemiological and public health research on risk behaviors and disease prevention. Respondent-driven sampling (RDS) is a peer-referral process widely used by many health organizations, where research subjects recruit other subjects from their social network. In such surveys, researchers observe who recruited whom, along with the time of recruitment and the total number of acquaintances (network degree) of respondents.</description>
    </item>
    
    <item>
      <title>Submodular Variational Inference for Network Reconstruction</title>
      <link>/publications/2016/lin-16e/</link>
      <pubDate>Sun, 03 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/lin-16e/</guid>
      <description>Abstract In real-world and online social networks, individuals receive and transmit information in real time. Cascading information transmissions (e.g. phone calls, text messages, social media posts) may be understood as a realization of a diffusion process operating on the network, and its branching path can be represented by a directed tree. The process only traverses and thus reveals a limited portion of the edges. The network reconstruction/inference problem is to infer the unrevealed connections.</description>
    </item>
    
    <item>
      <title>Learning network structures from firing patterns.</title>
      <link>/publications/2016/amin-16a/</link>
      <pubDate>Fri, 03 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/amin-16a/</guid>
      <description>Abstract How can we decipher the hidden structure of a network based on limited observations? This question arises in many scenarios ranging from social to wireless and to neural networks. In such settings, we typically observe the nodes&amp;rsquo; behaviors (e.g., the time a node learns about a piece of information, or the time a node gets infected by a disease), and we are interested in inferring the true network over which the diffusion takes place.</description>
    </item>
    
    <item>
      <title>Fast Constrained Submodular Maximization: Personalized Data Summarization</title>
      <link>/icml/publications/baharan-16b/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/baharan-16b/</guid>
      <description>Abstract Can we summarize multi-category data based on user preferences in a scalable manner? Many utility functions used for data summarization satisfy submodularity, a natural diminishing returns property. We cast personalized data summarization as an instance of a general submodular maximization problem subject to multiple constraints. We develop the first practical and FAst coNsTrained submOdular Maximization algorithm, FANTOM, with strong theoretical guarantees. FANTOM maximizes a submodular function (not necessarily monotone) subject to intersection of a p-system and l knapsacks constrains.</description>
    </item>
    
    <item>
      <title>Distributed Submodular Cover: Succinctly Summarizing Massive Data. </title>
      <link>/publications/2015/baharan-15b/</link>
      <pubDate>Tue, 03 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/baharan-15b/</guid>
      <description>Abstract How can one find a subset, ideally as small as possible, that well represents a massive dataset? I.e., its corresponding utility, measured according to a suitable utility function, should be comparable to that of the whole dataset. In this paper, we formalize this challenge as a submodular cover problem. Here, the utility is assumed to exhibit submodularity, a natural diminishing returns condition preva- lent in many data summarization applications. The classical greedy algorithm is known to provide solutions with logarithmic approximation guarantees compared to the optimum solution.</description>
    </item>
    
    <item>
      <title>Lazier Than Lazy Greedy.</title>
      <link>/publications/2015/baharan-15a/</link>
      <pubDate>Mon, 03 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/baharan-15a/</guid>
      <description>Abstract Is it possible to maximize a monotone submodular function faster than the widely used lazy greedy algorithm (also known as accelerated greedy), both in theory and practice? In this paper, we develop the first linear-time algorithm for maximizing a general monotone submodular function subject to a cardinality constraint. We show that our randomized algorithm, STOCHASTIC-GREEDY, can achieve a (1 ‚àí 1/e ‚àí Œµ) approximation guarantee, in expectation, to the optimum solution in time linear in the size of the data and independent of the cardinality constraint.</description>
    </item>
    
    <item>
      <title>Submodular Surrogates for Value of Information</title>
      <link>/publications/2015/chen-15a/</link>
      <pubDate>Mon, 03 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/chen-15a/</guid>
      <description>Abstract How should we gather information to make effective decisions? A classical answer to this fundamental problem is given by the decision-theoretic value of information. Unfortunately, optimizing this objective is intractable, and myopic (greedy) approximations are known to perform poorly. In this paper, we introduce DiRECt, an efficient yet near-optimal algorithm for nonmyopically optimizing value of information. Crucially, DiRECt uses a novel surrogate objective that is: (1) aligned with the value of information problem (2) efficient to evaluate and (3) adaptive submodular.</description>
    </item>
    
    <item>
      <title>Tradeoffs for Space, Time, Data and Risk in Unsupervised Learning.</title>
      <link>/publications/2015/lucic-15a/</link>
      <pubDate>Mon, 03 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/lucic-15a/</guid>
      <description>Abstract Faced with massive data, is it possible to trade off (statistical) risk, and (computational) space and time? This challenge lies at the heart of large-scale machine learning. Using k-means clustering as a prototypical unsupervised learning problem, we show how we can strategically summarize the data (control space) in order to trade off risk and time when data is generated by a probabilistic model. Our summarization is based on coreset constructions from computational geometry.</description>
    </item>
    
    <item>
      <title>Fast Mixing for Discrete Point Processes</title>
      <link>/publications/2015/rebeschini-15a/</link>
      <pubDate>Fri, 03 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/rebeschini-15a/</guid>
      <description>Abstract We investigate the systematic mechanism for designing fast mixing Markov chain Monte Carlo algorithms to sample from discrete point processes under the Dobrushin uniqueness condition for Gibbs measures. Discrete point processes are defined as probability distributions Œº(S)‚àù\exp(Œ≤f(S)) over all subsets $S\in 2^V$ of a finite set V through a bounded set function $f:2^V‚Üí\mathbb{R}$ and a parameter Œ≤&amp;gt;0. A subclass of discrete point processes characterized by submodular functions (which include log-submodular distributions, submodular point processes, and determinantal point processes) has recently gained a lot of interest in machine learning and shown to be effective for modeling diversity and coverage.</description>
    </item>
    
    <item>
      <title>Sequential Information Maximization: When is Greedy Near-optimal?</title>
      <link>/publications/2015/chen-15b/</link>
      <pubDate>Wed, 03 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/chen-15b/</guid>
      <description>Abstract Optimal information gathering is a central challenge in machine learning and science in general. A common objective that quantifies the usefulness of observations is Shannon‚Äôs mutual information, defined w.r.t. a probabilistic model. Greedily selecting observations that maximize the mutual information is the method of choice in numerous applications, ranging from Bayesian experimental design to automated diagnosis, to active learning in Bayesian models. Despite its importance and widespread use in applications, little is known about the theoretical properties of sequential information maximization, in particular under noisy observations.</description>
    </item>
    
    <item>
      <title>Normalization Phenomena in Asynchronous Networks.</title>
      <link>/publications/2015/karbasi-15b/</link>
      <pubDate>Sun, 03 May 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/karbasi-15b/</guid>
      <description>Abstract In this work we study a diffusion process in a network that consists of two types of vertices: inhibitory vertices (those obstructing the diffusion) and excitatory vertices (those facilitating the diffusion). We consider a continuous time model in which every edge of the network draws its transmission time randomly. For such an asynchronous diffusion process it has been recently proven that in Erd≈ës-R√©nyi random graphs a normalization phenomenon arises: whenever the diffusion starts from a large enough (but still tiny) set of active vertices, it only percolates to a certain level that depends only on the activation threshold and the ratio of inhibitory to excitatory vertices.</description>
    </item>
    
    <item>
      <title>Asynchronous decoding of LDPC codes over BEC.</title>
      <link>/publications/2015/haghighatshoar-15a/</link>
      <pubDate>Fri, 03 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/haghighatshoar-15a/</guid>
      <description>Abstract LDPC codes are typically decoded by running a synchronous message passing algorithm over the corresponding bipartite factor graph (made of variable and check nodes). More specifically, each synchronous round consists of 1) updating all variable nodes based on the information received from the check nodes in the previous round, and then 2) updating all the check nodes based on the information sent from variable nodes in the current round. However, in many applications, ranging from message passing in neural networks to hardware implementation of LDPC codes, assuming that all messages are sent and received at the same time is far from realistic.</description>
    </item>
    
    <item>
      <title>Non-Monotone Adaptive Submodular Maximization.</title>
      <link>/publications/2015/gotovos-15a/</link>
      <pubDate>Tue, 03 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/gotovos-15a/</guid>
      <description>Abstract A wide range of AI problems, such as sensor placement, active learning, and network influence maximization, require sequentially selecting elements from a large set with the goal of optimizing the utility of the selected subset. Moreover, each element that is picked may provide stochastic feedback, which can be used to make smarter decisions about future selections. Finding efficient policies for this general class of adaptive optimization problems can be extremely hard.</description>
    </item>
    
    <item>
      <title>Near-Optimally Teaching the Crowd to Classify.</title>
      <link>/publications/2014/singla-14a/</link>
      <pubDate>Fri, 03 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/publications/2014/singla-14a/</guid>
      <description>Abstract How should we present training examples to learners to teach them classification rules? This is a natural problem when training workers for crowdsourcing labeling tasks, and is also motivated by challenges in data-driven online education. We propose a natural stochastic model of the learners, modeling them as randomly switching among hypotheses based on observed feedback. We then develop STRICT, an efficient algorithm for selecting examples to teach to workers. Our solution greedily maximizes a submodular surrogate objective function in order to select examples to show to the learners.</description>
    </item>
    
    <item>
      <title>Near Optimal Bayesian Active Learning for Decision Making</title>
      <link>/publications/2014/javdani-14a/</link>
      <pubDate>Wed, 03 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/publications/2014/javdani-14a/</guid>
      <description>Abstract How should we gather information to make effective decisions? We address Bayesian active learning and experimental design problems, where we sequentially select tests to reduce uncertainty about a set of hypotheses. Instead of minimizing uncertainty per se, we consider a set of overlapping decision regions of these hypotheses. Our goal is to drive uncertainty into a single decision region as quickly as possible. We identify necessary and sufficient conditions for correctly identifying a decision region that contains all hypotheses consistent with observations.</description>
    </item>
    
    <item>
      <title>Streaming submodular maximization: massive data summarization on the fly.</title>
      <link>/publications/2014/badandidiyuru-14a/</link>
      <pubDate>Sun, 03 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/publications/2014/badandidiyuru-14a/</guid>
      <description>Abstract How can one summarize a massive data set &amp;ldquo;on the fly&amp;rdquo;, i.e., without even having seen it in its entirety? In this paper, we address the problem of extracting representative elements from a large stream of data. I.e., we would like to select a subset of say k data points from the stream that are most representative according to some objective function. Many natural notions of &amp;ldquo;representativeness&amp;rdquo; satisfy submodularity, an intuitive notion of diminishing returns.</description>
    </item>
    
    <item>
      <title>Distributed Submodular Maximization: Identifying Representative Elements in Massive Data.</title>
      <link>/publications/2013/baharan-13a/</link>
      <pubDate>Sun, 03 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/baharan-13a/</guid>
      <description>Abstract Many large-scale machine learning problems (such as clustering, non-parametric learning, kernel machines, etc.) require selecting, out of a massive data set, a manageable, representative subset. Such problems can often be reduced to maximizing a submodular set function subject to cardinality constraints. Classical approaches require centralized access to the full data set; but for truly large-scale problems, rendering the data centrally is often impractical. In this paper, we consider the problem of submodular function maximization in a distributed fashion.</description>
    </item>
    
    <item>
      <title>Noise-Enhanced Associative Memories</title>
      <link>/publications/2013/karbasi-13d/</link>
      <pubDate>Sun, 03 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/karbasi-13d/</guid>
      <description>Abstract Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms have allowed reliable learning and recall of an exponential number of patterns. Although these designs correct external errors in recall, they assume neurons that compute noiselessly, in contrast to the highly variable neurons in hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as the internal noise level is below a specified threshold, the error probability in the recall phase can be made exceedingly small.</description>
    </item>
    
    <item>
      <title>Iterative Learning and Denoising in Convolutional Neural Associative Memories.</title>
      <link>/publications/2013/karbasi-13b/</link>
      <pubDate>Thu, 03 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/karbasi-13b/</guid>
      <description>Abstract The task of a neural associative memory is to retrieve a set of previously memorized patterns from their noisy versions by using a network of neurons. Hence, an ideal network should be able to 1) gradually learn a set of patterns, 2) retrieve the correct pattern from noisy queries and 3) maximize the number of memorized patterns while maintaining the reliability in responding to queries. We show that by considering the inherent redundancy in the memorized patterns, one can obtain all the mentioned properties at once.</description>
    </item>
    
    <item>
      <title>Constrained Binary Identification Problem</title>
      <link>/publications/2013/karbasi-13e/</link>
      <pubDate>Mon, 03 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/karbasi-13e/</guid>
      <description>Abstract We consider the problem of building a binary decision tree, to locate an object within a set by way of the least number of membership queries. This problem is equivalent to the &amp;ldquo;20 questions game&amp;rdquo; of information theory and is closely related to lossless source compression. If any query is admissible, Huffman coding is optimal with close to H[P] questions on average, the entropy of the prior distribution P over objects.</description>
    </item>
    
    <item>
      <title>Coupled neural associative memories.</title>
      <link>/publications/2013/karbasi-13c/</link>
      <pubDate>Fri, 03 May 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/karbasi-13c/</guid>
      <description>Abstract We propose a novel architecture to design a neural associative memory that is capable of learning a large number of patterns and recalling them later in presence of noise. It is based on dividing the neurons into local clusters and parallel plains, an architecture that is similar to the visual cortex of macaque brain. The common features of our proposed model with those of spatially-coupled codes enable us to show that the performance of such networks in eliminating noise is drastically better than the previous approaches while maintaining the ability of learning an exponentially large number of patterns.</description>
    </item>
    
    <item>
      <title>Comparison-Based Learning with Rank Nets</title>
      <link>/publications/2012/karbasi-12a/</link>
      <pubDate>Wed, 03 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>/publications/2012/karbasi-12a/</guid>
      <description>Abstract We consider the problem of search through comparisons, where a user is presented with two candidate objects and reveals which is closer to her intended target. We study adap- tive strategies for finding the target, that require knowledge of rank relationships but not actual distances between objects. We propose a new strategy based on rank nets, and show that for target distributions with a bounded doubling constant, it finds the tar- get in a number of comparisons close to the entropy of the target distribution and, hence, of the optimum.</description>
    </item>
    
    <item>
      <title>Multi-level error-resilient neural networks</title>
      <link>/publications/2012/salavati-12a/</link>
      <pubDate>Thu, 03 May 2012 00:00:00 +0000</pubDate>
      
      <guid>/publications/2012/salavati-12a/</guid>
      <description>Abstract The problem of neural network association is to retrieve a previously memorized pattern from its noisy version using a network of neurons. An ideal neural network should include three components simultaneously: a learning algorithm, a large pattern retrieval capacity and resilience against noise. Prior works in this area usually improve one or two aspects at the cost of the third. Our work takes a step forward in closing this gap.</description>
    </item>
    
    <item>
      <title>Sequential group testing with graph constraints</title>
      <link>/publications/2012/karbasi-12c/</link>
      <pubDate>Tue, 03 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>/publications/2012/karbasi-12c/</guid>
      <description>Abstract In conventional group testing, the goal is to detect a small subset of defecting items D in a large population N by grouping arbitrary subset of N into different pools. The result of each group test T is a binary output depending on whether the group contains a defective item or not. The main challenge is to minimize the number of pools required to identify the set D. Motivated by applications in network monitoring and infection propagation, we consider the problem of group testing with graph constraints.</description>
    </item>
    
    <item>
      <title>Hot or not: Interactive content search using comparisons</title>
      <link>/publications/2012/karbasi-12b/</link>
      <pubDate>Sat, 03 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>/publications/2012/karbasi-12b/</guid>
      <description>Abstract In interactive content search through comparisons, a user searching for a target object in a database is asked to select the object most similar to her target from a small list of objects. A new object list is then presented to the user based on her earlier selections. This process is repeated until the target is included in the list presented, at which point the search terminates. We study this problem under the scenario of heterogeneous demand, where target objects are selected from a non-uniform probability distribution.</description>
    </item>
    
    <item>
      <title>Calibration in circular ultrasound tomography devices.</title>
      <link>/publications/2011/parhizkar-11a/</link>
      <pubDate>Sun, 03 Jul 2011 00:00:00 +0000</pubDate>
      
      <guid>/publications/2011/parhizkar-11a/</guid>
      <description>Abstract We consider the position calibration problem in circular tomography devices, where sensors deviate from a perfect circle. We introduce a new method of calibration based on the time-of-fiight measurements between sensors when the enclosed medium is homogeneous. Bounds on the reconstruction errors are proven and results of simulations mimicking a scanning device are presented.</description>
    </item>
    
    <item>
      <title>Content Search through Comparisons.</title>
      <link>/publications/2011/karbasi-11a/</link>
      <pubDate>Thu, 03 Mar 2011 00:00:00 +0000</pubDate>
      
      <guid>/publications/2011/karbasi-11a/</guid>
      <description>Abstract We study the problem of navigating through a database of similar objects using comparisons under heterogeneous demand, a problem closely related to small-world network design. We show that, under heterogeneous demand, the small-world network design problem is NP-hard. Given the above negative result, we propose a novel mechanism for small-world network design and provide an upper bound on its performance under heterogeneous demand. The above mechanism has a natural equivalent in the context of content search through comparisons, again under heterogeneous demand; we use this to establish both upper and lower bounds on content search through comparisons.</description>
    </item>
    
    <item>
      <title>Compression with graphical constraints: An interactive browser</title>
      <link>/publications/2011/karbasi-11b/</link>
      <pubDate>Thu, 03 Feb 2011 00:00:00 +0000</pubDate>
      
      <guid>/publications/2011/karbasi-11b/</guid>
      <description>Abstract We study the problem of searching for a given element in a set of objects using a membership oracle. The membership oracle, given a subset of objects A, and a target object t, determines whether A contains t or not. The goal is to find the target object with the minimum number of questions asked from the oracle. This problem is known to be strongly related to lossless source compression.</description>
    </item>
    
    <item>
      <title>Graph-Constrained Group Testing</title>
      <link>/publications/2010/cheraghchi-10a/</link>
      <pubDate>Mon, 03 May 2010 00:00:00 +0000</pubDate>
      
      <guid>/publications/2010/cheraghchi-10a/</guid>
      <description>Abstract Nonadaptive group testing involves grouping arbitrary subsets of n items into different pools. Each pool is then tested and defective items are identified. A fundamental question involves minimizing the number of pools required to identify at most d defective items. Motivated by applications in network tomography, sensor networks and infection propagation, a variation of group testing problems on graphs is formulated. Unlike conventional group testing problems, each group here must conform to the constraints imposed by a graph.</description>
    </item>
    
    <item>
      <title>From centralized to distributed sensor localization</title>
      <link>/publications/2010/karbasi-10a/</link>
      <pubDate>Sat, 03 Apr 2010 00:00:00 +0000</pubDate>
      
      <guid>/publications/2010/karbasi-10a/</guid>
      <description>Abstract In this work we consider the problem of sensor network localization when only the connectivity information is available. More specifically, we compare the performance of the centralized algorithm MDS-MAP with its distributed version HOP-TERRAIN. We show that both algorithms are able to localize sensors up to a bounded error decreasing at a rate inversely proportional to the radio range r.</description>
    </item>
    
    <item>
      <title>Distributed sensor network localization from local connectivity: performance analysis for the HOP-TERRAIN algorithm</title>
      <link>/publications/2010/karbasi-10b/</link>
      <pubDate>Wed, 03 Mar 2010 00:00:00 +0000</pubDate>
      
      <guid>/publications/2010/karbasi-10b/</guid>
      <description>Abstract This paper addresses the problem of determining the node locations in ad-hoc sensor networks when only connectivity information is available. In previous work, we showed that the localization algorithm MDS-MAP proposed by Y. Shang et al. is able to localize sensors up to a bounded error decreasing at a rate inversely proportional to the radio range r. The main limitation of MDS-MAP is the assumption that the available connectivity information is processed in a centralized way.</description>
    </item>
    
    <item>
      <title>Calibration for Ultrasound Breast Tomography Using Matrix Completion</title>
      <link>/publications/2010/parhizkar-11a/</link>
      <pubDate>Wed, 03 Feb 2010 00:00:00 +0000</pubDate>
      
      <guid>/publications/2010/parhizkar-11a/</guid>
      <description>Abstract We study the calibration process in circular ultrasound tomography devices where the sensor positions deviate from the circumference of a perfect circle. This problem arises in a variety of applications in signal processing ranging from breast imaging to sensor network localization. We introduce a novel method of calibration/localization based on the time-of-flight (ToF) measurements between sensors when the enclosed medium is homogeneous. In the presence of all the pairwise ToFs, one can easily estimate the sensor positions using multi-dimensional scaling (MDS) method.</description>
    </item>
    
    <item>
      <title>Support recovery in compressed sensing: An estimation theoretic approach</title>
      <link>/publications/2009/karbasi-09a/</link>
      <pubDate>Tue, 03 Mar 2009 00:00:00 +0000</pubDate>
      
      <guid>/publications/2009/karbasi-09a/</guid>
      <description>Abstract Compressed sensing (CS) deals with the reconstruction of sparse signals from a small number of linear measurements. One of the main challenges in CS is to find the support of a sparse signal from a set of noisy observations. In the CS literature, several information-theoretic bounds on the scaling law of the required number of measurements for exact support recovery have been derived, where the focus is mainly on random measurement matrices.</description>
    </item>
    
    <item>
      <title>Compressed Sensing with Probabilistic Measurements: A Group Testing Solution</title>
      <link>/publications/2009/cheraghchi-09a/</link>
      <pubDate>Tue, 03 Feb 2009 00:00:00 +0000</pubDate>
      
      <guid>/publications/2009/cheraghchi-09a/</guid>
      <description>Abstract Detection of defective members of large populations has been widely studied in the statistics community under the name &amp;ldquo;group testing&amp;rdquo;, a problem which dates back to World War II when it was suggested for syphilis screening. There the main interest is to identify a small number of infected people among a large population using collective samples. In viral epidemics, one way to acquire collective samples is by sending agents inside the population.</description>
    </item>
    
    <item>
      <title>A new DOA estimation method using a circular microphone array</title>
      <link>/publications/2007/karbasi-07a/</link>
      <pubDate>Wed, 03 Jan 2007 00:00:00 +0000</pubDate>
      
      <guid>/publications/2007/karbasi-07a/</guid>
      <description>Abstract This paper proposes a new DOA (direction of arrival) estimation method based on circular microphone array. For an arbitrary number of microphones, it is analytically shown that DOA estimation reduces to an efficient non-linear optimization problem. Simulation results demonstrate that deviation of the estimation error for 20 and 10 dB SNR is smaller than 0.7 degree which is comparable to high resolution DOA estimation methods. A larger number of microphones provide a more omni-directional spatial resolution.</description>
    </item>
    
    <item>
      <title>A DOA estimation method for an arbitrary triangular microphone arrangement.</title>
      <link>/publications/2006/karbasi-06a/</link>
      <pubDate>Tue, 03 Jan 2006 00:00:00 +0000</pubDate>
      
      <guid>/publications/2006/karbasi-06a/</guid>
      <description>Abstract This paper proposes a new DOA (direction of arrival) estimation method for an arbitrary triangular microphone arrangement. Using the phase rotation factors for the crosscorrelations between the adjacent-microphone signals, a general form of the integrated cross spectrum is derived. DOA estimation is reduced to a non-linear optimization problem of the general integrated cross spectrum. It is shown that a conventional DOA estimation for the equilateral triangular microphone arrangement is a special case of the proposed method.</description>
    </item>
    
    <item>
      <title>Cross Atlas Remapping via Optimal Transport (CAROT): Creating connectomes for any atlas when raw data is not available</title>
      <link>/publications/2022/nature-2022/</link>
      <pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/nature-2022/</guid>
      <description>Abstract Whether using large-scale projects&amp;mdash;like the Human Connectome Project (HCP), the Adolescent Brain Cognitive Development (ABCD) study, Healthy Brain Network (HBN), and the UK Biobank&amp;mdash;or pooling together several smaller studies, open-source, publicly available datasets allow for unpresented sample sizes and promote generalization efforts. Overall, releasing preprocessing data can enhance participant privacy, democratize science, and lead to unique scientific discoveries. But releasing preprocessed data also limits the choices available to the end-user.</description>
    </item>
    
    <item>
      <title>The Power of Subsampling in Submodular Maximization</title>
      <link>/publications/2021/chris-21/</link>
      <pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/chris-21/</guid>
      <description>Abstract We propose subsampling as a unified algorithmic technique for submodular maximization in centralized and online settings. The idea is simple: independently sample elements from the ground set, and use simple combinatorial techniques (such as greedy or local search) on these sampled elements. We show that this approach leads to optimal/state-of-the-art results despite being much simpler than existing methods. In the usual offline setting, we present SampleGreedy, which obtains a (p+2+o(1))-approximation for maximizing a submodular function subject to a p-extendible system using O(n+nk/p) evaluation and feasibility queries, where k is the size of the largest feasible set.</description>
    </item>
    
    <item>
      <title>Individualized functional networks reconfigure with cognitive state</title>
      <link>/publications/2020/mehraveh-2020a/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/mehraveh-2020a/</guid>
      <description>Abstract There is extensive evidence that functional organization of the human brain varies dynamically as the brain switches between task demands, or cognitive states. This functional organization also varies across subjects, even when engaged in similar tasks. To date, the functional network organization of the brain has been considered static. In this work, we use fMRI data obtained across multiple cognitive states (task-evoked and rest conditions) and across multiple subjects, to measure state- and subject-specific functional network parcellation (the assignment of nodes to networks).</description>
    </item>
    
    <item>
      <title>Stochastic Conditional Gradient Methods: From Convex Minimization to Submodular Maximization</title>
      <link>/publications/2020/mokhtaric-20a/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/mokhtaric-20a/</guid>
      <description>Abstract This paper considers stochastic optimization problems for a large class of objective functions, including convex and continuous submodular. Stochastic proximal gradient methods have been widely used to solve such problems; however, their applicability remains limited when the problem dimension is large and the projection onto a convex set is costly. Instead, stochastic conditional gradient methods are proposed as an alternative solution relying on (i) Approximating gradients via a simple averaging technique requiring a single stochastic gradient evaluation per iteration; (ii) Solving a linear program to compute the descent/ascent direction.</description>
    </item>
    
    <item>
      <title>Stochastic Conditional Gradient&#43;&#43;</title>
      <link>/publications/2020/hassani-2019a/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/hassani-2019a/</guid>
      <description>Abstract In this paper, we consider the general non-oblivious stochastic optimization where the underlying stochasticity may change during the optimization procedure and depends on the point at which the function is evaluated. We develop Stochastic Frank-Wolfe++ (SFW++), an efficient variant of the conditional gradient method for minimizing a smooth non-convex function subject to a convex body constraint. We show that SFW++ converges to an œµ-first order stationary point by using $O(1/œµ^3)$ stochastic gradients.</description>
    </item>
    
    <item>
      <title>Submodularity in Action: From Machine Learning to Signal Processing Applications</title>
      <link>/publications/2020/karbasi-2020d/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/karbasi-2020d/</guid>
      <description>Abstract Submodularity is a discrete domain functional prop- erty that can be interpreted as mimicking the role of the well- known convexity/concavity properties in the continuous domain. Submodular functions exhibit strong structure that lead to efficient optimization algorithms with provable near-optimality guarantees. These characteristics, namely, efficiency and provable performance bounds, are of particular interest for signal process- ing (SP) and machine learning (ML) practitioners as a variety of discrete optimization problems are encountered in a wide range of applications.</description>
    </item>
    
    <item>
      <title>There is no single functional atlas even for a single individual: Functional parcel definitions change with task</title>
      <link>/publications/2020/mehraveh-2020b/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/mehraveh-2020b/</guid>
      <description>Abstract The goal of human brain mapping has long been to delineate the functional subunits in the brain and elucidate the functional role of each of these brain regions. Recent work has focused on whole-brain parcellation of functional Magnetic Resonance Imaging (fMRI) data to identify these subunits and create a functional atlas. Functional connectivity approaches to understand the brain at the network level require such an atlas to assess connections between parcels and extract network properties.</description>
    </item>
    
    <item>
      <title> Modern Challenges for Machine Learning Applications of Submodularity: Privacy, Scalability, and Sequences</title>
      <link>/publications/thesis/marko-2020.md/</link>
      <pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/thesis/marko-2020.md/</guid>
      <description>Student: Marko Mitrovic
Dissertation Title: Modern Challenges for Machine Learning Applications of Submodularity: Privacy, Scalability, and Sequences
Date: Thursday, March 5, 2020 Time: 4:00 PM Location: Room 335, 3rd floor, 17 Hillhouse Avenue
Advisor: Amin Karbasi
Other committee members:
Dan Spielman Dragomir Radev Yaron Singer (Harvard) Abstract :In a nutshell, submodularity covers the class of all problems that exhibit some form of diminishing returns. From a theoretical perspective, this notion of diminishing returns is extremely useful as the resulting mathematical properties allow for provably efficient optimization.</description>
    </item>
    
    <item>
      <title>Computational Neuroscience</title>
      <link>/research/computational_neuroscience_neurimaging.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/research/computational_neuroscience_neurimaging.md/</guid>
      <description>The human brain is a complex network, consisting of functionally interconnected regions whose coordinated effort gives rise to different functions. Understanding what these regions are, how they interact, and how this interaction forms a wide range of behavior has long been an essential question for human neuroscience. Neuroimaging techniques have provided a unique opportunity to tackle this question in a data-driven way. Advances in neuroimaging techniques such as functional Magnetic Resonance Imaging (fMRI), have allowed us to approximately measure the neural activity in the brain.</description>
    </item>
    
    <item>
      <title>Non-convex Optimization</title>
      <link>/research/non_convex.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/research/non_convex.md/</guid>
      <description>Until recently, convex programs were seen as the defining boundary for tractability in continuous optimization. However, many problems of interest arising from machine learning and statistical modeling, such as training deep neural networks and learning latent variable models, are glaringly non-convex. While efficient algorithms are known for a few instances of non-convex problems, it remains a central challenge to discover general conditions under which a non-convex problem admits an efficient solution.</description>
    </item>
    
    <item>
      <title>An exemplar-based approach to individualized parcellation reveals the need for sex specific functional networks.</title>
      <link>/publications/2018/mehraveh-2018a/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/mehraveh-2018a/</guid>
      <description>Abstract Recent work with functional connectivity data has led to significant progress in understanding the functional organization of the brain. While the majority of the literature has focused on group-level parcellation approaches, there is ample evidence that the brain varies in both structure and function across individuals. In this work, we introduce a parcellation technique that incorporates delineation of functional networks both at the individual- and group-level. The proposed technique deploys the notion of ‚Äúsubmodularity‚Äù to jointly parcellate the cerebral cortex while establishing an inclusive correspondence between the individualized functional networks.</description>
    </item>
    
    <item>
      <title>Learning neural connectivity from firing activity: efficient algorithms with provable guarantees on topology.</title>
      <link>/publications/2018/karbasi-2018a/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/karbasi-2018a/</guid>
      <description>Abstract The connectivity of a neuronal network has a major effect on its functionality and role. It is generally believed that the complex network structure of the brain provides a physiological basis for information processing. Therefore, identifying the network‚Äôs topology has received a lot of attentions in neuroscience and has been the center of many research initiatives such as Human Connectome Project. Nevertheless, direct and invasive approaches that slice and observe the neural tissue have proven to be time consuming, complex and costly.</description>
    </item>
    
    <item>
      <title>Distributed Submodular Maximization</title>
      <link>/publications/2016/baharan-16a/</link>
      <pubDate>Sat, 03 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/baharan-16a/</guid>
      <description>Abstract Many large-scale machine learning problems‚Äìclustering, non-parametric learning, kernel machines, etc.‚Äìrequire selecting a small yet representative subset from a large dataset. Such problems can often be reduced to maximizing a submodular set function subject to various constraints. Classical approaches to submodular optimization require centralized access to the full dataset, which is impractical for truly large-scale problems. In this paper, we consider the problem of submodular function maximization in a distributed fashion.</description>
    </item>
    
    <item>
      <title>Fast Distributed Submodular Cover: Public-Private Data Summarization.</title>
      <link>/publications/2016/baharan-16c/</link>
      <pubDate>Thu, 03 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/baharan-16c/</guid>
      <description>Abstract In this paper, we introduce the public-private framework of data summarization motivated by privacy concerns in personalized recommender systems and online social services. Such systems have usually access to massive data generated by a large pool of users. A major fraction of the data is public and is visible to (and can be used for) all users. However, each user can also contribute some private data that should not be shared with other users to ensure her privacy.</description>
    </item>
    
    <item>
      <title>Fast Constrained Submodular Maximization: Personalized Data Summarization.</title>
      <link>/publications/2016/baharan-16b/</link>
      <pubDate>Mon, 03 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/baharan-16b/</guid>
      <description>Abstract Can we summarize multi-category data based on user preferences in a scalable manner? Many utility functions used for data summarization satisfy submodularity, a natural diminishing returns property. We cast personalized data summarization as an instance of a general submodular maximization problem subject to multiple constraints. We develop the first practical and FAst coNsTrained submOdular Maximization algorithm, FANTOM, with strong theoretical guarantees. FANTOM maximizes a submodular function (not necessarily monotone) subject to intersection of a p-system and l knapsacks constrains.</description>
    </item>
    
    <item>
      <title>From Small-World Networks to Comparison-Based Search.</title>
      <link>/publications/2015/karbasi-15a/</link>
      <pubDate>Thu, 03 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/karbasi-15a/</guid>
      <description>Abstract The problem of content search through comparisons has recently received considerable attention. In short, a user searching for a target object navigates through a database in the following manner. The user is asked to select the object most similar to her target from a small list of objects. A new object list is then presented to the user based on her earlier selection. This process is repeated until the target is included in the list presented, at which point the search terminates.</description>
    </item>
    
    <item>
      <title>Noise Facilitation in Associative Memories of Exponential Capacity</title>
      <link>/publications/2014/karbasi-14a/</link>
      <pubDate>Wed, 03 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/publications/2014/karbasi-14a/</guid>
      <description>Abstract Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms have allowed reliable learning and recall of an exponential number of patterns that satisfy certain subspace constraints. Although these designs correct external errors in recall, they assume neurons that compute noiselessly, in contrast to the highly variable neurons in brain regions thought to operate associatively, such as hippocampus and olfactory cortex. Here we consider associative memories with boundedly noisy internal computations and analytically characterize performance.</description>
    </item>
    
    <item>
      <title>Convolutional Neural Associative Memories: Massive Capacity with Noise Tolerance.</title>
      <link>/publications/2014/karbasi-14b/</link>
      <pubDate>Fri, 03 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/publications/2014/karbasi-14b/</guid>
      <description>Abstract The task of a neural associative memory is to retrieve a set of previously memorized patterns from their noisy versions using a network of neurons. An ideal network should have the ability to 1) learn a set of patterns as they arrive, 2) retrieve the correct patterns from noisy queries, and 3) maximize the pattern retrieval capacity while maintaining the reliability in responding to queries. The majority of work on neural associative memories has focused on designing networks capable of memorizing any set of randomly chosen patterns at the expense of limiting the retrieval capacity.</description>
    </item>
    
    <item>
      <title>Submodular maximization with cardinality constraints</title>
      <link>/icml/publications/3/</link>
      <pubDate>Fri, 03 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/3/</guid>
      <description>Abstract We consider the problem of maximizing a (non-monotone) submodular function subject to a cardi- nality constraint. In addition to capturing well-known combinatorial optimization problems, e.g., Max- k-Coverage and Max-Bisection, this problem has applications in other more practical settings such as natural language processing, information retrieval, and machine learning. In this work we present im- proved approximations for two variants of the cardinality constraint for non-monotone functions. When at most k elements can be chosen, we improve the current best 1/e ‚àí o(1) approximation to a factor that is in the range [1/e + 0.</description>
    </item>
    
    <item>
      <title>Calibration Using Matrix Completion With Application to Ultrasound Tomography.</title>
      <link>/publications/2013/parhizkar-13a/</link>
      <pubDate>Tue, 03 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/parhizkar-13a/</guid>
      <description>Abstract We study the application of matrix completion in the process of calibrating physical devices. In particular we propose an algorithm together with reconstruction bounds for calibrating circular ultrasound tomography devices. We use the time-of-flight (ToF) measurements between sensor pairs in a homogeneous medium to calibrate the system. The calibration process consists of a low-rank matrix completion algorithm to de-noise and estimate random and structured missing ToFs, and the classic multi-dimensional scaling method to estimate the sensor positions from the ToF measurements.</description>
    </item>
    
    <item>
      <title>Robust Localization From Incomplete Local Information</title>
      <link>/publications/2013/karbasi-13a/</link>
      <pubDate>Tue, 03 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/karbasi-13a/</guid>
      <description>Abstract We consider the problem of localizing wireless devices in an ad hoc network embedded in a d-dimensional Euclidean space. Obtaining a good estimate of where wireless devices are located is crucial in wireless network applications including environment monitoring, geographic routing, and topology control. When the positions of the devices are unknown and only local distance information is given, we need to infer the positions from these local distance measurements. This problem is particularly challenging when we only have access to measurements that have limited accuracy and are incomplete.</description>
    </item>
    
    <item>
      <title>Graph-Constrained Group Testing</title>
      <link>/publications/2012/cheraghchi-12a/</link>
      <pubDate>Mon, 03 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>/publications/2012/cheraghchi-12a/</guid>
      <description>Abstract Nonadaptive group testing involves grouping arbitrary subsets of n items into different pools. Each pool is then tested and defective items are identified. A fundamental question involves minimizing the number of pools required to identify at most d defective items. Motivated by applications in network tomography, sensor networks and infection propagation, a variation of group testing problems on graphs is formulated. Unlike conventional group testing problems, each group here must conform to the constraints imposed by a graph.</description>
    </item>
    
    <item>
      <title>Low-Rank Matrix Approximation Using Point-Wise Operators</title>
      <link>/publications/2012/amini-12a/</link>
      <pubDate>Mon, 03 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>/publications/2012/amini-12a/</guid>
      <description>Abstract The problem of extracting low-dimensional structure from high-dimensional data arises in many applications such as machine learning, statistical pattern recognition, wireless sensor networks, and data compression. If the data is restricted to a lower dimensional subspace, then simple algorithms using linear projections can find the subspace and consequently estimate its dimensionality. However, if the data lies on a low-dimensional but nonlinear space (e.g., manifolds), then its structure may be highly nonlinear and, hence, linear methods are doomed to fail.</description>
    </item>
    
    <item>
      <title>Group Testing With Probabilistic Tests: Theory, Design and Application.</title>
      <link>/publications/2011/cheraghchi-11a/</link>
      <pubDate>Sat, 03 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>/publications/2011/cheraghchi-11a/</guid>
      <description>Abstract Identification of defective members of large populations has been widely studied in the statistics community under the name of group testing. It involves grouping subsets of items into different pools and detecting defective members based on the set of test results obtained for each pool. In a classical noiseless group testing setup, it is assumed that the sampling procedure is fully known to the reconstruction algorithm, in the sense that the existence of a defective member in a pool results in the test outcome of that pool to be positive.</description>
    </item>
    
    <item>
      <title>Online Learning</title>
      <link>/research/online_learning.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/research/online_learning.md/</guid>
      <description>In many practical applications, the environment is so complex that it may be infeasible to lay out a precise model and use the classical mathematical optimization methods. It is then necessary, and very often beneficial, to consider a robust approach, by considering optimization as a process that learns from experience as more aspects of the problem are being observed. This view of optimization as a process has become prominent in various fields and led to many successes in modeling and systems.</description>
    </item>
    
    <item>
      <title>Allocating tasks to machines in computing clusters</title>
      <link>/publications/2014/karbasi-14c/</link>
      <pubDate>Mon, 03 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/publications/2014/karbasi-14c/</guid>
      <description>Abstract Allocating tasks to machines in computing clusters is described. In an embodiment a set of tasks associated with a job are received at a scheduler. In an embodiment an index can be computed for each combination of tasks and processors and stored in a lookup table. In an example the index may be include an indication of the preference for the task to be processed on a particular processor, an indication of a waiting time for the task to be processed and an indication of how other tasks being processed in the computing cluster may be penalized by assigning a task to a particular processor.</description>
    </item>
    
    <item>
      <title>Fast multi-stage submodular maximization: Extended version</title>
      <link>/icml/publications/4/</link>
      <pubDate>Fri, 03 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/4/</guid>
      <description>Abstract Motivated by extremely large-scale machine learning problems, we introduce a new multi- stage algorithmic framework for submodular maximization (called MULTGREED), where at each stage we apply an approximate greedy proce- dure to maximize surrogate submodular functions. The surrogates serve as proxies for a target sub- modular function but require less memory and are easy to evaluate. We theoretically analyze the per- formance guarantee of the multi-stage framework and give examples on how to design instances of MULTGREED for a broad range of natural sub- modular functions.</description>
    </item>
    
    <item>
      <title>Hot or Not: Interactive Content Search Using Comparisons</title>
      <link>/publications/2013/karbasi-13f/</link>
      <pubDate>Sun, 03 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/karbasi-13f/</guid>
      <description>Abstract In comparison-based active learning, a user searching for a target object navigates through a database in the following manner. The user is asked to select the object most similar to her target from small list of objects. A new object list is then presented to the user based on her earlier selection. This process is repeated until the target is included in the list presented, at which point the search terminates.</description>
    </item>
    
    <item>
      <title>Submodular Optimization</title>
      <link>/research/submodular_optimization.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/research/submodular_optimization.md/</guid>
      <description>The difficulty of searching through a massive amount of data in order to quickly make an informed decision is one of today‚Äôs most ubiquitous challenges. Many scientific and engineering models feature inherently discrete decision variables‚Äîfrom phrases in a corpus to objects in an image. Similarly, nearly all aspects of the machine learning pipeline involve discrete tasks, from data summarization and sketching to feature selection and model explanation. The study of how to make near-optimal decisions from a massive pool of possibilities is at the heart of combinatorial optimization.</description>
    </item>
    
    <item>
      <title>Streaming submodular maximization: massive data summarization on the fly</title>
      <link>/icml/publications/5/</link>
      <pubDate>Fri, 03 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/5/</guid>
      <description>Abstract How can one summarize a massive data set &amp;ldquo;on the fly&amp;rdquo;, i.e., without even having seen it in its entirety? In this paper, we address the problem of extracting representative elements from a large stream of data. I.e., we would like to select a subset of say k data points from the stream that are most representative according to some objective function. Many natural notions of &amp;ldquo;representativeness&amp;rdquo; satisfy submodularity, an intuitive notion of diminishing returns.</description>
    </item>
    
    <item>
      <title>On Streaming and Communication Complexity of the Set Cover Problem</title>
      <link>/icml/publications/6/</link>
      <pubDate>Fri, 03 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/6/</guid>
      <description>Abstract Wedevelopthefirststreamingalgorithmandthefirsttwo-partycom- munication protocol that uses a constant number of passes/rounds and sublin- ear space/communication for logarithmic approximation to the classic Set Cover problem. Specifically, for n elements and m sets, our algorithm/protocol achieves a space bound of $O(m ¬∑ n^Œ¥ log^2 n log m)$ using O(4^{1/Œ¥} ) passes/rounds while achieving an approximation factor of $O(4^{1/Œ¥} log n)$ in polynomial time (for $Œ¥ = Œ©(1/ {log n})$). If we allow the algorithm/protocol to spend exponential time per pass/round, we achieve an approximation factor of O(41/Œ¥).</description>
    </item>
    
    <item>
      <title>Streaming algorithms for submodular function maximization</title>
      <link>/icml/publications/7/</link>
      <pubDate>Sat, 03 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/7/</guid>
      <description>Abstract We consider the problem of maximizing a nonnegative submodular set function $f:2^{‚Ñù+}$ subject to a p-matchoid constraint in the single-pass streaming setting. Previous work in this context has considered streaming algorithms for modular functions and monotone submodular functions. The main result is for submodular functions that are {\em non-monotone}. We describe deterministic and randomized algorithms that obtain a $Œ©(1/p)$-approximation using O(klogk)-space, where k is an upper bound on the cardinality of the desired set.</description>
    </item>
    
    <item>
      <title>Online Submodular Maximization with Preemption</title>
      <link>/icml/publications/8/</link>
      <pubDate>Sat, 03 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/8/</guid>
      <description>Abstract Submodular function maximization has been studied extensively in recent years under various constraints and models. The problem plays a major role in various disciplines. We study a natural online variant of this problem in which elements arrive one-by-one and the algorithm has to maintain a solution obeying certain constraints at all times. Upon arrival of an element, the algorithm has to decide whether to accept the element into its solution and may preempt previously chosen elements.</description>
    </item>
    
    <item>
      <title>Streaming Weak Submodularity: Interpreting Neural Networks on the Fly</title>
      <link>/icml/publications/9/</link>
      <pubDate>Tue, 03 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/9/</guid>
      <description>Abstract In many machine learning applications, it is important to explain the predictions of a black-box classifier. For example, why does a deep neural network assign an image to a particular class? We cast interpretability of black-box classifiers as a combinatorial maximization problem and propose an efficient streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of random stream order and a weakly submodular objective function.</description>
    </item>
    
    <item>
      <title>Distributed Submodular Maximization</title>
      <link>/icml/publications/10/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/10/</guid>
      <description>Abstract Many large-scale machine learning problems‚Äìclustering, non-parametric learning, kernel machines, etc.‚Äìrequire selecting a small yet representative subset from a large dataset. Such problems can often be reduced to maximizing a submodular set function subject to various constraints. Classical approaches to submodular optimization require centralized access to the full dataset, which is impractical for truly large-scale problems. In this paper, we consider the problem of submodular function maximization in a distributed fashion.</description>
    </item>
    
    <item>
      <title>Distributed Submodular Cover: Succinctly Summarizing Massive Data</title>
      <link>/icml/publications/11/</link>
      <pubDate>Sat, 03 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/11/</guid>
      <description>Abstract How can one find a subset, ideally as small as possible, that well represents a massive dataset? I.e., its corresponding utility, measured according to a suitable utility function, should be comparable to that of the whole dataset. In this paper, we formalize this challenge as a submodular cover problem. Here, the utility is assumed to exhibit submodularity, a natural diminishing returns condition preva- lent in many data summarization applications. The classical greedy algorithm is known to provide solutions with logarithmic approximation guarantees compared to the optimum solution.</description>
    </item>
    
    <item>
      <title>Fast Distributed Submodular Cover: Public-Private Data Summarization</title>
      <link>/icml/publications/12/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/12/</guid>
      <description>Abstract In this paper, we introduce the public-private framework of data summarization motivated by privacy concerns in personalized recommender systems and online social services. Such systems have usually access to massive data generated by a large pool of users. A major fraction of the data is public and is visible to (and can be used for) all users. However, each user can also contribute some private data that should not be shared with other users to ensure her privacy.</description>
    </item>
    
    <item>
      <title>Distributed Submodular Maximization: Identifying Representative Elements in Massive Data</title>
      <link>/icml/publications/13/</link>
      <pubDate>Thu, 03 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/13/</guid>
      <description>Abstract Many large-scale machine learning problems (such as clustering, non-parametric learning, kernel machines, etc.) require selecting, out of a massive data set, a manageable yet representative subset. Such problems can often be reduced to maximizing a submodular set function subject to cardinality constraints. Classical approaches require centralized access to the full data set; but for truly large-scale problems, rendering the data centrally is often impractical. In this paper, we con- sider the problem of submodular function maximization in a distributed fashion.</description>
    </item>
    
    <item>
      <title>Randomized Composable Core-sets for Distributed Submodular Maximization</title>
      <link>/icml/publications/14/</link>
      <pubDate>Sat, 03 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/14/</guid>
      <description>Abstract An effective technique for solving optimization problems over massive data sets is to partition the data into smaller pieces, solve the problem on each piece and compute a representative solution from it, and finally obtain a solution inside the union of the representative solutions for all pieces. This technique can be captured via the concept of {\em composable core-sets}, and has been recently applied to solve diversity maximization problems as well as several clustering problems.</description>
    </item>
    
    <item>
      <title>Fast greedy algorithms in mapreduce and streaming</title>
      <link>/icml/publications/15/</link>
      <pubDate>Thu, 03 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/15/</guid>
      <description>Abstract Greedy algorithms are practitioners‚Äô best friends‚Äîthey are intu- itive, simple to implement, and often lead to very good solutions. However, implementing greedy algorithms in a distributed setting is challenging since the greedy choice is inherently sequential, and it is not clear how to take advantage of the extra processing power. Our main result is a powerful sampling technique that aids in parallelization of sequential algorithms. We then show how to use this primitive to adapt a broad class of greedy algorithms to the MapReduce paradigm; this class includes maximum cover and submodular maximization subject to p-system constraints.</description>
    </item>
    
    <item>
      <title>A new framework for distributed submodular maximization </title>
      <link>/icml/publications/16/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/16/</guid>
      <description>Abstract A wide variety of problems in machine learning, including exemplar clustering, document summarization, and sensor placement, can be cast as constrained submodular maximization problems. A lot of recent effort has been devoted to developing distributed algorithms for these problems. However, these results suffer from high number of rounds, suboptimal approximation ratios, or both. We develop a framework for bringing existing algorithms in the sequential setting to the distributed setting, achieving near optimal approximation ratios for many settings in only a constant number of MapReduce rounds.</description>
    </item>
    
    <item>
      <title>Stochastic Submodular Maximization: The Case of Coverage Functions</title>
      <link>/icml/publications/17/</link>
      <pubDate>Tue, 03 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/17/</guid>
      <description>Abstract Stochastic optimization of continuous objectives is at the heart of modern ma- chine learning. However, many important problems are of discrete nature and often involve submodular objectives. We seek to unleash the power of stochastic continuous optimization, namely stochastic gradient descent and its variants, to such discrete problems. We first introduce the problem of stochastic submodular optimization, where one needs to optimize a submodular objective which is given as an expectation.</description>
    </item>
    
    <item>
      <title>Gradient Methods for Submodular Maximization</title>
      <link>/icml/publications/18/</link>
      <pubDate>Tue, 03 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/18/</guid>
      <description>Abstract In this paper, we study the problem of maximizing continuous submodular func- tions that naturally arise in many learning applications such as those involving utility functions in active learning and sensing, matrix approximations and network inference. Despite the apparent lack of convexity in such functions, we prove that stochastic projected gradient methods can provide strong approximation guarantees for maximizing continuous submodular functions with convex constraints. More specifically, we prove that for monotone continuous DR-submodular functions, all fixed points of projected gradient ascent provide a factor 1/2 approximation to the global maxima.</description>
    </item>
    
    <item>
      <title>A class of submodular functions for document summarization,</title>
      <link>/icml/publications/19/</link>
      <pubDate>Mon, 03 Jan 2011 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/19/</guid>
      <description>Abstract We design a class of submodular functions meant for document summarization tasks. These functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity. Critically, our functions are monotone nondecreasing and submodular, which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality. When evaluated on DUC 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization.</description>
    </item>
    
    <item>
      <title>Submodularity for data selection in statistical machine translation</title>
      <link>/icml/publications/20/</link>
      <pubDate>Fri, 03 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/20/</guid>
      <description>Abstract We introduce submodular optimization to the problem of training data subset selection for statistical machine translation (SMT). By explicitly formulating data selection as a submodular program, we ob- tain fast scalable selection algorithms with mathematical performance guarantees, re- sulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible. We present a new class of submodular functions designed specifically for SMT and evaluate them on two differ- ent translation tasks.</description>
    </item>
    
    <item>
      <title>Temporal corpus summarization using submodular word coverage</title>
      <link>/icml/publications/21/</link>
      <pubDate>Tue, 03 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/21/</guid>
      <description>Abstract In many areas of life, we now have almost complete electronic archives reaching back for well over two decades. This includes, for example, the body of research papers in computer science, all news articles written in the US, and most people‚Äôs personal email. However, we have only rather limited methods for analyzing and understanding these collections. While keyword-based retrieval systems allow efficient access to individual documents in archives, we still lack methods for understanding a corpus as a whole.</description>
    </item>
    
    <item>
      <title>Greed is good: Near-optimal submodular maximization via greedy optimization</title>
      <link>/icml/publications/22/</link>
      <pubDate>Tue, 03 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/22/</guid>
      <description>Abstract It is known that greedy methods perform well for maximizing monotone submodular functions. At the same time, such methods perform poorly in the face of non-monotonicity. In this paper, we show‚Äîarguably, surprisingly‚Äîthat invoking the classical greedy algorithm O( k)-times leads to the (currently) fastest deterministic algorithm, called REPEATEDGREEDY, for maximizing a general submodular function subject to k-independent system constraints. REPEATEDGREEDY ‚àö‚àö achieves $(1 + O(1/{\sqrt{k}}))k$ approximation using $O(nr \sqrt{k})$ function evaluations (here, n and r de- note the size of the ground set and the maximum size of a feasible solution, respectively).</description>
    </item>
    
    <item>
      <title>Submodular dictionary selection for sparse representation</title>
      <link>/icml/publications/23/</link>
      <pubDate>Sun, 03 Jan 2010 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/23/</guid>
      <description>Abstract We develop an efficient learning framework to construct signal dictionaries for sparse represen- tation by selecting the dictionary columns from multiple candidate bases. By sparse, we mean that only a few dictionary elements, compared to the ambient signal dimension, can exactly repre- sent or well-approximate the signals of interest. We formulate both the selection of the dictionary columns and the sparse representation of signals as a joint combinatorial optimization problem.</description>
    </item>
    
    <item>
      <title>Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection</title>
      <link>/icml/publications/24/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/24/</guid>
      <description>Abstract We study the problem of selecting a subset of k random variables from a large set, in order to obtain the best linear prediction of another vari- able of interest. This problem can be viewed in the context of both feature selection and sparse approximation. We analyze the performance of widely used greedy heuristics, using insights from the maximization of submodular functions and spectral analysis. We introduce the submod- ularity ratio as a key quantity to help understand why greedy algorithms perform well even when the variables are highly correlated.</description>
    </item>
    
    <item>
      <title>RESTRICTED STRONG CONVEXITY IMPLIES WEAK SUBMODULARITY</title>
      <link>/icml/publications/25/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/25/</guid>
      <description>Abstract We connect high-dimensional subset selection and submodular maxi- mization. Our results extend the work of Das and Kempe [In ICML (2011) 1057‚Äì1064] from the setting of linear regression to arbitrary objective func- tions. For greedy feature selection, this connection allows us to obtain strong multiplicative performance bounds on several methods without statistical modeling assumptions. We also derive recovery guarantees of this form un- der standard assumptions. Our work shows that greedy algorithms perform within a constant factor from the best possible subset-selection solution for a broad class of general objective functions.</description>
    </item>
    
    <item>
      <title>Maximizing the spread of in uence through a social network</title>
      <link>/icml/publications/26/</link>
      <pubDate>Fri, 03 Jan 2003 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/26/</guid>
      <description>Abstract Models for the processes by which ideas and influence propagate through a social network have been studied in a number of do- mains, including the diffusion of medical and technological innova- tions, the sudden and widespread adoption of various strategies in game-theoretic settings, and the effects of ‚Äúword of mouth‚Äù in the promotion of new products. Recently, motivated by the design of viral marketing strategies, Domingos and Richardson posed a fun- damental algorithmic problem for such social network processes: if we can try to convince a subset of individuals to adopt a new product or innovation, and the goal is to trigger a large cascade of further adoptions, which set of individuals should we target?</description>
    </item>
    
    <item>
      <title>An online algorithm for maximizing submodular functions,</title>
      <link>/icml/publications/27/</link>
      <pubDate>Sat, 03 Jan 2009 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/27/</guid>
      <description>Abstract We present an algorithm for solving a broad class of online resource allocation problems. Our online algorithm can be applied in environments where abstract jobs arrive one at a time, and one can complete the jobs by investing time in a number of abstract activities, according to some schedule. We assume that the fraction of jobs completed by a schedule is a monotone, submodular function of a set of pairs (v,œÑ), where œÑ is the time invested in activity v.</description>
    </item>
    
    <item>
      <title>Linear submodular bandits and their application to diversified retrieva</title>
      <link>/icml/publications/28/</link>
      <pubDate>Mon, 03 Jan 2011 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/28/</guid>
      <description>Abstract Diversified retrieval and online learning are two core research areas in the design of modern information retrieval systems. In this paper, we propose the linear sub- modular bandits problem, which is an online learning setting for optimizing a gen- eral class of feature-rich submodular utility models for diversified retrieval. We present an algorithm, called LSBGREEDY, and prove that it efficiently converges to a near-optimal model. As a case study, we applied our approach to the setting of personalized news recommendation, where the system must recommend small sets of news articles selected from tens of thousands of available articles each day.</description>
    </item>
    
    <item>
      <title>Online submodular minimization for combinatorial structures</title>
      <link>/icml/publications/29/</link>
      <pubDate>Mon, 03 Jan 2011 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/29/</guid>
      <description>Abstract Most results for online decision problems with structured concepts, such as trees or cuts, as- sume linear costs. In many settings, how- ever, nonlinear costs are more realistic. Ow- ing to their non-separability, these lead to much harder optimization problems. Going beyond linearity, we address online approx- imation algorithms for structured concepts that allow the cost to be submodular, i.e., nonseparable. In particular, we show regret bounds for three Hannan-consistent strategies that capture different settings.</description>
    </item>
    
    <item>
      <title>Dynamic resource allocation in conservation planning</title>
      <link>/icml/publications/30/</link>
      <pubDate>Mon, 03 Jan 2011 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/30/</guid>
      <description>Abstract Consider the problem of protecting endangered species by selecting patches of land to be used for conservation purposes. Typically, the availability of patches changes over time, and recommendations must be made dynamically. This is a chal- lenging prototypical example of a sequential optimization problem under uncertainty in computational sustainability. Ex- isting techniques do not scale to problems of realistic size. In this paper, we develop an efficient algorithm for adaptively making recommendations for dynamic conservation planning, and prove that it obtains near-optimal performance.</description>
    </item>
    
    <item>
      <title>Online submodular set cover, ranking, and repeated active learning</title>
      <link>/icml/publications/31/</link>
      <pubDate>Mon, 03 Jan 2011 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/31/</guid>
      <description>Abstract We propose an online prediction version of submodular set cover with connections to ranking and repeated active learning. In each round, the learning algorithm chooses a sequence of items. The algorithm then receives a monotone submodu- lar function and suffers loss equal to the cover time of the function: the number of items needed, when items are selected in order of the chosen sequence, to achieve a coverage constraint. We develop an online learning algorithm whose loss con- verges to approximately that of the best sequence in hindsight.</description>
    </item>
    
    <item>
      <title>Learning optimally diverse rankings over large document collections</title>
      <link>/icml/publications/32/</link>
      <pubDate>Sun, 03 Jan 2010 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/32/</guid>
      <description>Abstract Most learning to rank research has assumed that the utility of presenting different documents to users is independent, producing learned ranking functions that often return redundant results. The few approaches that avoid this repetition have rather unsatisfyingly lacked theoretical foundations, or do not scale. We present a learning-to-rank formulation that optimizes the fraction of satisfied users, with a scalable algorithm that also explicitly takes document similarity and ranking context into account.</description>
    </item>
    
    <item>
      <title>Randomized sensing in adversarial environment</title>
      <link>/icml/publications/33/</link>
      <pubDate>Mon, 03 Jan 2011 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/33/</guid>
      <description>Abstract How should we manage a sensor network to opti- mally guard security-critical infrastructure? How should we coordinate search and rescue helicopters to best locate survivors after a major disaster? In both applications, we would like to control sensing resources in uncertain, adversarial environments. In this paper, we introduce RSENSE, an efficient algo- rithm which guarantees near-optimal randomized sensing strategies whenever the detection perfor- mance satisfies submodularity, a natural diminishing returns property, for any fixed adversarial scenario.</description>
    </item>
    
    <item>
      <title>Influence maximization in dynamic social networks</title>
      <link>/icml/publications/34/</link>
      <pubDate>Thu, 03 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/34/</guid>
      <description>Abstract Abstract‚ÄîSocial influence and influence diffusion has been widely studied in online social networks. However, most existing works on influence diffusion focus on static networks. In this paper, we study the problem of maximizing influence diffusion in a dynamic social network. Specifically, the network changes over time and the changes can be only observed by periodically probing some nodes for the update of their connections. Our goal then is to probe a subset of nodes in a social network so that the actual influence diffusion process in the network can be best uncovered with the probing nodes.</description>
    </item>
    
    <item>
      <title>Online submodular maximization under a matroid constraint with application to learning assignments</title>
      <link>/icml/publications/35/</link>
      <pubDate>Fri, 03 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/35/</guid>
      <description>Abstract Which ads should we display in sponsored search in order to maximize our revenue? How should we dynamically rank information sources to maximize the value of the ranking? These applications exhibit strong diminishing returns: Redundancy decreases the marginal utility of each ad or information source. We show that these and other problems can be formalized as repeatedly selecting an assignment of items to positions to maximize a sequence of monotone submodular functions that arrive one by one.</description>
    </item>
    
    <item>
      <title>Optimal greedy diversity for recommendation</title>
      <link>/icml/publications/36/</link>
      <pubDate>Sat, 03 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/36/</guid>
      <description>Abstract The need for diversification manifests in various recommendation use cases. In this work, we pro- pose a novel approach to diversifying a list of rec- ommended items, which maximizes the utility of the items subject to the increase in their diversity. From a technical perspective, the problem can be viewed as maximization of a modular function on the polytope of a submodular function, which can be solved optimally by a greedy method.</description>
    </item>
    
    <item>
      <title>Eficient online multi-robot exploration via distributed sequential greedy assignment</title>
      <link>/icml/publications/37/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/37/</guid>
      <description>Abstract This work addresses the problem of efficient on- line exploration and mapping using multi-robot teams via a distributed algorithm for planning for multi-robot exploration‚Äî distributed sequential greedy assignment (DSGA)‚Äîbased on the sequential greedy assignment (SGA) algorithm. SGA permits bounds on suboptimality but requires all robots to plan in series. Rather than plan for robots sequentially as in SGA, DSGA assigns plans to subsets of robots during a fixed number of rounds.</description>
    </item>
    
    <item>
      <title>Stochastic conditional gradient methods: From convex minimization to submodular maximization</title>
      <link>/icml/publications/38/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/38/</guid>
      <description>Abstract This paper considers stochastic optimization problems for a large class of objective functions, including convex and continuous submodular. Stochastic proximal gradient methods have been widely used to solve such problems; however, their applicability remains limited when the prob- lem dimension is large and the projection onto a convex set is computationally costly. Instead, stochastic conditional gradient algorithms are proposed as an alternative solution which rely on (i) Approximating gradients via a simple averaging technique requiring a single stochastic gradient evaluation per iteration; (ii) Solving a linear program to compute the descent/ascent direction.</description>
    </item>
    
    <item>
      <title>Projection-free online optimization with stochastic gradient: From convexity to submodularity,</title>
      <link>/icml/publications/39/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/39/</guid>
      <description>Abstract Online optimization has been a successful framework for solving large-scale problems under computational constraints and partial information. Current methods for online convex optimization require either a projection or exact gradient computation at each step, both of which can be prohibitively expensive for large-scale applications. At the same time, there is a growing trend of non-convex optimization in machine learning community and a need for online methods. Continuous submodular functions, which exhibit a natural diminishing returns condition, have recently been proposed as a broad class of non-convex functions which may be efficiently optimized.</description>
    </item>
    
    <item>
      <title>Online continuous submodular maximization</title>
      <link>/icml/publications/40/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/40/</guid>
      <description>Abstract In this paper, we consider an online optimization process, where the objective functions are not convex (nor concave) but instead belong to a broad class of continuous submodular functions. We first propose a variant of the Frank-Wolfe algorithm that has access to the full gradient of the objective functions. We ‚àö against a (1 ‚àí 1/e)-approximation to the best feasible solution in hindsight. However, in many scenarios, T ) (where T is the horizon of the online optimization problem) only an unbiased estimate of the gradients are available.</description>
    </item>
    
    <item>
      <title>Stochastic conditional gradient&#43;&#43;</title>
      <link>/icml/publications/41/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/41/</guid>
      <description>Abstract In this paper, we consider the general non-oblivious stochastic optimization where the underly- ing stochasticity may change during the optimization procedure and depends on the point at which the function is evaluated. We develop Stochastic Frank-Wolfe++ (SFW++), an efficient variant of the conditional gradient method for minimizing a smooth non-convex function subject to a convex body constraint. We show that SFW++ converges to an oÃ®-first order stationary point by using O(1/oÃ®3) stochastic gradients.</description>
    </item>
    
    <item>
      <title>Optimal algorithms for continuous non-monotone submodular and dr-submodular maximization</title>
      <link>/icml/publications/42/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/42/</guid>
      <description>Abstract In this paper we study the fundamental problems of maximizing abcontinuous non-monotone submodular function over a hypercube, with and without coordinate- wise concavity. This family of optimization problems has several applications in machine learning, economics, and communication systems. Our main result is the first 1 -approximation algorithm for continuous submodular function maximization; 2 the approximation factor of 1 is the best possible for algorithms that use only 2 polynomially many queries.</description>
    </item>
    
    <item>
      <title>Continuous dr-submodular maximization: Structure and algorithms</title>
      <link>/icml/publications/43/</link>
      <pubDate>Tue, 03 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/43/</guid>
      <description>Abstract DR-submodular continuous functions are important objectives with wide real-world applications spanning MAP inference in determinantal point processes (DPPs), and mean-field inference for probabilistic submodular models, amongst others. DR-submodularity captures a subclass of non-convex functions that enables both exact minimization and approximate maximization in polynomial time. In this work we study the problem of maximizing non-monotone continuous DR- submodular functions under general down-closed convex constraints. We start by investigating geometric properties that underlie such objectives, e.</description>
    </item>
    
    <item>
      <title>Submodular functions: from discrete to continuous domains</title>
      <link>/icml/publications/44/</link>
      <pubDate>Tue, 03 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/44/</guid>
      <description>Abstract Submodular set-functions have many applications in combinatorial optimization, as they can be minimized and approximately maximized in polynomial time. A key element in many of the algorithms and analyses is the possibility of extending the submodular set-function to a convex function, which opens up tools from convex optimization. Submodularity goes beyond set-functions and has naturally been considered for problems with multiple labels or for functions defined on continuous domains, where it corresponds essentially to cross second-derivatives being nonpositive.</description>
    </item>
    
    <item>
      <title>Guaranteed non-convex optimization: Submodular maximization over continuous domains</title>
      <link>/icml/publications/45/</link>
      <pubDate>Tue, 03 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/45/</guid>
      <description>Abstract Submodular continuous functions are a category of (generally) non-convex/non-concave functions with a wide spectrum of applications. We characterize these functions and demonstrate that they can be maximized efficiently with approximation guarantees. Specifically, i) We introduce the weak DR property that gives a unified characterization of submodularity for all set, integer-lattice and continuous functions; ii) for maximizing monotone DR-submodular continuous functions under general down-closed convex constraints, we propose a Frank-Wolfe variant with (1-1/e) approximation guarantee, and sub-linear convergence rate; iii) for maximizing general non-monotone submodular continuous functions subject to box constraints, we propose a DoubleGreedy algorithm with 1/3 approximation guarantee.</description>
    </item>
    
    <item>
      <title>Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization</title>
      <link>/icml/publications/46/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/46/</guid>
      <description>Abstract Many problems in artificial intelligence require adaptively making a sequence of decisions with uncertain outcomes under partial observability. Solving such stochastic optimization problems is a fundamental but notoriously difficult challenge. In this paper, we introduce the concept of adaptive submodularity, generalizing submodular set functions to adaptive policies. We prove that if a problem satisfies this property, a simple adaptive greedy algorithm is guaranteed to be competitive with the optimal policy.</description>
    </item>
    
    <item>
      <title>Submodular surrogates for value of information</title>
      <link>/icml/publications/47/</link>
      <pubDate>Sat, 03 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/47/</guid>
      <description>Abstract How should we gather information to make effective decisions? A classical answer to this fundamental problem is given by the decision-theoretic value of information. Unfortunately, optimizing this objective is intractable, and myopic (greedy) approximations are known to perform poorly. In this paper, we introduce DIRECT, an efficient yet near-optimal algorithm for nonmyopically optimizing value of information. Crucially, DIRECT uses a novel surrogate objective that is: (1) aligned with the value of information problem (2) efficient to evaluate and (3) adaptive submod- ular.</description>
    </item>
    
    <item>
      <title>Learning sparse combinatorial representations via two-stage submodular maximization</title>
      <link>/icml/publications/48/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/48/</guid>
      <description>Abstract We consider the problem of learning sparse rep- resentations of data sets, where the goal is to re- duce a data set in manner that optimizes mul- tiple objectives. Motivated by applications of data summarization, we develop a new model which we refer to as the two-stage submodu- lar maximization problem. This task can be viewed as a combinatorial analogue of repre- sentation learning problems such as dictionary learning and sparse regression.</description>
    </item>
    
    <item>
      <title>An Analysis of Approximations for Maximizing Submodular Set Functions‚ÄîI</title>
      <link>/icml/publications/49/</link>
      <pubDate>Sat, 03 Jan 1987 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/49/</guid>
      <description>Abstract LetN be a finite set andz be a real-valued function defined on the set of subsets ofN that satisfies z(S)+z(T)‚â•z(S‚ãÉT)+z(S‚ãÇT) for allS, T inN. Such a function is called submodular. We consider the problem maxS‚äÇN{a(S):|S|‚â§K,z(S) submodular}.
Several hard combinatorial optimization problems can be posed in this framework. For example, the problem of finding a maximum weight independent set in a matroid, when the elements of the matroid are colored and the elements of the independent set can have no more thanK colors, is in this class.</description>
    </item>
    
    <item>
      <title>Accelerated greedy algorithms for maximizing submodular set functions</title>
      <link>/icml/publications/50/</link>
      <pubDate>Tue, 03 Jan 1978 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/50/</guid>
      <description>Abstract Given a finite set E and a real valued function f on P(E) (the power set of E) the optimal subset problem (P) is to find S ‚äÇ E maximizing f over P(E). Many combinatorial optimization problems can be formulated in these terms. Here, a family of approximate solution methods is studied : the greedy algorithms.
After having described the standard greedy algorithm (SG) it is shown that, under certain assumptions (namely : submodularity of f) the computational complexity of (SG) can often be significantly reduced, thus leading to an accelerated greedy algorithm (AG).</description>
    </item>
    
    <item>
      <title>Submodular Maximization With Cardinality Constraints</title>
      <link>/icml/publications/51/</link>
      <pubDate>Fri, 03 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/51/</guid>
      <description>Abstract We consider the problem of maximizing a (non-monotone) submodular function subject to a cardi- nality constraint. In addition to capturing well-known combinatorial optimization problems, e.g., Max- k-Coverage and Max-Bisection, this problem has applications in other more practical settings such as natural language processing, information retrieval, and machine learning. In this work we present im- proved approximations for two variants of the cardinality constraint for non-monotone functions. When at most k elements can be chosen, we improve the current best 1/e ‚àí o(1) approximation to a factor that is in the range [1/e + 0.</description>
    </item>
    
    <item>
      <title>Differentially Private Submodular Maximization: Data Summarization in Disguise</title>
      <link>/icml/publications/52/</link>
      <pubDate>Tue, 03 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/52/</guid>
      <description>Abstract Many data summarization applications are cap- tured by the general framework of submodular maximization. As a consequence, a wide range of efficient approximation algorithms have been developed. However, when such applications in- volve sensitive data about individuals, their pri- vacy concerns are not automatically addressed. To remedy this problem, we propose a gen- eral and systematic study of differentially private submodular maximization. We present privacy- preserving algorithms for both monotone and non-monotone submodular maximization under cardinality, matroid, and p-extendible system constraints, with guarantees that are competitive with optimal solutions.</description>
    </item>
    
    <item>
      <title>Regularized Submodular Maximization at Scale</title>
      <link>/icml/publications/70/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/70/</guid>
      <description>Abstract In this paper, we propose scalable methods for maximizing a regularized submodular function f=g‚àí‚Ñì expressed as the difference between a monotone submodular function g and a modular function ‚Ñì. Indeed, submodularity is inherently related to the notions of diversity, coverage, and representativeness. In particular, finding the mode of many popular probabilistic models of diversity, such as determinantal point processes, submodular probabilistic models, and strongly log-concave distributions, involves maximization of (regularized) submodular functions.</description>
    </item>
    
    <item>
      <title>fast algorithms for maximizing submodular functions</title>
      <link>/icml/publications/53/</link>
      <pubDate>Thu, 03 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/53/</guid>
      <description>Abstract There has been much progress recently on improved approximations for problems involving submodular objective functions, and many interesting techniques have been developed. However, the resulting algorithms are often slow and impractical. In this paper we develop algorithms that match the best known approximation guarantees, but with significantly improved running times, for maximizing a monotone submodular function f : 2[n] ‚Üí; R + subject to various constraints. As in previous work, we measure the number of oracle calls to the objective function which is the dominating term in the running time.</description>
    </item>
    
    <item>
      <title>Efficient Algorithms and Lower Bound for Submodular Maximization</title>
      <link>/icml/publications/54/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/54/</guid>
      <description>Abstract In this work, we study constrained submodular maximization problems and design algorithms that improve the state-of-the-art performance guarantees. We first present the \emph{adaptive decreasing threshold} algorithm, which achieves an approximation ratio of (1‚àí1/e‚àíŒµ) by performing queries per element. To the best of our knowledge, this is currently the fastest known \textbf{deterministic} algorithm, and nearly achieves the optimal approximation ratio. We also study several other well-known constrained monotone submodular maximization problems.</description>
    </item>
    
    <item>
      <title>Robust Guarantees of Stochastic Greedy Algorithms</title>
      <link>/icml/publications/55/</link>
      <pubDate>Tue, 03 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/55/</guid>
      <description>Abstract In this paper we analyze the robustness of stochastic variants of the greedy algorithm for submodular maximization. Our main result shows that for maximizing a monotone submod- ular function under a cardinality constraint, itera- tively selecting an element whose marginal con- tribution is approximately maximal in expecta- tion is a sufficient condition to obtain the opti- mal approximation guarantee with exponentially high probability, assuming the cardinality is suf- ficiently large.</description>
    </item>
    
    <item>
      <title>Constrained Non-Monotone Submodular Maximization: Offline and Secretary Algorithms</title>
      <link>/icml/publications/56/</link>
      <pubDate>Sun, 03 Jan 2010 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/56/</guid>
      <description>Abstract Constrained submodular maximization problems have long been studied, most recently in the context of auc- tions and computational advertising, with near-optimal results known under a variety of constraints when the submodular function is monotone. The case of non-monotone submodular maximization is less well understood: the first approximation algorithms even for the unconstrained setting were given by Feige et al. (FOCS ‚Äô07). More recently, Lee et al. (STOC ‚Äô09, APPROX ‚Äô09) show how to approximately maximize non-monotone submodular functions when the constraints are given by the intersection of p matroid constraints; their algorithm is based on local-search procedures that consider p-swaps, and hence the running time may be nŒ©(p), implying their algorithm is polynomial-time only for constantly many matroids.</description>
    </item>
    
    <item>
      <title>Greed Is Good: Near-Optimal Submodular Maximization via Greedy Optimization</title>
      <link>/icml/publications/57/</link>
      <pubDate>Tue, 03 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/57/</guid>
      <description>Abstract It is known that greedy methods perform well for maximizing monotone submodular functions. At the same time, such methods perform poorly in the face of non-monotonicity. In this paper, we show‚Äîarguably, surprisingly‚Äîthat invoking the classical greedy algorithm O( k)-times leads to the (currently) fastest deterministic algorithm, called REPEATEDGREEDY, for maximizing a general submodular function subject to k-independent system constraints. REPEATEDGREEDY ‚àö‚àö achieves (1 + O(1/ k))k approximation using O(nr k) function evaluations (here, n and r de- note the size of the ground set and the maximum size of a feasible solution, respectively).</description>
    </item>
    
    <item>
      <title>Learning Sparse Combinatorial Representations via Two-stage Submodular Maximization</title>
      <link>/icml/publications/58/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/58/</guid>
      <description>Abstract We consider the problem of learning sparse representations of data sets, where the goal is to reduce a data set in manner that optimizes multiple objectives. Motivated by applications of data summarization, we develop a new model which we refer to as the two-stage submodular maximization problem. This task can be viewed as a combinatorial analogue of representation learning problems such as dictionary learning and sparse regression. The two-stage problem strictly generalizes the problem of cardinality constrained submodular maximization, though the objective function is not submodular and the techniques for submodular maximization cannot be applied.</description>
    </item>
    
    <item>
      <title>Probabilistic Submodular Maximization in Sub-Linear Time</title>
      <link>/icml/publications/59/</link>
      <pubDate>Tue, 03 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/59/</guid>
      <description>Abstract In this paper, we consider optimizing submodular functions that are drawn from some unknown distribution. This setting arises, e.g., in recommender systems, where the utility of a subset of items may depend on a user-specific submodular utility function. In modern applications, the ground set of items is often so large that even the widely used (lazy) greedy algorithm is not efficient enough. As a remedy, we introduce the problem of sublinear time probabilistic submodular maximization: Given training examples of functions (e.</description>
    </item>
    
    <item>
      <title>A Nearly-linear Time Algorithm for Submodular Maximization with a Knapsack Constraint</title>
      <link>/icml/publications/60/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/60/</guid>
      <description>Abstract We consider the problem of maximizing a monotone submodular function subject to a knap- sack constraint. Our main contribution is an algorithm that achieves a nearly-optimal, 1‚àí1/e‚àíoÃ® approximation, using (1/oÃ®)O(1/oÃ®4)nlog2 n function evaluations and arithmetic operations. Our algorithm is impractical but theoretically interesting, since it overcomes a fundamental running time bottleneck of the multilinear extension relaxation framework. This is the main approach for obtaining nearly-optimal approximation guarantees for important classes of constraints but it leads to Œ©(n2) running times, since evaluating the multilinear extension is expensive.</description>
    </item>
    
    <item>
      <title>Submodular Maximization Through Barrier Functions</title>
      <link>/icml/publications/61/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/61/</guid>
      <description>Abstract In this paper, we introduce a novel technique for constrained submodular maximization, inspired by barrier functions in continuous optimization. This connection not only improves the running time for constrained submodular maximization but also provides the state of the art guarantee. More precisely, for maximizing a monotone submodular function subject to the combination of a k-matchoid and ‚Ñì-knapsack constraint (for ‚Ñì‚â§k), we propose a potential function that can be approximately minimized.</description>
    </item>
    
    <item>
      <title>Submodular Streaming in All Its Glory: Tight Approximation, Minimum Memory and Low Adaptive Complexity</title>
      <link>/icml/publications/62/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/62/</guid>
      <description>Abstract Streaming algorithms are generally judged by the quality of their solution, memory footprint, and computational complexity. In this paper, we study the problem of maximizing a mono- tone submodular function in the streaming set- ting with a cardinality constraint k. We first propose SIEVE-STREAMING++, which requires just one pass over the data, keeps only O(k) el- ements and achieves the tight 1/2-approximation guarantee. The best previously known stream- ing algorithms either achieve a suboptimal 1/4- approximation with ‚á•(k) memory or the opti- mal 1/2-approximation with O(k log k) memory.</description>
    </item>
    
    <item>
      <title>submodular maximization meets streaming: matching, magroids and more</title>
      <link>/icml/publications/63/</link>
      <pubDate>Sat, 03 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/63/</guid>
      <description>Abstract We study the problem of finding a maximum matching in a graph given by an input stream listing its edges in some arbitrary order, where the quantity to be maximized is given by a monotone submodular function on subsets of edges. This problem, which we call maximum submodular-function matching (MSM), is a natural generalization of maximum weight matching (MWM), which is in turn a generalization of maximum cardinality matching (MCM).</description>
    </item>
    
    <item>
      <title>The one-way communication complexity of submodular maximization with applications to streaming and robustness</title>
      <link>/icml/publications/64/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/64/</guid>
      <description>Abstract We consider the classical problem of maximizing a monotone submodular function subject to a cardinality constraint, which, due to its numerous applications, has recently been studied in various computational models. We consider a clean multi-player model that lies between the offline and streaming model, and study it under the aspect of one-way communication complexity. Our model captures the streaming setting (by considering a large number of players), and, in addition, two player approximation results for it translate into the robust setting.</description>
    </item>
    
    <item>
      <title>Approximability of monotone submodular function maximization under cardinality and matroid constraints in the streaming model</title>
      <link>/icml/publications/65/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/65/</guid>
      <description>Abstract Maximizing a monotone submodular function under various constraints is a classical and intensively studied problem. However, in the single-pass streaming model, where the elements arrive one by one and an algorithm can store only a small fraction of input elements, there is much gap in our knowledge, even though several approximation algorithms have been proposed in the literature. In this work, we present the first lower bound on the approximation ratios for cardinality and matroid constraints that beat 1‚àí1/e in the single-pass streaming model.</description>
    </item>
    
    <item>
      <title>Optimal Streaming Algorithms for Submodular Maximization with Cardinality Constraints</title>
      <link>/icml/publications/66/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/66/</guid>
      <description>Abstract We study the problem of maximizing a non-monotone submodular function subject to a cardinality constraint in the streaming model. Our main contributions are two single-pass (semi-)streaming algorithms that use O ÃÉ(k) ¬∑ poly(1/Œµ) memory, where k is the size constraint. At the end of the stream, both our algorithms post-process their data structures using any offline algorithm for submodular maximization, and obtain a solution whose approximation guarantee is Œ± ‚àí Œµ, where Œ± is the approximation of the offline algorithm.</description>
    </item>
    
    <item>
      <title>Do Less, Get More: Streaming Submodular Maximization with Subsampling</title>
      <link>/icml/publications/67/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/67/</guid>
      <description>Abstract In this paper, we develop the first one-pass streaming algorithm for submodular maximization that does not evaluate the entire stream even once. By carefully sub- sampling each element of the data stream, our algorithm enjoys the tightest approx- imation guarantees in various settings while having the smallest memory footprint and requiring the lowest number of function evaluations. More specifically, for a monotone submodular function and a p-matchoid constraint, our randomized algorithm achieves a 4p approximation ratio (in expectation) with O(k) memory and O(km/p) queries per element (k is the size of the largest feasible solution and m is the number of matroids used to define the constraint).</description>
    </item>
    
    <item>
      <title>streaming submodular maximization under a k-system constraint</title>
      <link>/icml/publications/68/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/68/</guid>
      <description>Abstract In this paper, we propose a novel framework that converts streaming algorithms for monotone submodular maximization into streaming algo- rithms for non-monotone submodular maximiza- tion. This reduction readily leads to the currently tightest deterministic approximation ratio for sub- modular maximization subject to a k-matchoid constraint. Moreover, we propose the first stream- ing algorithm for monotone submodular maxi- mization subject to k-extendible and k-set system constraints. Together with our proposed reduction, we obtain O(k log k) and O(k2 log k) approxima- tion ratio for submodular maximization subject to the above constraints, respectively.</description>
    </item>
    
    <item>
      <title>The Power of Randomization: Distributed Submodular Maximization on Massive Datasets</title>
      <link>/icml/publications/69/</link>
      <pubDate>Sat, 03 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/69/</guid>
      <description>Abstract A wide variety of problems in machine learning, including exemplar clustering, document summarization, and sensor placement, can be cast as constrained submodular maximization problems. Unfortunately, the resulting submodular optimization problems are often too large to be solved on a single machine. We consider a distributed, greedy algorithm that combines previous approaches with randomization. The result is an algorithm that is embarrassingly parallel and achieves provable, constant factor, worst-case approximation guarantees.</description>
    </item>
    
    <item>
      <title>Parallel Double Greedy Submodular Maximization‚Äù, Pan, Jegelka, Gonzalez, Bradley, Jordan</title>
      <link>/icml/publications/71/</link>
      <pubDate>Fri, 03 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/71/</guid>
      <description>Abstract Many machine learning problems can be reduced to the maximization of sub- modular functions. Although well understood in the serial setting, the parallel maximization of submodular functions remains an open area of research with recent results [1] only addressing monotone functions. The optimal algorithm for maximizing the more general class of non-monotone submodular functions was introduced by Buchbinder et al. [2] and follows a strongly serial double-greedy logic and program analysis.</description>
    </item>
    
    <item>
      <title>Optimal Distributed Submodular Optimization via Sketching</title>
      <link>/icml/publications/72/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/72/</guid>
      <description>Abstract We present distributed algorithms for several classes of submodular optimization problems such as k-cover, set cover, facility location, and probabilistic coverage. The new algorithms enjoy almost optimal space complexity, optimal approximation guarantees, optimal communication complexity (and run in only four rounds of computation), addressing major shortcomings of prior work. We first present a distributed algorithm for k-cover using only √ï(n) space per machine, and then extend it to several submodular optimization problems, improving previous results for all the above problems-e.</description>
    </item>
    
    <item>
      <title>The adaptive complexity of maximizing a submodular function</title>
      <link>/icml/publications/73/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/73/</guid>
      <description>Abstract In this paper we study the adaptive complexity of submodular optimization. Informally, the adaptive complexity of a problem is the minimal number of sequential rounds required to achieve a constant factor approximation when polynomially-many queries can be executed in parallel at each round. Adaptivity is a fundamental concept that is heavily studied in computer science, largely due to the need for parallelizing computation. Somewhat surprisingly, very little is known about adaptivity in submodular optimization.</description>
    </item>
    
    <item>
      <title>An Exponential Speedup in Parallel Running Time for Submodular Maximization without Loss in Approximation</title>
      <link>/icml/publications/74/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/74/</guid>
      <description>Abstract In this paper we study the adaptivity of submodular maximization. Adaptivity quantifies the number of sequential rounds that an algorithm makes when function evaluations can be executed in parallel. Adaptivity is a fundamental concept that is heavily studied across a variety of areas in computer science, largely due to the need for parallelizing computation. For the canonical problem of maximizing a monotone submodular function under a cardinality constraint, it is well known that a simple greedy algorithm achieves a 1‚àí1/e approximation and that this approximation is optimal for polynomial-time algorithms.</description>
    </item>
    
    <item>
      <title>Submodular Maximization with Nearly-optimal Approximation and Adaptivity in Nearly-linear Time</title>
      <link>/icml/publications/75/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/75/</guid>
      <description>Abstract In this paper, we study the tradeoff between the approximation guarantee and adaptivity for the problem of maximizing a monotone submodular function subject to a cardinality con- straint. The adaptivity of an algorithm is the number of sequential rounds of queries it makes to the evaluation oracle of the function, where in every round the algorithm is allowed to make polynomially-many parallel queries. Adaptivity is an important consideration in settings where the objective function is estimated using samples and in applications where adaptivity is the main running time bottleneck.</description>
    </item>
    
    <item>
      <title>Submodular Maximization with Nearly Optimal Approximation, Adaptivity and Query Complexity</title>
      <link>/icml/publications/76/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/76/</guid>
      <description>Abstract Submodular optimization generalizes many classic problems in combinatorial optimization and has recently found a wide range of applications in machine learning (e.g., feature engineering and active learning). For many large-scale optimization problems, we are often concerned with the adaptivity complexity of an algorithm, which quantifies the number of sequential rounds where polynomially-many independent function evaluations can be executed in parallel. While low adaptivity is ideal, it is not sufficient for a distributed algorithm to be efficient, since in many practical applications of submodular optimization the number of function evaluations becomes prohibitively expensive.</description>
    </item>
    
    <item>
      <title>Unconstrained submodular maximization with constant adaptive complexity</title>
      <link>/icml/publications/77/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/77/</guid>
      <description>Abstract In this paper, we consider the unconstrained submodular maximization problem. We propose the first algorithm for this problem that achieves a tight (1/2‚àíŒµ)-approximation guarantee using O ÃÉ(Œµ‚àí1) adaptive rounds and a linear number of function evaluations. No previously known algorithm for this problem achieves an approximation ratio better than 1/3 using less than Œ©(n) rounds of adaptivity, where n is the size of the ground set. Moreover, our algorithm easily extends to the maximization of a non-negative continuous DR-submodular function subject to a box constraint, and achieves a tight (1/2 ‚àí Œµ)-approximation guarantee for this problem while keeping the same adaptive and query complexities.</description>
    </item>
    
    <item>
      <title>Continuous Submodular Maximization: Beyond DR-Submodularity</title>
      <link>/icml/publications/78/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/icml/publications/78/</guid>
      <description>Abstract In this paper, we propose the first continuous optimization algorithms that achieve a constant fac- tor approximation guarantee for the problem of monotone continuous submodular maximization subject to a linear constraint. We first prove that a simple variant of the vanilla coordinate ascent, called Coordinate-Ascent+, achieves a ( e-1/(2e‚àí1) ‚àí Œµ)-approximation guarantee while performing O(n/Œµ) itera- tions, where the computational complexity of each iteration is roughly O(n/‚àöŒµ + n log n) (here, n denotes the dimension of the optimization problem).</description>
    </item>
    
    <item>
      <title>Qinghao Liang won Best Paper award at Graphs in Biomedical Imaging</title>
      <link>/news/grail-2022.md/</link>
      <pubDate>Thu, 09 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/news/grail-2022.md/</guid>
      <description>Qinghao Liang, Javid Dadashkarimi, Wei Dai, Amin Karbasi, Joseph Chang, Harrison H. Zhou, and Dustin Scheinost won best paper award at 4th Workshop on GRaphs in biomedicAl Image anaLysis (GRAIL) at MICCAI 2022 for the paper &amp;ldquo;Transforming connectomes to ‚Äúany‚Äù parcellation via graph matching&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>Jane Received a Graduate Fellowship for STEM diversity from NSA</title>
      <link>/news/jane-2022.md/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/news/jane-2022.md/</guid>
      <description>Jane Lee received Graduate Fellowship for STEM diversity from NSA.</description>
    </item>
    
    <item>
      <title>Amin Karbasi wins Bell Labs Prize for Brain Mapping Technology</title>
      <link>/news/bell-2022.md/</link>
      <pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/news/bell-2022.md/</guid>
      <description>Prof. Amin Karbasi and Mehraveh Salehi (Ph.D. ‚Äô19, Electrical Engineering) won second place at Nokia‚Äôs Bell Labs Prize ceremony Tuesday for their work on understanding how information flows in the human brain based on different cognitive tasks. The team says their innovation makes a concrete connection between artificial intelligence and natural intelligence.
The Bell Labs Prize recognizes disruptive innovations that solve the key challenges facing humanity. Eight months ago, numerous academics from across the world applied to work with Nokia researchers to help advance their innovations.</description>
    </item>
    
    <item>
      <title>Chris Harshaw Graduated</title>
      <link>/news/chris-2021.md/</link>
      <pubDate>Mon, 27 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/news/chris-2021.md/</guid>
      <description>Chris Harshaw Graduated with his PhD thesis entitled ‚ÄúAlgorithmic Advances for the Design and Analysis of Randomized Experiments‚Äù.
Adviserr: Daniel Spielman and Amin Karbasi
Abstract: Randomized experiments are the gold standard for investigating the causal effect of treatment on a population. In this dissertation, we present algorithmic advances for three different problems arising in the design and analysis of randomized experiments: covariate balancing, variance estimation, and bipartite experiments. In the first chapter, we describe an inherent trade-off between covariate balancing and robustness, which we formulate as a distributional discrepancy problem.</description>
    </item>
    
    <item>
      <title>NSF invests The Institute for Learning-enabled Optimization at Scale</title>
      <link>/news/karbasi-21.md/</link>
      <pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>/news/karbasi-21.md/</guid>
      <description>The TILOS mission is to make impossible optimizations possible, at scale and in practice. Our research will pioneer learning-enabled optimizations that transform chip design, robotics, networks and other use domains that are vital to our nation‚Äôs health, prosperity and welfare. TILOS is a National Science Foundation funded National Artificial Intelligence (AI) Research Institute: TILOS
TIIOS is a partnership of faculty from University of California, San Diego, Massachusetts Institute of Technology, National University, University of Pennsylvania, University of Texas at Austin, and Yale University.</description>
    </item>
    
    <item>
      <title>ICML 2021 Workshop on Overparameterization: Pitfalls &amp; Opportunities</title>
      <link>/news/icml-2021.md/</link>
      <pubDate>Sun, 30 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/news/icml-2021.md/</guid>
      <description>Yasaman Bahri Research Scientist
Google Research, Brain Team
Quanquan Gu Department of Computer Science
University of California, Los Angeles
Amin Karbasi Yale Institute for Network Science Yale University
Hanie Sedghi Research Scientist
Google Brain
Brief Description and Outline Modern machine learning models are often highly overparameterized. The prime examples of late are neural network architectures that can achieve state-of-the-art performance while having many more parameters than the number of training examples.</description>
    </item>
    
    <item>
      <title>Mingrui Zhang Graduated</title>
      <link>/news/mingrui-2021.md/</link>
      <pubDate>Sat, 10 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/news/mingrui-2021.md/</guid>
      <description>Mingrui Zhang Graduated with his PhD thesis entitled ‚ÄúScalable Projection-Free Optimization‚Äù.
Adviserr: Amin Karbasi
Abstract: As a projection-free algorithm, Frank-Wolfe (FW) method, also known as conditional gradient, has recently received considerable attention in the machine learning community. In this dissertation1, we study several topics on the FW variants for scalable projection-free optimization. We first propose 1-SFW, the first projection-free method that requires only one sample per iteration to update the optimization variable and yet achieves the best known complexity bounds for convex, non-convex, and monotone DR-submodular settings.</description>
    </item>
    
    <item>
      <title>Scalable Projection-Free Optimization</title>
      <link>/publications/thesis/mingrui-21.md/</link>
      <pubDate>Wed, 10 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/thesis/mingrui-21.md/</guid>
      <description>Adviserr: Amin Karbasi
Abstract: As a projection-free algorithm, Frank-Wolfe (FW) method, also known as conditional gradient, has recently received considerable attention in the machine learning community. In this dissertation1, we study several topics on the FW variants for scalable projection-free optimization. We first propose 1-SFW, the first projection-free method that requires only one sample per iteration to update the optimization variable and yet achieves the best known complexity bounds for convex, non-convex, and monotone DR-submodular settings.</description>
    </item>
    
    <item>
      <title>ICML 2020 Tutorial on Submodular Optimization: From Discrete to Continuous and Back</title>
      <link>/icml/icml-20.md/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/icml/icml-20.md/</guid>
      <description>Hamed Hassani School of Engineering and Applied Sciences
University of Pennsylvania
Philadelphia, PA 19104
Amin Karbasi Yale Institute for Network Science Yale University New Haven, CT 06520
Slides Module 1 Module 2 Module 3 Module 4 Videos Part I Part II Part III Part IV Brief Description and Outline This tutorial will cover recent advancements in discrete optimization methods for large-scale machine learning. Traditionally, machine learning has been harnessing convex optimization to design fast algorithms with provable guarantees for a broad range of applications.</description>
    </item>
    
    <item>
      <title>ICML 2020 Tutorial on Submodular Optimization: From Discrete to Continuous and Back</title>
      <link>/news/icml-20.md/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/news/icml-20.md/</guid>
      <description>Hamed Hassani School of Engineering and Applied Sciences
University of Pennsylvania
Philadelphia, PA 19104
Amin Karbasi Yale Institute for Network Science Yale University New Haven, CT 06520
Slides Module 1 Module 2 Module 3 Module 4 Videos Part I Part II Part III Part IV Brief Description and Outline This tutorial will cover recent advancements in discrete optimization methods for large-scale machine learning. Traditionally, machine learning has been harnessing convex optimization to design fast algorithms with provable guarantees for a broad range of applications.</description>
    </item>
    
    <item>
      <title>Amin Karbasi is promoted to Associate Professor</title>
      <link>/news/amin-2020.md/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/news/amin-2020.md/</guid>
      <description>Amin Karbasi is promoted to Associate Professor.</description>
    </item>
    
    <item>
      <title>Hunala is launched!</title>
      <link>/news/hunala-2020.md/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/news/hunala-2020.md/</guid>
      <description>Yale app Hunala aims to be ‚ÄòWaze for coronavirus‚Äô Yale News: A team of Yale researchers has developed a new app, Hunala, that aims to be the ‚ÄúWaze for coronavirus Led by Sterling Professor Nicholas Christakis, a physician and social networks expert, with colleagues in the Yale School of Engineering and Applied Science, the free app provides a daily snapshot of personal and regional risk for COVID-19 infection based on data from the Centers for Disease Control and Prevention and users‚Äô self-reported health and demographic information.</description>
    </item>
    
    <item>
      <title>Marko Mitrovic Graduated</title>
      <link>/news/marko-2020.md/</link>
      <pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/news/marko-2020.md/</guid>
      <description>Student: Marko Mitrovic
Dissertation Title: Modern Challenges for Machine Learning Applications of Submodularity: Privacy, Scalability, and Sequences
Date: Thursday, March 5, 2020 Time: 4:00 PM Location: Room 335, 3rd floor, 17 Hillhouse Avenue
Advisor: Amin Karbasi
Other committee members:
Dan Spielman Dragomir Radev Yaron Singer (Harvard) Abstract :In a nutshell, submodularity covers the class of all problems that exhibit some form of diminishing returns. From a theoretical perspective, this notion of diminishing returns is extremely useful as the resulting mathematical properties allow for provably efficient optimization.</description>
    </item>
    
    <item>
      <title>Lin Chen Graduated</title>
      <link>/news/lin-2020.md/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/news/lin-2020.md/</guid>
      <description>The final doctoral examination for Lin Chen will take place on Wednesday, March 11th, at 2:00pm in HLH17, Room 335.
The title of the thesis is: Online Optimization: Convex and Submodular Functions
Advisor: Amin Karbasi
Members of the Committee are:
Professor Negahban Professor Spielman Abstract: In the first part, we study switching-constrained online convex optimization (OCO), where the player has a limited number of opportunities to change her action. While the discrete analog of this online learning task has been studied extensively, previous work in the continuous setting has neither established the minimax rate nor algorithmically achieved it.</description>
    </item>
    
    <item>
      <title>Online Optimization: Convex and Submodular Functions</title>
      <link>/publications/thesis/lin-2020.md/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/thesis/lin-2020.md/</guid>
      <description>The final doctoral examination for Lin Chen will take place on Wednesday, March 11th, at 2:00pm in HLH17, Room 335.
The title of the thesis is: Online Optimization: Convex and Submodular Functions
Advisor: Amin Karbasi
Members of the Committee are:
Professor Negahban Professor Spielman Abstract: In the first part, we study switching-constrained online convex optimization (OCO), where the player has a limited number of opportunities to change her action. While the discrete analog of this online learning task has been studied extensively, previous work in the continuous setting has neither established the minimax rate nor algorithmically achieved it.</description>
    </item>
    
    <item>
      <title>Brain Initiative Trainee Award 2020</title>
      <link>/news/dadashkarimi-20.md/</link>
      <pubDate>Fri, 21 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/news/dadashkarimi-20.md/</guid>
      <description>Javid Dadashkarimi won Brain Initiative Trainee Award for `A web-based toolkit for visualizing and interpreting complex connectomic results in BISWeb &amp;lsquo;. This award is given to students who had significant research impact that can create a dynamic understanding of brain function.</description>
    </item>
    
    <item>
      <title>Interactive Decision Making</title>
      <link>/research/interactive_decision_making.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/research/interactive_decision_making.md/</guid>
      <description>In computer science, and machine learning in particular, the primary purpose of many systems is to help humans make decisions. Simultaneously, many of these systems also stand to benefit from having a human in the loop, whether it is to reinforce good decisions, warn against bad decisions, or simply to provide expert advice in areas of uncertainty.
A simple, concrete example can be seen in recommender systems. Whether it is through explicit feedback (such as rating a movie on Netflix) or implicit feedback (such as clicking/not clicking on an advertisement), the vast majority of successful, real-world recommender systems are constantly interacting with and adapting to each user.</description>
    </item>
    
    <item>
      <title>Open Positions at INFERENCE, INFORMATION, AND DECISION GROUP</title>
      <link>/openings/openings.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/openings/openings.md/</guid>
      <description>Postdoc The I.I.D. Group at Yale University has a postdoctoral open position. We are looking for an applicant who has a strong background and interest in theoretical machine learning, statistics, and optimization. Many problems studied in our group have interdisciplinary components, including computational neuroscience, healthcare, and social networks. See our research page for an overview of the projects that we are working on. The postdoctoral fellow will have the flexibility to pursue any topics within our ongoing areas of research such as discrete and continuous optimization, online and reinforcement learning, statistical learning theory, etc.</description>
    </item>
    
    <item>
      <title>Open Positions at INFERENCE, INFORMATION, AND DECISION GROUP</title>
      <link>/openings/openings.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/openings/openings.md/</guid>
      <description>PhD students The I.I.D. Systems Group at the Yale Institute of Network Science has an open position for a Ph.D. student. We are looking for exceptional applicants who have a strong background in areas such as machine learning, statistics, optimization, theoretical computer science and distributed systems. Many problems studied in our group have a strong interdisciplinary component. See our research page for an overview of the projects that we are working on.</description>
    </item>
    
    <item>
      <title>PhD students</title>
      <link>/openings/phd.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/openings/phd.md/</guid>
      <description>The I.I.D. Systems Group at the Yale Institute of Network Science has an open position for a Ph.D. student. We are looking for exceptional applicants who have a strong background in areas such as machine learning, statistics, optimization, theoretical computer science and distributed systems. Many problems studied in our group have a strong interdisciplinary component. See our research page for an overview of the projects that we are working on.</description>
    </item>
    
    <item>
      <title>Postdocs</title>
      <link>/openings/postdoc.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/openings/postdoc.md/</guid>
      <description>The I.I.D. Systems Group at the Yale Institute of Network Science has open positions for a postdoctoral scholar. We are looking for exceptional applicants who have a strong background in areas such as machine learning, statistics, optimization, theoretical computer science and distributed systems. Applicants for the postdoctoral scholar position need to have a strong publication record at the relevant conferences and journals. Many problems studied in our group have a strong interdisciplinary component.</description>
    </item>
    
    <item>
      <title>Stochastic Processes</title>
      <link>/courses/stochastic.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/courses/stochastic.md/</guid>
      <description>Course ID: S&amp;amp;DS 351, MATH 251, S&amp;amp;DS 551, ENAS 502 In this course we will go through the following topics:
Review important notions of probability Expectation Convergence of random variables Law of large numbers Poisson Processes Renewal Processes Markov Chains and Random Walks Martingales If time permits, we will look at a few interesting applications of the above topics. Textbook: We mainly use &amp;ldquo;Introduction to Stochastic Processes&amp;rdquo; by Erhan Cinlar</description>
    </item>
    
    <item>
      <title>Individualized and Task-Specific Functional Brain Mapping</title>
      <link>/publications/thesis/mehraveh-19.md/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/thesis/mehraveh-19.md/</guid>
      <description>Mehraveh Salehi Graduated with her PhD thesis entitled ‚ÄúIndividualized and Task-Specific Human Brain Mapping‚Äù.
The final doctoral examination for Mehraveh Salehi took place on Tuesday, September 3rd, at 11:00am at YINS, 17 Hillhouse, 3rd floor.
The title of the thesis is: Individualized and Task-Specific Functional Brain Mapping
Adviserr:
Amin Karbasi Todd Constable Members of the Committee are:
Professor Papademetris Professor Tassiulas Professor Jeff Bilmes Abstract: Understanding the human brain, with its remarkable ability to control higher thought, behavior, and memory, remains one of the greatest intellectual challenges in all of science.</description>
    </item>
    
    <item>
      <title>Mehraveh Salehi Graduated</title>
      <link>/news/mehraveh-19.md/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/news/mehraveh-19.md/</guid>
      <description>Mehraveh Salehi Graduated with her PhD thesis entitled ‚ÄúIndividualized and Task-Specific Human Brain Mapping‚Äù.
The final doctoral examination for Mehraveh Salehi took place on Tuesday, September 3rd, at 11:00am at YINS, 17 Hillhouse, 3rd floor.
The title of the thesis is: Individualized and Task-Specific Functional Brain Mapping
Adviserr:
Amin Karbasi Todd Constable Members of the Committee are:
Professor Papademetris Professor Tassiulas Professor Jeff Bilmes Abstract: Understanding the human brain, with its remarkable ability to control higher thought, behavior, and memory, remains one of the greatest intellectual challenges in all of science.</description>
    </item>
    
    <item>
      <title>NSF CAREER Award 2019</title>
      <link>/news/karbasi-19a.md/</link>
      <pubDate>Sun, 21 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/news/karbasi-19a.md/</guid>
      <description>Amin Karbasi won NSF CAREER Award 2019: link
ABSTRACT: The difficulty of searching through a massive amount of data in order to quickly make an informed decision is one of today&amp;rsquo;s most ubiquitous challenges. Many scientific and engineering models feature data with inherently discrete characteristics, where discrete means that the data takes on a finite set of possible values. Examples of such data include phrases in text to objects in an image.</description>
    </item>
    
    <item>
      <title>Facebook-Main Award 2019 for Mehraveh Salehi</title>
      <link>/news/mehraveh-19b.md/</link>
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/news/mehraveh-19b.md/</guid>
      <description>Facebook-Main Award for Mehraveh Salehi. Congratulations Mehraveh! She wins FACEBOOK-MAIN AWARD as a leading woman in AI-Neuroscience. Here is her very accessible presentation on the role of submodularity on decoding brain states.</description>
    </item>
    
    <item>
      <title>Ivy 3-Minute Thesis Competition Award</title>
      <link>/news/mehraveh-19c.md/</link>
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/news/mehraveh-19c.md/</guid>
      <description>On April 25, 2019, Columbia University and the United Nations hosted the first Ivy-wide 3-Minute Thesis Competition to celebrate the diverse scholarly work that PhD students are doing across the Ivy League community. Following opening remarks by Elliott Harris, UN Assistant Secretary-General and Chief Economist, fourteen PhD students from Brown University, Columbia University, Cornell University, Dartmouth College, Princeton University, the University of Pennsylvania and Yale gave presentations about their research in just 3 minutes.</description>
    </item>
    
    <item>
      <title>Amazon Research Award</title>
      <link>/news/karbasi-19b.md/</link>
      <pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/news/karbasi-19b.md/</guid>
      <description>Amin Karbasi won Amazon Research Award! link
The ARA program funds projects conducted primarily by PhD students or post docs, under the supervision of the faculty member awarded the funds. To encourage collaboration and the sharing of insights, each funded proposal team is assigned an appropriate Amazon research contact. We also invite ARA recipients to speak at Amazon offices worldwide about their work and to meet with our research groups face-to-face, and encourage ARA recipients to publish their research outcome and commit related code to open-source code repositories.</description>
    </item>
    
    <item>
      <title>Understanding Ourselves Through Neuroimaging and Algorithms</title>
      <link>/news/mehraveh-18a.md/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/news/mehraveh-18a.md/</guid>
      <description>Understanding Ourselves Through Neuroimaging and Algorithms: link
Combining neuroscience with algorithms and network science, Yale researchers have developed a method of analyzing the neuronal connections of individual brains that allow them to successfully predict the subjects‚Äô IQs, their sex, and even tasks they were performing at the time of the brain scan.
In a collaboration between the labs of Amin Karbasi, assistant professor of electrical engineering &amp;amp; computer science and Todd Constable, professor of radiology and biomedical imaging and of neurosurgery, the researchers analyzed the functional MRI scans of more than 100 subjects from the Human Connectome Project, a five-year effort to create a network map of the human brain.</description>
    </item>
    
    <item>
      <title>Dynamic and Discrete Optimization</title>
      <link>/courses/fall-19.md/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/courses/fall-19.md/</guid>
      <description>Course ID: EENG 433 The study of fundamental techniques in discrete optimization and its numerous industry applications, including airline scheduling, telecommunication routing, recommender systems, and predicting financial markets. Topics include linear programs, dynamic programs, finite and infinite state scenarios, bandit optimization, and potentially submodular functions and relationships between discrete and continuous optimization methods through the lens of submodularity. Familiarity with discrete mathematics, algorithms, combinatorics, and calculus is assumed.
Meeting Info TTh 1pm-2:15pm in BCT CO31</description>
    </item>
    
    <item>
      <title>ONR Young Investigator Award 2019</title>
      <link>/news/karbasi-18a.md/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/news/karbasi-18a.md/</guid>
      <description>Amin Karbasi won ONR Young Investigator Award 2019
Area: Robust and Interactive Information Gathering in Dynamic Environments.</description>
    </item>
    
    <item>
      <title>ISIT 2018 Tutorial on Submodularity</title>
      <link>/news/isit-18a.md/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/news/isit-18a.md/</guid>
      <description>ISIT 2018 Tutorial on Submodularity in Information and Data Science.
Submodularity is a structural property over functions that has received significant attention in the mathematics community, owing to their natural and wide ranging applicability. In particular, numerous challenging problems in information theory, machine learning, and artificial intelligence rely on optimization techniques for which submodularity is key to solving them efficiently.We will start by defining submodularity and polymatroidality ‚Äî we will survey a surprisingly diverse set of functions that are submodular and operations that preserve submodularity.</description>
    </item>
    
    <item>
      <title>CoderPortfolio„ÅÆÁâπÂæ¥</title>
      <link>/posts/featuresofcoderportfolio.ja/</link>
      <pubDate>Fri, 03 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/featuresofcoderportfolio.ja/</guid>
      <description>Â§âÊõ¥ÁÇπ ShortCode &amp;ldquo;private content&amp;quot;„ÇíËøΩÂä†„Åó„Åæ„Åó„Åü Á∂∫È∫ó„Å™„Çµ„Ç§„Éà„ÇíÂ¥©„Åï„Åö„Å´„ÄÅ„ÅÇ„Å™„Åü„ÅÆÂÄã‰∫∫ÁöÑ„Å™„Ç≥„É≥„ÉÜ„É≥„ÉÑÔºàË∂£Âë≥„ÇÑÊÑüÊÉÖÔºâ„ÇíÁ∞°Âçò„Å´‰ºù„Åà„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ
ÂÆüÈöõ„ÅÆÂãï„Åç„Å´„Å§„ÅÑ„Å¶„ÅØ„ÄÅ&amp;ldquo;„ÉÜ„Éº„Éû„Éá„É¢&amp;quot;„Çí„ÅîË¶ß„Åè„Å†„Åï„ÅÑ„ÄÇ
„É™„É≥„ÇØ„Å´Ëµ§„ÅÑ‰∏ãÁ∑ö„ÅÆ„Ç¢„Éã„É°„Éº„Ç∑„Éß„É≥„ÇíËøΩÂä†„Åó„Åæ„Åó„Åü „Çµ„Ç§„Éà„Åå„Çà„Çä„Ç´„É©„Éï„É´„Å´Áæé„Åó„Åè„Å™„Çä„Åæ„Åó„Åü„ÄÇ
SNS„Ç∑„Çß„Ç¢„ÅÆ„Éú„Çø„É≥„ÇíËøΩÂä†„Åó„Åæ„Åó„Åü „ÅÇ„Å™„Åü„ÅÆË®ò‰∫ã„Åå„Çà„ÇäÂΩ±ÈüøÂäõ„ÇíÊåÅ„Å§„Çà„ÅÜ„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇ
„Ç≥„Éº„Éâ„ÅÆ„Éè„Ç§„É©„Ç§„Éà„ÇíÂ§âÊõ¥„Åó„Åæ„Åó„Åü Êó•Êú¨„ÅÆ„Çµ„Éº„Éì„Çπ Qiita „ÅÆ„Éè„Ç§„É©„Ç§„Éà„ÇíÂèÇÁÖß„Åó„Åæ„Åó„Åü„ÄÇ
„Å®„Å¶„ÇÇÂÑ™„Åó„ÅèÁæé„Åó„ÅÑ„Éè„Ç§„É©„Ç§„Éà„Åß„Åô„ÄÇ
„Éò„ÉÉ„ÉÄ„Éº„Å´ÂΩ±„ÇíËøΩÂä†„Åó„Åæ„Åó„Åü „Çà„ÇäÂ¢ÉÁïåÁ∑ö„Åå„ÅØ„Å£„Åç„Çä„Åó„Åæ„Åó„Åü„ÄÇ
ShortCode &amp;ldquo;portfolio&amp;quot;„ÇíËøΩÂä†„Åó„Åæ„Åó„Åü „ÅÇ„Å™„Åü„ÅÆ‰ΩúÂìÅ„ÇíÁ∂∫È∫ó„Å´Ë°®Á§∫„Åß„Åç„Çã„Çà„ÅÜ„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇ ÂÆüÈöõ„ÅÆÂãï„Åç„Å´„Å§„ÅÑ„Å¶„ÅØ„ÄÅ&amp;ldquo;„ÉÜ„Éº„Éû„Éá„É¢&amp;quot;„Çí„ÅîË¶ß„Åè„Å†„Åï„ÅÑ„ÄÇ</description>
    </item>
    
    <item>
      <title>Features Of CoderPortfolio</title>
      <link>/posts/featuresofcoderportfolio/</link>
      <pubDate>Fri, 03 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/featuresofcoderportfolio/</guid>
      <description>Change Point &amp;ldquo;private content&amp;rdquo; short code added. It is now possible to communicate your personal hobby and your own content.
You can use it easily. Please see &amp;ldquo;theme-demo&amp;rdquo; for details and demo.
An animation of red underline was added to Anchor. The site became a little fun and colorful.
The button of the SNS share was added. It became to have an influence when writing articles more.
Changed the color scheme of code highlight.</description>
    </item>
    
    <item>
      <title>Theme Demo</title>
      <link>/posts/theme-demo/</link>
      <pubDate>Fri, 03 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/theme-demo/</guid>
      <description>Style Demo h1 Heading h2 Heading h3 Heading h4 Heading h5 Heading h6 Heading This is bold text
This is bold text
This is italic text
This is italic text
Deleted text
This is text with inline math $\sum_{n=1}^{\infty} 2^{-n} = 1$ and with math blocks:
$$ \sum_{n=1}^{\infty} 2^{-n} = 1 $$
Heading Another heading text text text text text text Block quotes are written like so.
They can span multiple paragraphs, if you like.</description>
    </item>
    
    <item>
      <title>„ÉÜ„Éº„Éû„Éá„É¢</title>
      <link>/posts/theme-demo.ja/</link>
      <pubDate>Fri, 03 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/theme-demo.ja/</guid>
      <description>Style Demo h1 Heading h2 Heading h3 Heading h4 Heading h5 Heading h6 Heading This is bold text
This is bold text
This is italic text
This is italic text
Deleted text
This is text with inline math $\sum_{n=1}^{\infty} 2^{-n} = 1$ and with math blocks:
$$ \sum_{n=1}^{\infty} 2^{-n} = 1 $$
Heading Another heading text text text text text text Block quotes are written like so.
They can span multiple paragraphs, if you like.</description>
    </item>
    
    <item>
      <title>CVPR 2018 Tutorial on Big Data Summarization</title>
      <link>/news/cvp4-18a.md/</link>
      <pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/news/cvp4-18a.md/</guid>
      <description>CVPR 2018 tutorial on Big Data Summarization: Algorithms and Applications.
The increasing amounts of data in computer vision requires robust tools to extract most important information from large collections of data. The summarization problem addresses this challenge by finding a small subset of most informative data points from large datasets. However, summarization often leads to optimization programs that are nonconvex and NP-hard. While (non)convex programming and submodular optimization have been studied intensively in mathematics, successful and effective applications of them for the problem of information summarization along with new theoretical results have recently emerged.</description>
    </item>
    
    <item>
      <title>Google PhD felloswhip</title>
      <link>/news/lin-18a.md/</link>
      <pubDate>Thu, 24 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/news/lin-18a.md/</guid>
      <description>Lin Chen received Google PhD felloswhip 2018. Congrats Lin.</description>
    </item>
    
    <item>
      <title>Dynamic and Discrete Optimization</title>
      <link>/courses/spring-18.md/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/courses/spring-18.md/</guid>
      <description>Course ID: EENG 400 The study of fundamental techniques in discrete optimization and its numerous industry applications, including airline scheduling, telecommunication routing, recommender systems, and predicting financial markets. Topics include linear programs, dynamic programs, finite and infinite state scenarios, bandit optimization, and potentially submodular functions and relationships between discrete and continuous optimization methods through the lens of submodularity. Familiarity with discrete mathematics, algorithms, combinatorics, and calculus is assumed.
Meeting Info TTh 1pm-2:15pm in HLH17 03</description>
    </item>
    
    <item>
      <title>AFOSR Young Investigator Research Award 2018</title>
      <link>/news/karbasi-17a.md/</link>
      <pubDate>Wed, 11 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/news/karbasi-17a.md/</guid>
      <description>Amin Karbasi won AFOSR Young Investigator Research Award 2018!
ARLINGTON, Virginia &amp;ndash; The Air Force Office of Scientific Research today announced it will award approximately $19.9 million in grants to 45 scientists and engineers from 38 research institutions and small businesses who submitted winning research proposals through the Air Force&amp;rsquo;s Young Investigator Research Program (YIP).
The YIP is open to scientists and engineers at research institutions across the United States who received Ph.</description>
    </item>
    
    <item>
      <title>NIPS 2017 workshop</title>
      <link>/news/nips-17.md/</link>
      <pubDate>Mon, 09 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/news/nips-17.md/</guid>
      <description>We are organizing a workshop at NIPS 2017 on Discrete Structures in Machine Learning.
Abstract: Traditionally, machine learning has been focused on methods where objects reside in continuous domains. The goal of this workshop is to advance state-of-the-art methods in machine learning that involve discrete structures.
Models with ultimately discrete solutions play an important role in machine learning. At its core, statistical machine learning is concerned with making inferences from data, and when the underlying variables of the data are discrete, both the tasks of model inference as well as predictions using the inferred model are inherently discrete algorithmic problems.</description>
    </item>
    
    <item>
      <title>Simons Research Fellowship 2017</title>
      <link>/news/simons-16.md/</link>
      <pubDate>Sat, 02 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/news/simons-16.md/</guid>
      <description>Amin Karbasi received Simons Research Fellowship 2017 for the ‚ÄúFoundations of Machine Learning‚Äù program: link
T‚Äãhe goal of this program is to grow the reach and impact of CS theory within machine learning.
One central component of the program will be ‚Äãformalizing basic questions in developing areas of practice‚Äã, and gaining fundamental insights into these. Target areas of particular interest are ‚Äãinteractive learning‚Äã and representation learning. Interactive learning consists of scenarios in which the communication between human and learner is richer than a one-¬≠way transmission of labeled examples; this happens, for instance, in teaching, or explanation-based learning, and in crowdsourcing.</description>
    </item>
    
    <item>
      <title>Microsoft Azure Research Award 2017</title>
      <link>/news/azure-17.md/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/news/azure-17.md/</guid>
      <description>Amin Karbasi Received Microsoft Azure Research Award 2017</description>
    </item>
    
    <item>
      <title>MICCAI Young Scientist Award</title>
      <link>/news/mehraveh-17a.md/</link>
      <pubDate>Mon, 09 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/news/mehraveh-17a.md/</guid>
      <description>MICCAI Young Scientist Award for ‚ÄùA Submodular Approach to Create Individualized Parcellations of Human Brain‚Äù</description>
    </item>
    
    <item>
      <title>Theoretical Challenges in Network Science</title>
      <link>/courses/fall-17.md/</link>
      <pubDate>Tue, 03 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/courses/fall-17.md/</guid>
      <description>Course ID: ENAS 962 This is an interdisciplinary course with a focus on the emerging science of complex networks and their mathematical models. Students learn about the recent research on the structure and analysis of such networks, and on models that abstract their basic properties. Topics include random graphs and their properties, probabilistic techniques for link analysis, centralized and decentralized search algorithms, random walks, diffusion and epidemic processes, and spectral methods.</description>
    </item>
    
    <item>
      <title>Grainger Award 2017</title>
      <link>/news/frontier-16.md/</link>
      <pubDate>Mon, 19 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/news/frontier-16.md/</guid>
      <description>Grainger Award 2017 from National Academy of Engineering for Advancement of Interdisciplinary Research.
Amin Karbasi (Yale University) and Amit Surana (United Technologies Research Center) have received a Grainger Grant to ‚Äúdevelop a unified approach for saliency detection in heterogeneous temporal data.‚Äù The grant will support the team‚Äôs interdisciplinary research to develop algorithms for compressing massive amounts of time‚Äîvarying data into small salient or informative datasets to allow faster decision making. For example, if salient or significant images in particular frames can be identified from terabytes of video stream, the original video can be summarized using a much smaller set of frames, enabling much faster video processing for surveillance applications such as anomaly detection and activity classification.</description>
    </item>
    
    <item>
      <title>Stochastic Processes</title>
      <link>/courses/spring-16.md/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/courses/spring-16.md/</guid>
      <description>Course ID: ENAS 496, ENAS 502, STAT 251, STAT 551, MATH 251 Introduction to the study of random processes including linear prediction and Kalman filtering, Poison counting process and renewal processes, Markov chains, branching processes, birth-death processes, Markov random fields, martingales, and random walks. Applications chosen from communications, networking, image reconstruction, Bayesian statistics, finance, probabilistic analysis of algorithms, and genetics and evolution.
Meeting Info MW 1pm-2:15pm in WLH 208
Final Exam Tuesday, May 10, 2016 at 7pm</description>
    </item>
    
    <item>
      <title>AISTATS Best Student Paper Award</title>
      <link>/news/aistats-15.md/</link>
      <pubDate>Sun, 13 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/news/aistats-15.md/</guid>
      <description>Our paper Tradeoffs for Space, Time, Data and Risk in Unsupervised Learning received the Best Student Paper Award from the 18th International Conference on Artificial Intelligence and Statistics (AISTATS), 2015.</description>
    </item>
    
    <item>
      <title>Google Faculty Research Award</title>
      <link>/news/google-16.md/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/news/google-16.md/</guid>
      <description>Amin Karbasi received Google Faculty Research Award 2015:link
Awardees:
Adnan Darwiche, University of California, Los Angeles Alexandr Andoni, Columbia University Amin Karbasi, Yale University Amir Ali Ahmadi, Princeton University Animashree Anandkumar, University of California, Irvine Barna Saha, University of Massachusetts - Amherst Emmanuel Abbe, Princeton University Fei Sha, University of California, Los Angeles J. Zico Kolter, Carnegie Mellon University Lei Xing, Stanford University Li Ma, Duke University Marc Deisenroth, Imperial College London Mehryar Mohri, New York University Michela Milano, University of Bologna Raquel Urtasun, University of Toronto Stefanie Jegelka, Massachusetts Institute of Technology Anirban Dasgupta - Indian Institute of Technology Gandhinagar Ronojoy Adhikari - Institute of Mathematical Sciences Chennai </description>
    </item>
    
    <item>
      <title>Theoretical Challenges in Network Science</title>
      <link>/courses/fall-15.md/</link>
      <pubDate>Fri, 03 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>/courses/fall-15.md/</guid>
      <description>Course ID: ENAS 962 This is an interdisciplinary course with a focus on the emerging science of complex networks and their mathematical models. Students learn about the recent research on the structure and analysis of such networks, and on models that abstract their basic properties. Topics include random graphs and their properties, probabilistic techniques for link analysis, centralized and decentralized search algorithms, random walks, diffusion and epidemic processes, and spectral methods.</description>
    </item>
    
    <item>
      <title>Probability and Stochastic Procsses</title>
      <link>/courses/spring-15.md/</link>
      <pubDate>Sat, 03 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/courses/spring-15.md/</guid>
      <description>Course ID: ENAS 496 and 502 A study of stochastic processes and estimation, including fundamentals of detection and estimation. Vector space representation of random variables, Bayesian and Neyman-Pearson hypothesis testing, Bayesian and nonrandom parameter estimation, minimum-variance unbiased estimators, and the Cramer-Rao bound. Stochastic processes. Linear prediction and Kalman filtering. Poison counting process and renewal processes, Markov chains, branching processes, birth-death processes, and semi-Markov processes. Applications from communications, networking, and stochastic control.</description>
    </item>
    
    <item>
      <title>Creating a New Theme</title>
      <link>/posts/creating-a-new-theme/</link>
      <pubDate>Sun, 28 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/posts/creating-a-new-theme/</guid>
      <description>Introduction This tutorial will show you how to create a simple theme in Hugo. I assume that you are familiar with HTML, the bash command line, and that you are comfortable using Markdown to format content. I&amp;rsquo;ll explain how Hugo uses templates and how you can organize your templates to create a theme. I won&amp;rsquo;t cover using CSS to style your theme.
We&amp;rsquo;ll start with creating a new site with a very basic template.</description>
    </item>
    
    <item>
      <title>(Hu)go Template Primer</title>
      <link>/posts/hugo-template-primer/</link>
      <pubDate>Wed, 02 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/posts/hugo-template-primer/</guid>
      <description>Hugo uses the excellent go html/template library for its template engine. It is an extremely lightweight engine that provides a very small amount of logic. In our experience that it is just the right amount of logic to be able to create a good static website. If you have used other template systems from different languages or frameworks you will find a lot of similarities in go templates.
This document is a brief primer on using go templates.</description>
    </item>
    
    <item>
      <title>Getting Started with Hugo</title>
      <link>/posts/hugoisforlovers/</link>
      <pubDate>Wed, 02 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/posts/hugoisforlovers/</guid>
      <description>Step 1. Install Hugo Goto hugo releases and download the appropriate version for your os and architecture.
Save it somewhere specific as we will be using it in the next step.
More complete instructions are available at installing hugo
Step 2. Build the Docs Hugo has its own example site which happens to also be the documentation site you are reading right now.
Follow the following steps:
Clone the hugo repository Go into the repo Run hugo in server mode and build the docs Open your browser to http://localhost:1313 Corresponding pseudo commands:</description>
    </item>
    
    <item>
      <title>Migrate to Hugo from Jekyll</title>
      <link>/posts/migrate-from-jekyll/</link>
      <pubDate>Mon, 10 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/posts/migrate-from-jekyll/</guid>
      <description>Move static content to static Jekyll has a rule that any directory not starting with _ will be copied as-is to the _site output. Hugo keeps all static content under static. You should therefore move it all there. With Jekyll, something that looked like
‚ñæ &amp;lt;root&amp;gt;/ ‚ñæ images/ logo.png should become
‚ñæ &amp;lt;root&amp;gt;/ ‚ñæ static/ ‚ñæ images/ logo.png Additionally, you&amp;rsquo;ll want any files that should reside at the root (such as CNAME) to be moved to static.</description>
    </item>
    
    <item>
      <title>Theoretical Challenges in Network Science</title>
      <link>/courses/fall-14.md/</link>
      <pubDate>Fri, 03 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/courses/fall-14.md/</guid>
      <description>Course ID: ENAS 962 This is an interdisciplinary course with a focus on the emerging science of complex networks and their mathematical models. Students learn about the recent research on the structure and analysis of such networks, and on models that abstract their basic properties. Topics include random graphs and their properties, probabilistic techniques for link analysis, centralized and decentralized search algorithms, random walks, diffusion and epidemic processes, and spectral methods.</description>
    </item>
    
    <item>
      <title>IEEE Data Storage Best Student Paper Award</title>
      <link>/news/ieee-15.md/</link>
      <pubDate>Thu, 12 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>/news/ieee-15.md/</guid>
      <description>Our paper Noise-Enhanced Associative Memories received the IEEE Data Storage Best Student Paper Award.
IEEE Data Storage Best Student Paper Award for 2013
Awardees: Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi and Lar R. Varshney
Paper: ‚ÄúNoise-Enhanced Associative Memories,‚Äù Proceedings of Neural Information Processing Systems, pp. 1682‚Äì1690, 2013. NIPS Proceedings
Abstract: Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms have allowed reliable learning and recall of an exponential number of patterns.</description>
    </item>
    
    <item>
      <title>Graph-Based Information Processing: Scaling Laws and Applications</title>
      <link>/publications/thesis/karbasi-13e/</link>
      <pubDate>Thu, 03 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/thesis/karbasi-13e/</guid>
      <description>Abstract We live in a world characterized by massive information transfer and real-time communication. The demand for efficient yet low-complexity algorithms is widespread across different fields, including machine learning, signal processing and communications. Most of the problems that we encounter across these disciplines involves a large number of modules interacting with each other. It is therefore natural to represent these interactions and the ow of information between the modules in terms of a graph.</description>
    </item>
    
    <item>
      <title>About Hugo</title>
      <link>/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>This theme is created based on hugo-coder.
I made it possible to tell yourself more by my change.
Please see &amp;ldquo;FeaturesOfCoderPortfolio&amp;rdquo; in the post about the change. Regarding other demo contents, it is hugo-coder&amp;rsquo;s thing.
Have questions or suggestions? Feel free to open an issue on GitHub or ask me on Twitter.
Hugo is a static site engine written in Go.
It makes use of a variety of open source projects including:</description>
    </item>
    
    <item>
      <title>Amin Karbasi</title>
      <link>/people/amin-karbasi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/amin-karbasi/</guid>
      <description>Amin Karbasi is currently an associate professor of electrical engineering, computer science, and statistics and data science at Yale university. Prior to that he was a post-doctoral scholar at ETH Zurich, Switzerland (2013-2014). He obtained his Ph.D. (2012) and M.Sc. (2007) in computer and communication sciences from EPFL, Switzerland and his B.Sc. (2004) in electrical engineering from the same university.
Student Awards I am proudest of the recognitions my students/mentees received, including:</description>
    </item>
    
    <item>
      <title>Amin Karbasi</title>
      <link>/people/amin-karbasi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/amin-karbasi/</guid>
      <description>Amin Karbasi is currently an associate professor of electrical engineering, computer science, and statistics and data science at Yale university. Prior to that he was a post-doctoral scholar at ETH Zurich, Switzerland (2013-2014). He obtained his Ph.D. (2012) and M.Sc. (2007) in computer and communication sciences from EPFL, Switzerland and his B.Sc. (2004) in electrical engineering from the same university.
Student Awards I am proudest of the recognitions my students/mentees received, including:</description>
    </item>
    
    <item>
      <title>Chris Harshaw</title>
      <link>/people/harshaw.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/harshaw.md/</guid>
      <description>Chris Harshaw is a 6th year PhD student in the Computer Science department where he is advised by Amin Karbasi and Dan Spielman. His research focuses on developing algorithmic techniques for causal inference, with the goal of enabling experimenters and analysts to be more confident in their findings. From an algorithmic perspective, he works on spectral algorithms, submodular optimization, and non-convex programming. More broadly, his interests lie in optimization, statistics, and their intersection.</description>
    </item>
    
    <item>
      <title>Chris Xu</title>
      <link>/people/xu.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/xu.md/</guid>
      <description>He is a first year PhD student in Statistics and Data Science. He received his bachelor&amp;rsquo;s degree in Mathematics and Computer Science from Courant Institute of Mathematical Sciences, NYU. He completed his Master‚Äôs degree in Mathematics at Courant Institute under the supervision of Professor Afonso S. Bandeira. His research interest lies broadly in the intersection of optimization, statistics, information theory, computation, and machine learning.</description>
    </item>
    
    <item>
      <title>Ehsan Kazemi</title>
      <link>/people/kazemi.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/kazemi.md/</guid>
      <description>Ehsan Kazemi was Postdoc in the Department of Electrical Engineering and currently working in Google at Z√ºrich.</description>
    </item>
    
    <item>
      <title>Farzin Haddadpour</title>
      <link>/people/haddadpour.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/haddadpour.md/</guid>
      <description>Farzin is a Postdoctoral Associate in the Institute for Network Science. He obtained his PhD from EECS department at Pennsylvania State University, working on the topic of the fault-tolerant distributed algorithms for machine learning problems. He is graduated from Sharif University of Technology respectively. Prior to joining Pennsylvania State University, he was research assistant at the Information Engineering Department of the Chinese University of Hong Kong. His research interests are broadly in the areas of information and coding theory and its application in machine learning and distributed computing.</description>
    </item>
    
    <item>
      <title>Felix Zhou</title>
      <link>/people/felix.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/felix.md/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Grigoris Velegkas</title>
      <link>/people/grigoris.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/grigoris.md/</guid>
      <description>I am Ph.D. student in computer science at Yale University. Before joing Yale I was at National Technical University of Athens.</description>
    </item>
    
    <item>
      <title>Hunala: A Personalized Risk Assessment Tool for Respiratory Illness</title>
      <link>/hunala/hunala.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/hunala/hunala.md/</guid>
      <description>Visit Hunala project to get informed your daily risk of getting infected. Hunala Gives a personalized daily assessment on your risk of contracting respiratory diseases, including COVID-19, updated daily. Helps you keep yourself and your community safe by keeping you informed about the spread of respiratory disease
What Hunala Does Gives a personalized daily assessment on your risk of contracting respiratory diseases, including COVID-19, updated daily. Helps you keep yourself and your community safe by keeping you informed about the spread of respiratory disease.</description>
    </item>
    
    <item>
      <title>Insu Han</title>
      <link>/people/han.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/han.md/</guid>
      <description>I am a Postdoctoral Associate in the Institute for Network Science. I obtained my Ph.D. degree in the School of Electrical Engineering at Korea Advanced Institute of Science and Technology (KAIST), where I am advised by Jinwoo Shin. I recieved an M.S. in Electrical engineering and a B.S. in Electrical Engineering and Mathematics (minored) from KAIST. My research interests focus on approximate algorithm design and analysis for large-scale machine learning and its applications.</description>
    </item>
    
    <item>
      <title>Jane Lee</title>
      <link>/people/jane.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/jane.md/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Javid Dadashkarimi</title>
      <link>/people/dadashkarimi.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/dadashkarimi.md/</guid>
      <description>I‚Äôm Javid Dadashkarimi PhD candidate in computer science department at Yale University. I‚Äôm so grateful to work with Dustin Scheinost and Amin Karbasi in my PhD. Currently, I am interested in finding a mechanism to transfer human brain images between two atlases, with different resolutions, or with variable shapes: Check out my website for more information.
PhD, Computer Science Yale University
Advisor: Dustin Scheinost, Amin Karbasi
Thesis: Visualizing and Analysis of Functional Brain Connectomes</description>
    </item>
    
    <item>
      <title>Lin Chen</title>
      <link>/people/lin.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/lin.md/</guid>
      <description>Lin Chen is a postdoctoral scholar at the Simons Institute for the Theory of Computing, University of California, Berkeley . His research interests focus on machine learning theory. He was PhD student in the Department of Electrical Engineering. His research focuses on theoretical machine learning, including online optimization, submodular optimization, and adversarial robustness.
PhD Computer Science Yale University, New Haven, 2017-2020
Advisor: Professor Amin Karbasi Thesis: Online Optimization: Convex and Submodular Functions</description>
    </item>
    
    <item>
      <title>Mandy Singer</title>
      <link>/contact/contact.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contact/contact.md/</guid>
      <description>Mandy Singer is the administrative assistant for the Yale Institute for Network Science. Mandy has phenomenally varied experience, with talents as a repo-woman, cherry-picker operator, and is all-around competent. She has yet to be unable to do anything we have asked her to do.
Directions from Union Station to YINS building All you need is to drive the State St all the way up to Grove St and then turn left and follow up until Hillhouse Ave.</description>
    </item>
    
    <item>
      <title>Marko Mitrovic</title>
      <link>/people/marko.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/marko.md/</guid>
      <description>Marko was a Computer Science Ph.D. student advised by Amin Karbasi. His research focused on machine learning applications of submodularity. He is now at Google in Mountain View, California. His personal website can be found here.</description>
    </item>
    
    <item>
      <title>Mehraveh Salehi</title>
      <link>/people/salehi.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/salehi.md/</guid>
      <description>Mehraveh is currently Chief Experience Officer at Summary Analytics. She is interested in combinatorial optimization and machine learning algorithms with a focus on submodularity and its applications in the human brain. Her Ph.D. thesis focused on developing predictive models that relate human behavior to individual brain functional connectivity patterns as measured by fMRI. She enjoys playing Santour (a Persian music instrument), watching movies, and running.</description>
    </item>
    
    <item>
      <title>Mingrui Zhang</title>
      <link>/people/minguri.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/minguri.md/</guid>
      <description>Mingrui was a Ph.D. student in the Department of Statistics and Data Science. His research interest is theoretical machine learning and optimization, including online learning, submodular maximization, and projection-free optimization methods. Prior to joining Yale, he received his Bachelor&amp;rsquo;s degree in math and applied math from Peking University. He likes reading books and watching movies. He is also a supporter of Liverpool FC.</description>
    </item>
    
    <item>
      <title>Mohammad Shadravan</title>
      <link>/people/shadravan.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/shadravan.md/</guid>
      <description>Mohammad was a Postdoctoral Associate in the Institute for Network Science. He earned his PhD from Columbia University under the supervision of Professor Clifford Stein and Shipra Agrawal. His research interests lie primarily in the area of submodulariy in machine learning, algorithms for massive dataset, and optimization for sequential decision making. He completed his Master‚Äôs degree in the department of Combinatorics and Optimization at the University of Waterloo. Before that, he obtained his B.</description>
    </item>
    
    <item>
      <title>Outreach</title>
      <link>/outreach/outreach.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/outreach/outreach.md/</guid>
      <description>Tutorials ICML 2020 Tutorial on Submodular Optimization: From Discrete to Continuous and Back ISIT 2018 tutorial on ‚ÄúSubmodularity in Information and Data Science‚Äù. Videos: Part One Slides: Part One, Part Two. CVPR 2018 tutorial ‚ÄúBig Data Summarization: Algorithms and Applications‚Äù in CVPR 2018. Videos: Part One, Part Two. Slides: Part One, Part Two. Workshops Amin Karbasi joint with Yaron Singer, Jeff A Bilmes, Andreas Krause, and Stefanie Jegelka, organized Discrete Structures in Machine Learning workshop at NIPS 2017.</description>
    </item>
    
    <item>
      <title>Peiyuan Zhang</title>
      <link>/people/peiyuan.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/peiyuan.md/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Siddharth Mitra</title>
      <link>/people/siddhart.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/siddhart.md/</guid>
      <description>I am a student of mathematics and computer science broadly interested in optimization, machine learning, and theoretical computer science. My recent interests surround online optimization and I have been working on designing adaptive algorithms for various online learning problems. I also have a soft spot for physics ‚Äì I think it‚Äôs an all-round really fun subject and also provides tons of intuition.
In Fall 2020, I will be starting my PhD in CS at Yale.</description>
    </item>
    
    <item>
      <title>Software</title>
      <link>/software/software.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/software/software.md/</guid>
      <description>We highly believe in open and reproducible science. To this end, we try our best to publicly release our software and data sets.
Paper Submodular Maximization beyond Non-negativity: Guarantees, Fast Algorithms, and Applications:
Code is released here Paper: Streaming Weak Submodularity: Interpreting Neural Networks on the Fly
Code is released here. Paper: Probabilistic Submodular Maximization in Sub-Linear Time
Code is released here Paper: Differentially Submodular Maximization: Data Summarization in Disguise</description>
    </item>
    
    <item>
      <title>Yifei Min</title>
      <link>/people/yifei.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/yifei.md/</guid>
      <description>Yifei Min is a 2nd year PhD in Statistics and Data Science. His research interest is in theoretical machine learning and optimization. He likes tennis and movies.</description>
    </item>
    
  </channel>
</rss>
