+++
title = "ICML 2020 Tutorial on Submodular Optimization: From Discrete to Continuous and Back"
slug = "icml-20.md"
author="Hamed Hassani (UPenn), Amin Karbasi (Yale)"
description = "news"
date = "2020-05-30"
+++

![math fest 1](/images/icml-2020/Hassani.jpg "hassani")
![math fest 1](/images/icml-2020/karbasi.jpeg "karbasi") 


1.  [Hamed Hassani](https://www.seas.upenn.edu/~hassani/)
> School of Engineering and Applied Sciences\
> University of Pennsylvania\
> Philadelphia, PA 19104

2. [Amin Karbasi](http://iid.yale.edu/people/amin-karbasi)
> Yale Institute for Network Science \
> Yale University \
> New Haven, CT 06520

# Slides
 
1. [Module 1](/files/icml-2020/module1.pdf)
2. [Module 2](/files/icml-2020/module2.pdf)
3. [Module 3](/files/icml-2020/module3.pdf)
4. [Module 4](/files/icml-2020/module4.pdf)

# Videos
 
1. [Part I](https://slideslive.com/38930508/submodular-optimization-from-discrete-to-continuous-and-back-part-i?ref=account-folder-55826-folders)
2. [Part II](https://slideslive.com/38930509/submodular-optimization-from-discrete-to-continuous-and-back-part-ii?ref=account-folder-55826-folders)
3. [Part III](https://slideslive.com/38930506/submodular-optimization-from-discrete-to-continuous-and-back-part-iii?ref=account-folder-55826-folders)
4. [Part IV](https://slideslive.com/38930507/submodular-optimization-from-discrete-to-continuous-and-back-part-iv?ref=account-folder-55826-folders)

# Brief Description and Outline

This tutorial will cover recent advancements in discrete optimization methods for large-scale machine learning. Traditionally, machine learning has been harnessing convex optimization to design
fast algorithms with provable guarantees for a broad range of applications. In recent years, however,
there has been a surge of interest in applications that involve discrete optimization. For discrete
domains, the analog of convexity is considered to be submodularity, and the evolving theory of
submodular optimization has been a catalyst for progress in extraordinarily varied application
areas including active learning and experimental design, vision, sparse reconstruction, graph inference, video analysis, clustering, document summarization, object detection, information retrieval,
network inference, interpreting neural network, and discrete adversarial attacks.

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;![math fest 1](/images/icml-2020/submodular.png "submodular") 

As applications and techniques of submodular optimization mature, a fundamental gap between
theory and application emerges. In the past decade, paradigms such as large-scale learning, distributed systems, and sequential decision making have enabled a quantum leap in the performance
of learning methodologies. Incorporating these paradigms in discrete problems has led to fundamentally new frameworks for submodular optimization. The goal of this tutorial is to cover rigorous
and scalable foundations for discrete optimization in complex, dynamic environments, addressing
challenges of scalability and uncertainty, and facilitating distributed and sequential learning in
broader discrete settings. More specifically, we will cover advancements in four areas:

1. ## Submodular Optimization with Perfect Information. 
State-of-the-art submodular optimization techniques have mostly assumed perfect knowledge about the optimization problem
and the environment. More precisely, the existing techniques have been designed based on
the availability of an oracle with full information about the objective function at any queried
point. In this context, recent work in submodular optimization has widely focused on developing fast, efficient, and provable methodologies in the presence of massive, high-dimensional
data:

*  Fast Greedy Methods. The first class of methods consider cases where the size and
dimension of the data is large, but data can be fully accessible and stored in memory.
Accordingly, fast greedy algorithms have been proposed recently with almost optimal
run-time efficiency in terms of the query complexity and dimension of data while maintaining optimal quality guarantees in terms of objective value [1, 2, 3, 4,49, 50-61](/icml/icml-20.md/#1).

* Streaming Algorithms: In cases where the data sets are too large to be stored in
memory, we consider streaming techniques, where computation is performed on the 
y.
A recent line of work has been focused on developing algorithms that obtain desirable
guarantees for important tasks in machine learning while using modest memory resources
[5, 6, 7, 8, 9,62-68](/icml/icml-20.md/#5).

*  Distributed Optimization: The rise of distributed computation frameworks such
as MapReduce, Hadoop, and Spark, enabled unprecedented advancement in machine
1
learning. Although submodular optimization is a canonical example of computation that
cannot be parallelized, there is a great deal of work on new algorithm design approaches
that can utilize distributed computational resources [10, 11, 12, 13, 14, 15, 16,](/icml/icml-20.md/#10)[69-77](/icml/icml-20.md/#69).


2. ## Submodular Optimization with Imperfect Information 
 In many modern applications, perfect-information oracles can not assumed, and instead, we resort to approximate or
imperfect ones in order to address challenges such as scalability, uncertainty in data and models, as well as dynamic and adversarial environments. We will formalize oracles with imperfect
information through three main classes{Stochastic, Online, and Bandit} each of which opens
a new avenue in the field of discrete (submodular) optimization and requires fundamentally
novel techniques:

* The Stochastic Oracle. The stochastic oracle returns a stochastic, but unbiased estimate of the function value at any query point. Given this type of oracle access to the
function values, the primary question is to develop stochastic submodular optimization
techniques with minimal sample complexity (i.e. the total number of queries from the
stochastic oracle) and computation complexity [17, 18](/icml/icml-20.md/#17). The stochastic oracle is motivated form practical and large-scale applications and covers popular instances of discrete
optimization: (i) Oftentimes in practice the objective is defined in terms of an empirical
risk, i.e., through a finite sum of loss functions associated to the data points. This formulation appears in submodular applications such as data summarization [19, 20, 21](/icml/icml-20.md/#19),
recommender systems [22, 18](/icml/icml-20.md/#22), and sparse regression [23, 24, 25](/icml/icml-20.md/#23) etc. (ii) In some other
cases, the objective is given as an expectation over an explicit stochastic model which is
hard/intractable to compute. For example, in influence maximization in social networks
[26](/icml/icml-20.md/#26),the objective value is defned as the expectation of a stochastic difiusion process,
quantifying the expected fraction of the network influenced from a selected seed set.

* Online/Bandit Oracle. This type of oracle is the model of choice when the optimizer/learner has to interact with an unknown, evolving, or even adversarially changing
environment. The oracle's outcome in this setting varies with time, and depending on
how limited the oracle's outcome is, we can define two different scenarios: online, and
bandit settings. The primary optimization goal in this setting is to provide no-regret
mechanisms to maximize the cumulative objective over time. Online and Bandit submodular optimization, introduced in [27, 28, 29](/icml/icml-20.md/#27), arises naturally in discrete applications
involving dynamic environments, e.g. online/dynamic resource allocation [30, 27](/icml/icml-20.md/#23), online
ranking [31, 32](/icml/icml-20.md/#31), sensor placement in unknown or adversarial environments [33](/icml/icml-20.md/#33), in
uencing dynamic social networks [34](/icml/icml-20.md/#34), online recommender systems [35, 36](/icml/icml-20.md/#35), and dynamic
multi-robot coverage problems [37](/icml/icml-20.md/#37).

<!--&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;![math fest 1](/images/icml-2020/bandit.png "bandit")--> 

3. ## A Non-Convex Bridge Between Discrete and Continuous Optimization
To address
discrete otimization with imperfect oracles, we will describe a recently developed framework
which bridges discrete and continuous optimization. Through appropriate continuous extensions, this framework devises algorithms that succeed in finding good solutions by making
a large number of small, exploratory steps using noisy estimates of the objective value, and
moving toward the optimal solution. As we will discuss, solving the resulting continuous problems requires novel methods beyond the state-of-the-art, because they are highly non-convex
and possess undesirable stationary points. We show how tools from discrete and continuous
optimization as well as techniques that exploit the special structure (submodularity) admitted
by these problems and avoid those bad stationary points [18](/icml/icml-20.md/#18) [38, 39, 40, 41, 42, 43](/icml/icml-20.md/#38).
Indeed, submodular optimization is inherently related to continuous optimization as many
submodular optimization methods rely on continuous relaxations. This connection has recently been strengthen by introducing continuous submodular functions [44, 45, 78](/icml/icml-20.md/#44), [18](/icml/icml-20.md/#18). Such
functions are not convex (nor concave) but still allow finding near-optimal solutions, thus
providing a rich framework for studying non-convex optimization problems.

4. ## Beyond Submodularity
Finally, one can generalize the notion of submodularity and still
provide approximation guarantees. Several of such generalizations, e.g., adaptive submodularity [46, 47](/icml/icml-20.md/#46), weak submodularity [25](/icml/icml-20.md/#25), [9](/icml/icml-20.md/#9), two-stage submodularity [48](/icml/icml-20.md/#48), etc, have been
recently proposed. These generalizations extend the applications of submodularity to interactive settings, dictionary learning, and sparse recovery.

<!--1. B. Mirzasoleiman, A. Badanidiyuru, A. Karbasi, J. Vondrak, and A. Krause, "Lazier than lazy greedy,"
in AAAI, 2015.
2. B. Mirzasoleiman, A. Badanidiyuru, and A. Karbasi, "Fast constrained submodular maximization:
Personalized data summarization," in ICML, 2016.
3. N. Buchbinder, M. Feldman, J. Naor, and R. Schwartz, "Submodular maximization with cardinality
constraints.," in SODA, pp. 1433{1452, 2014.
4. K. Wei, R. Iyer, and J. Bilmes, "Fast multi-stage submodular maximization: Extended version," 2014.
5. A. Badanidiyuru, B. Mirzasoleiman, A. Karbasi, and A. Krause, "Streaming submodular maximization: Massive data summarization on the fly," in Proceedings of the 20th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD), 2014.
6. E. D. Demaine, P. Indyk, S. Mahabadi, and A. Vakilian, "On streaming and communication complexity
of the set cover problem," in International Symposium on Distributed Computing, pp. 484{498, Springer,2014.
7. C. Chekuri, S. Gupta, and K. Quanrud, Streaming algorithms for submodular function maximization," in ICALP, pp. 318{330, 2015.
8. N. Buchbinder, M. Feldman, and R. Schwartz, Online Submodular Maximization with Preemption," in SODA, pp. 1202{1216, 2015.
9. E. R. Elenberg, A. G. Dimakis, M. Feldman, and A. Karbasi, "Streaming weak submodularity: Interpreting neural networks on the fly," arXiv preprint arXiv:1703.02647, 2017.
10. B. Mirzasoleiman, A. Karbasi, R. Sarkar, and A. Krause, "Distributed submodular maximization,"Journal of Machine Learning Research (JMLR), 2016.
11. B. Mirzasoleiman, A. Karbasi, A. Badanidiyuru, and A. Krause, "Distributed submodular cover: Succinctly summarizing massive data," in NIPS, 2015.
12. B. Mirzasoleiman, M. Zadimoghaddam, and A. Karbasi, "Fast distributed submodular cover: Publicprivate data summarization," in NIPS, 2016.
13. B. Mirzasoleiman, A. Karbasi, R. Sarkar, and A. Krause, "Distributed submodular maximization:
Identifying representative elements in massive data," in NIPS, 2013.
14. V. Mirrokni and M. Zadimoghaddam, "Randomized composable core-sets for distributed submodular
maximization," in Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, pp. 153{162, ACM, 2015.
15. R. Kumar, B. Moseley, S. Vassilvitskii, and A. Vattani, "Fast greedy algorithms in mapreduce and
streaming," in SPAA, 2013.
16. R. Barbosa, A. Ene, H. L. Nguyen, and J. Ward, "A new framework for distributed submodular maximization," in FOCS, pp. 645{654, 2016.
17. M. Karimi, M. Lucic, H. Hassani, and A. Krasue, "Stochastic submodular maximization: The case for
coverage functions," in NIPS, 2017.
18. H. Hassani, M. Soltanolkotabi, and A. Karbasi, "Gradient methods for submodular maximization," in
Advances in Neural Information Processing Systems (NIPS), 2017.
19. H. Lin and J. Bilmes, "A class of submodular functions for document summarization," in Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies-Volume 1, pp. 510{520, Association for Computational Linguistics, 2011.
20. K. Kirchhofi and J. Bilmes, "Submodularity for data selection in statistical machine translation," in
EMNLP, pp. 131{141, 2014.
21. R. Sipos, A. Swaminathan, P. Shivaswamy, and T. Joachims, "Temporal corpus summarization using
submodular word coverage," in CIKM, pp. 754{763, ACM, 2012.
22. M. Feldman, C. Harshaw, and A. Karbasi, "Greed is good: Near-optimal submodular maximization via
greedy optimization," Conference on Learning Theory (COLT), 2017.
23. A. Krause and V. Cevher, "Submodular dictionary selection for sparse representation," in International
Conference on Machine Learning (ICML), no. CONF, 2010.
24. A. Das and D. Kempe, "Submodular meets spectral: Greedy algorithms for subset selection, sparse
approximation and dictionary selection," ICML, 2011.
25. E. R. Elenberg, R. Khanna, A. G. Dimakis, and S. Negahban, "Restricted strong convexity implies
weak submodularity," arXiv preprint arXiv:1612.00804, 2016.
26. D. Kempe, J. Kleinberg, and E. Tardos, "Maximizing the spread of in
uence through a social network,"
9th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 137{146, 2003.
5
27. M. Streeter and D. Golovin, "An online algorithm for maximizing submodular functions," in Advances
in Neural Information Processing Systems, pp. 1577{1584, 2009.
28. Y. Yue and C. Guestrin, "Linear submodular bandits and their application to diversified retrieval," in
Advances in Neural Information Processing Systems, pp. 2483{2491, 2011.
29. S. Jegelka and J. A. Bilmes, "Online submodular minimization for combinatorial structures," in International Conference on Machine Learning (ICML), (Bellevue, Washington), 2011.
30. D. Golovin, A. Krause, B. Gardner, S. J. Converse, and S. Morey, "Dynamic resource allocation in
conservation planning," in Twenty-fifth AAAI Conference on Artificial Intelligence, 2011.
31. A. Guillory and J. A. Bilmes, "Online submodular set cover, ranking, and repeated active learning," in
Advances in Neural Information Processing Systems, pp. 1107{1115, 2011.
32. A. Slivkins, F. Radlinski, and S. Gollapudi, "Learning optimally diverse rankings over large document
collections," 2010.
33. A. Krause, A. Roper, and D. Golovin, "Randomized sensing in adversarial environments," in TwentySecond International Joint Conference on Artificial Intelligence, 2011.
34. H. Zhuang, Y. Sun, J. Tang, J. Zhang, and X. Sun, "In
uence maximization in dynamic social networks,"
in 2013 IEEE 13th International Conference on Data Mining, pp. 1313{1318, IEEE, 2013.
35. D. Golovin, A. Krause, and M. Streeter, "Online submodular maximization under a matroid constraint
with application to learning assignments," arXiv preprint arXiv:1407.1082, 2014.
36. A. Ashkan, B. Kveton, S. Berkovsky, and Z. Wen, "Optimal greedy diversity for recommendation," in
Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015.
37. M. Corah and N. Michael, "Eficient online multi-robot exploration via distributed sequential greedy
assignment.," in Robotics: Science and Systems, 2017.
38. A. Mokhtari, H. Hassani, and A. Karbasi, "Stochastic conditional gradient methods: From convex
minimization to submodular maximization," arXiv preprint arXiv:1804.09554, 2018.
39. L. Chen, C. Harshaw, H. Hassani, and A. Karbasi, "Projection-free online optimization with stochastic
gradient: From convexity to submodularity," ICML, 2018.
40. L. Chen, H. Hassani, and A. Karbasi, "Online continuous submodular maximization," AISTATS, 2018.
41. H. Hassani, A. Karbasi, A. Mokhtari, and Z. Shen, "Stochastic conditional gradient++," arXiv preprint
arXiv:1902.06992, 2019.
42. R. Niazadeh, T. Roughgarden, and J. Wang, "Optimal algorithms for continuous non-monotone submodular and dr-submodular maximization," in Advances in Neural Information Processing Systems,
pp. 9594{9604, 2018.
43. A. Bian, K. Levy, A. Krause, and J. M. Buhmann, "Continuous dr-submodular maximization: Structure
and algorithms," in Advances in Neural Information Processing Systems, pp. 486{496, 2017.
44. F. Bach, "Submodular functions: from discrete to continuous domains," arXiv preprint
arXiv:1511.00394, 2015.
45. A. Bian, B. Mirzasoleiman, J. M. Buhmann, and A. Krause, "Guaranteed non-convex optimization:
Submodular maximization over continuous domains," arXiv preprint arXiv:1606.05615, 2016.
46. D. Golovin and A. Krause, "Adaptive submodularity: Theory and applications in active learning and
stochastic optimization," Journal of Artificial Intelligence Research, vol. 42, pp. 427{486, 2011.
47. Y. Chen, S. Javdani, A. Karbasi, J. A. Bagnell, S. S. Srinivasa, and A. Krause, "Submodular surrogates
for value of information.," in AAAI, 2015.
48. E. Balkanski, A. Krause, B. Mirzasoleiman, and Y. Singer, "Learning sparse combinatorial representations via two-stage submodular maximization," in Proceedings of The 33rd International Conference on
Machine Learning, 2016.-->
