+++
date = "2021-09-01"
title = "Multiple Descent: Design Your Own Generalization Curve"
math = "true"
description = "publication"
publish = "NeurIPS"
category = "conference"
weight=2
author = "Lin Chen, Yifei Min, Mikhail Belkin, Amin Karbasi"
link = "https://arxiv.org/pdf/2008.01036.pdf"
keyword3="submodular optimization"
+++

# Abstract

This paper explores the generalization loss of linear regression in variably parameter- ized families of models, both under-parameterized and over-parameterized. We show that the generalization curve can have an arbitrary number of peaks, and moreover, locations of those peaks can be explicitly controlled. Our results highlight the fact that both classical U-shaped generalization curve and the recently observed double descent curve are not intrinsic properties of the model family. Instead, their emergence is due to the interaction between the properties of the data and the inductive biases of learning algorithms.
