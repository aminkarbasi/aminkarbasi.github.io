<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
<meta charset="utf-8">
<title>
    Publications &ndash; Javid Dadashkarimi
  </title>
<meta name="author" content="Monica Dinculescu" />
<meta name="description" content="Monica Dinculescu's blog" />
<meta name="viewport" content="width=device-width, minimum-scale=1, initial-scale=1, user-scalable=yes">
<link rel="alternate" type="application/rss+xml" href="atom.xml" />
<link rel="stylesheet" href="css/base.css" type="text/css" media="screen, projection" />
<link rel="shortcut icon" type="image/x-icon" href="images/favicon.png" />
<link rel="icon" type="image/png" href="images/favicon.png">
<link rel="manifest" href="manifest.json">

<link rel="manifest" href="https://meowni.ca/makes/manifest.json">

<meta name="theme-color" content="#7BE4D5">

<meta name="mobile-web-app-capable" content="yes">
<meta name="application-name" content="Monica's blog">

<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="apple-mobile-web-app-title" content="Monica's blog">

<link rel="apple-touch-icon" sizes="192x192" href="https://meowni.ca/makes/icons/icon-192x192.png">

<meta name="msapplication-TileImage" content="images/manifest/icon-192x192.png">
<meta name="msapplication-TileColor" content="#7BE4D5">
<meta name="msapplication-tap-highlight" content="no">

<meta name="twitter:card" content="Monica Dinculescu's blog">
<meta name="twitter:site" content="@notwaldorf">
<meta property="og:type" content="website">
<meta property="og:site_name" content="my-app">
<meta property="og:image" content="icons/icon-192x192.png" />
<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Rubik:wght@400;700&display=swap" rel="stylesheet">
<script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-47334320-2']);
    _gaq.push(['_setDomainName', 'meowni.ca']);
    _gaq.push(['_trackPageview']);
    (function () {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
</head>

<body>
<nav>
<ul>
<li id="home_link" class="og"><a href="index.html">üè†</a></li>
<li><a href="people.html">People</a></li>
  <li><a href="pubs.html">Publications</a></li>
<li><a href="talks.html">Talks</a></li>
<li class="news"><a href="news.html">News</a></li>
</nav>
<div class="splash">
<div class="splash-wrapper">
<div class="splash-copy">
<h1>Publications</h1>
<p> For an up-to-date listing, see the following <a href="https://scholar.google.com/citations?user=VusVB38AAAAJ&hl=en">Google Scholar page</a>.</p>
</div>
<div id="birbs">
  <img src="figs/bookstore.png" alt="Church Doors" class="left-image" id="text-image">


  </div>
</div>
</div>
<div class="dots"></div>
</div>
<section class="content">
<style>
  p {
    font-size: var(--font-size-s);
  }
  .content .listing.table li > a {
    flex-shrink: 0;
    flex-grow: 0;
    width: 40%;
  }
  .content .listing.table li > div {
    flex-grow: 1;
    text-transform: none;
    letter-spacing: 0;
    margin-left: var(--spacing-xs);
  }
  .content .listing.table li > div:before {
    content: '‚Üí ';
    font-weight:bold;
  }
  @media screen and (max-width:600px) {
    .content .listing.table li {
      flex-wrap: wrap;
      justify-content: flex-start;
      margin-bottom: var(--spacing-m);
    }
    .content .listing.table li > div {
      margin-left: 0;
    }
    .content .listing.table li > a {
      width: 100% !important;
      flex-shrink: 0;
    }
  }
</style>

<style>
table {
  display: grid;
  border-collapse: collapse;
  gap: 4px;
  max-width: 100%;
  overflow: scroll;
}
table.small {
  grid-template-columns: 1fr 8fr;
}
table.all {
  grid-template-columns: 2.5fr 2fr 1.6fr .5fr;
}

thead,
tbody,
tr {
  display: contents;
}

th,
td {
  font-size: 14px;
}
td.small {
  font-size: 14px;
}
td.center {
  text-align: center;
}
th {
  border-bottom: 3px solid black;
  text-align: left;
  font-size: 16px;
}
table.all td {
  padding-top: 10px;
  padding-bottom: 10px;
}
td a:link, td a:visited {
  color: black;
  text-decoration-color: var(--red);
  text-decoration-thickness: .125em;
}
tr.full {
  display: none;
}
tr.full.show {
  display: contents;
}
tr.full td {
  grid-column: 1 / -1;
  padding: 20px 40px;
  border-left: 3px solid #ddd;
  font-size: 16px;
  line-height: 1.5;
}
button {
  background: transparent;
  font-family: inherit;
  font-size: 14px;
  margin: 0;
  padding: 0;
  border: none;
  cursor: pointer;
  font-weight: bold;
  border-bottom: 2px solid var(--red);
  color: black;
}
tr.full td button {
  margin-right: 24px;
}
@media screen and (max-width:600px) {
  table.all {
    grid-template-columns: 1fr 1fr 1.5fr 0.7fr;
  }
  tr.full td {
    max-width: 80%;
    padding: 16px;
  }
}

</style>
<script>
  function toggleReview(el) {
    const tr = el.parentElement.parentElement;
    const nextTr = tr.nextElementSibling;
    nextTr.classList.toggle('show');
  }
  function hideReview(el) {
    const tr = el.parentElement.parentElement;
    tr.classList.remove('show');
  }
  function copyReview(el) {
    const tr = el.parentElement.parentElement;
    const copyText = tr.querySelector('div').textContent;
    navigator.clipboard.writeText(copyText);
  }
</script>
<section class="content">
    <p>
    </p>
</section>



<br/>
<br/>

<table class="all">
<thead>
<tr>
<th>Title</th>
<th>Author</th>
<th>Journal</th>
<th>Year</th>
</tr>
</thead>
<tbody>

  <tr>
    <td><a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841524002834"> A Flow-based Truncated Denoising Diffusion Model for super-resolution Magnetic Resonance Spectroscopic Imaging</a></td>
    <td>Dong, Siyuan and Cai, Zhuotong and Hangel, Gilbert and Bogner, Wolfgang and Widhalm, Georg and Huang, Yaqing and Liang, Qinghao and You, Chenyu and Kumaragamage, Chathura and Fulbright, Robert K and others</td>
    <td>Medical Image Analysis</td>
    <td>2025</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2406.05660">Injecting Undetectable Backdoors in Obfuscated Neural Networks and Language Models</a></td>
    <td>Alkis Kalavasis, Amin Karbasi, Argyris Oikonomou, Katerina Sotiraki, Grigoris Velegkas, Manolis Zampetakis</td>
    <td>Neural Information Processing Systems</td>
    <td>2024</td>
  </tr>

  <tr>
    <td><a href="https://arxiv.org/pdf/2102.13028">Batched neural bandits</a></td>
    <td>Quanquan Gu, Amin Karbasi, Khashayar Khosravi, Vahab Mirrokni, and Dongruo Zhou</td>
    <td>ACM/JMS Journal of Data Science</td>
    <td>2024</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2302.10359">Replicable clustering</a></td>
    <td>Hossein Esfandiari, Amin Karbasi, Vahab Mirrokni, Grigoris Velegkas, Felix Zhou</td>
    <td>Advances in Neural Information Processing Systems</td>
    <td>2024</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2402.06082">SubGen: Token Generation in Sublinear Time and Memory</a></td>
    <td>Amir Zandieh, Insu Han, Vahab Mirrokni, Amin Karbasi</td>
    <td>arXiv preprint arXiv:2402.06082</td>
    <td>2024</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2402.13857">Replicable learning of large-margin halfspaces</a></td>
    <td>Alkis Kalavasis, Amin Karbasi, Kasper Green Larsen, Grigoris Velegkas, Felix Zhou</td>
    <td>arXiv preprint arXiv:2402.13857</td>
    <td>2024</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2305.16903">Submodular minimax optimization: Finding effective sets</a></td>
    <td>Loay Mualem, Ethan R. Elenberg, Moran Feldman, Amin Karbasi</td>
    <td>International Conference on Artificial Intelligence and Statistics</td>
    <td>2024</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2301.09627">The impossibility of parallelizing boosting</a></td>
    <td>Amin Karbasi, Kasper Green Larsen</td>
    <td>International Conference on Algorithmic Learning Theory</td>
    <td>2024</td>
</tr>
<tr>
    <td><a href="https://proceedings.mlr.press/v247/attias24a/attias24a.pdf">Universal Rates for Regression: Separations between Cut-Off and Absolute Loss</a></td>
    <td>Attias, Idan; Hanneke, Steve; Kalavasis, Alkis; Karbasi, Amin; Velegkas, Grigoris</td>
    <td>The Thirty Seventh Annual Conference on Learning Theory</td>
    <td>2024</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2310.17759">Optimal guarantees for algorithmic reproducibility and gradient complexity in convex optimization</a></td>
    <td>Liang Zhang, Junchi Yang, Amin Karbasi, Niao He</td>
    <td>Advances in Neural Information Processing Systems 36</td>
    <td>2024</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2406.05660">Injecting Undetectable Backdoors in Deep Learning and Language Models</a></td>
    <td>Alkis Kalavasis, Amin Karbasi, Argyris Oikonomou, Katerina Sotiraki, Grigoris Velegkas, Manolis Zampetakis</td>
    <td>arXiv preprint arXiv:2406.05660</td>
    <td>2024</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2405.15599">On the Computational Landscape of Replicable Learning</a></td>
    <td>Alkis Kalavasis, Amin Karbasi, Grigoris Velegkas, Felix Zhou</td>
    <td>arXiv preprint arXiv:2405.15599</td>
    <td>2024</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2411.13513">Procurement Auctions via Approximately Optimal Submodular Optimization</a></td>
    <td>Yuan Deng, Amin Karbasi, Vahab Mirrokni, Renato Paes Leme, Grigoris Velegkas, Song Zuo</td>
    <td>arXiv preprint arXiv:2411.13513</td>
    <td>2024</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2410.11303">TSDS: Data Selection for Task-Specific Model Finetuning</a></td>
    <td>Zifan Liu, Amin Karbasi, Theodoros Rekatsinas</td>
    <td>arXiv preprint arXiv:2410.11303</td>
    <td>2024</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2410.11272">Cognitive Overload Attack: Prompt Injection for Long Context</a></td>
    <td>Bibek Upadhayay, Vahid Behzadan, Amin Karbasi</td>
    <td>arXiv</td>
    <td>2024</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2410.02536">Intelligence at the Edge of Chaos</a></td>
    <td>Shiyang Zhang, Aakash Patel, Syed A Rizvi, Nianchen Liu, Sizhuang He, Amin Karbasi, Emanuele Zappala, David van Dijk</td>
    <td>arXiv</td>
    <td>2024</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2410.11303">TSDS: Data Selection for Task-Specific Model Finetuning</a></td>
    <td>Zifan Liu, Amin Karbasi, Theodoros Rekatsinas</td>
    <td>arXiv preprint arXiv:2410.11303</td>
    <td>2024</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2409.12367">Extracting Memorized Training Data via Decomposition</a></td>
    <td>Ellen Su, Anu Vellore, Amy Chang, Raffaele Mura, Blaine Nelson, Paul Kassianik, Amin Karbasi</td>
    <td>arXiv</td>
    <td>2024</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2312.02119">Tree of Attacks: Jailbreaking Black-Box LLMs Automatically</a></td>
    <td>Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, Amin Karbasi</td>
    <td>arXiv preprint arXiv:2312.02119</td>
    <td>2023</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2302.02451">KDeformer: Accelerating Transformers via Kernel Density Estimation</a></td>
    <td>Amir Zandieh, Insu Han, Majid Daliri, Amin Karbasi</td>
    <td>International Conference on Machine Learning</td>
    <td>2023</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2310.05869">Hyperattention: Long-context Attention in Near-linear Time</a></td>
    <td>Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David P. Woodruff, Amir Zandieh</td>
    <td>arXiv preprint arXiv:2310.05869</td>
    <td>2023</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2305.14311">Statistical Indistinguishability of Learning Algorithms</a></td>
    <td>Alkis Kalavasis, Amin Karbasi, Shay Moran, Grigoris Velegkas</td>
    <td>International Conference on Machine Learning</td>
    <td>2023</td>
</tr>
<tr>
    <td><a href="https://www.biorxiv.org/content/10.1101/2023.09.12.557460v1.full.pdf">BrainLM: A Foundation Model for Brain Activity Recordings</a></td>
    <td>Josue Ortega Caro, Antonio H. de O. Fonseca, Christopher Averill, Syed A. Rizvi, Matteo Rosati, James L. Cross, Prateek Mittal, Emanuele Zappala, Daniel Levine, Rahul M. Dhodapkar, Chadi G. Abdallah, David van Dijk</td>
    <td>bioRxiv</td>
    <td>2023</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2307.03848">Optimal Learners for Realizable Regression: PAC Learning and Online Learning</a></td>
    <td>Idan Attias, Steve Hanneke, Alkis Kalavasis, Amin Karbasi, Grigoris Velegkas</td>
    <td>Advances in Neural Information Processing Systems 36</td>
    <td>2023</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2009.13998">How Do You Want Your Greedy: Simultaneous or Repeated?</a></td>
    <td>Moran Feldman, Christopher Harshaw, Amin Karbasi</td>
    <td>Journal of Machine Learning Research 24 (72)</td>
    <td>2023</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2305.19562">Replicability in Reinforcement Learning</a></td>
    <td>Amin Karbasi, Grigoris Velegkas, Lin F. Yang, Felix Zhou</td>
    <td>Advances in Neural Information Processing Systems 36</td>
    <td>2023</td>
</tr>
<tr>
    <td><a href="https://proceedings.mlr.press/v206/lee23b/lee23b.pdf">Exact Gradient Computation for Spiking Neural Networks via Forward Propagation</a></td>
    <td>Jane H. Lee, Saeid Haghighatshoar, Amin Karbasi</td>
    <td>International Conference on Artificial Intelligence and Statistics</td>
    <td>2023</td>
</tr>
<tr>
    <td><a href="https://www.biorxiv.org/content/10.1101/2023.09.11.557287v1.full.pdf">Cell2sentence: Teaching Large Language Models the Language of Biology</a></td>
    <td>Daniel Levine, Syed Asad Rizvi, Sacha L√©vy, Nazreen Pallikkavaliyaveetil, Ruiming Wu, Zihe Zheng, Antonio Oliveira Fonseca, Xingyu Chen, Sina Ghadermarzi, Rahul M. Dhodapkar, David van Dijk</td>
    <td>bioRxiv</td>
    <td>2023</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2306.08803">Langevin Thompson Sampling with Logarithmic Communication: Bandits and Reinforcement Learning</a></td>
    <td>Amin Karbasi, Nikki Lijing Kuang, Yi-An Ma, Siddharth Mitra</td>
    <td>International Conference on Machine Learning</td>
    <td>2023</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2305.18424">Repeated Random Sampling for Minimizing the Time-to-Accuracy of Learning</a></td>
    <td>Patrik Okanovic, Roger Waleffe, Vasilis Mageirakos, Konstantinos E. Nikolakakis, Amin Karbasi, Dionysis Kalogerias, Nezihe Merve G√ºrel, Theodoros Rekatsinas</td>
    <td>arXiv preprint arXiv:2305.18424</td>
    <td>2023</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2305.09557">Learning from Aggregated Data: Curated Bags versus Random Bags</a></td>
    <td>Lin Chen, Gang Fu, Amin Karbasi, Vahab Mirrokni</td>
    <td>arXiv preprint arXiv:2305.09557</td>
    <td>2023</td>
</tr>
<tr>
    <td><a href="https://www.biorxiv.org/content/10.1101/2022.07.19.500642v2.full.pdf">Cross Atlas Remapping via Optimal Transport (CAROT): Creating Connectomes for Different Atlases When Raw Data Is Not Available</a></td>
    <td>Javid Dadashkarimi, Amin Karbasi, Qinghao Liang, Matthew Rosenblatt, Stephanie Noble, Maya Foster, Raimundo Rodriguez, Brendan Adkinson, Jean Ye, Huili Sun, Chris Camp, Michael Farruggia, Link Tejavibulya, Wei Dai, Rongtao Jiang, Angeliki Pollatou, Dustin Scheinost</td>
    <td>Medical Image Analysis 88</td>
    <td>2023</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2305.02247">Select Without Fear: Almost All Mini-Batch Schedules Generalize Optimally</a></td>
    <td>Konstantinos E. Nikolakakis, Amin Karbasi, Dionysis Kalogerias</td>
    <td>arXiv preprint arXiv:2305.02247</td>
    <td>2023</td>
</tr>
<tr>
    <td><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9934563/">Stacking Multiple Optimal Transport Policies to Map Functional Connectomes</a></td>
    <td>Javid Dadashkarimi, Matthew Rosenblatt, Amin Karbasi, Dustin Scheinost</td>
    <td>2023 57th Annual Conference on Information Sciences and Systems (CISS)</td>
    <td>2023</td>
</tr>
<tr>
  <td><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/b8362385b08348d21162310c5b4e9541-Abstract-Conference.html">Universal Rates for Active Learning</a></td>
  <td>Steve Hanneke, Amin Karbasi, Shay Moran, Grigoris Velegkas</td>
  <td>Neural Information Processing Systems</td>
  <td>2022</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2211.14103">Conditional Gradient Methods</a></td>
    <td>G√°bor Braun, Alejandro Carderera, Cyrille W. Combettes, Hamed Hassani, Amin Karbasi, Aryan Mokhtari, Sebastian Pokutta</td>
    <td>arXiv preprint arXiv:2211.14103</td>
    <td>2022</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2204.12446">Beyond Lipschitz: Sharp Generalization and Excess Risk Bounds for Full-Batch GD</a></td>
    <td>Konstantinos E. Nikolakakis, Farzin Haddadpour, Amin Karbasi, Dionysios S. Kalogerias</td>
    <td>arXiv preprint arXiv:2204.12446</td>
    <td>2022</td>
</tr>

<tr>
    <td><a href="https://arxiv.org/pdf/2210.01898">Replicable Bandits</a></td>
    <td>Hossein Esfandiari, Alkis Kalavasis, Amin Karbasi, Andreas Krause, Vahab Mirrokni, Grigoris Velegkas</td>
    <td>arXiv preprint arXiv:2210.01898</td>
    <td>2022</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2006.09327">Submodular Maximization in Clean Linear Time</a></td>
    <td>Wenxin Li, Moran Feldman, Ehsan Kazemi, Amin Karbasi</td>
    <td>Advances in Neural Information Processing Systems 35</td>
    <td>2022</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2209.04121">Fast Neural Kernel Embeddings for General Activations</a></td>
    <td>Insu Han, Amir Zandieh, Jaehoon Lee, Roman Novak, Lechao Xiao, Amin Karbasi</td>
    <td>Advances in Neural Information Processing Systems 35</td>
    <td>2022</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2210.02297">Multiclass Learnability Beyond the PAC Framework: Universal Rates and Partial Concept Classes</a></td>
    <td>Alkis Kalavasis, Grigoris Velegkas, Amin Karbasi</td>
    <td>Advances in Neural Information Processing Systems 35</td>
    <td>2022</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2206.00860">Self-Consistency of the Fokker Planck Equation</a></td>
    <td>Zebang Shen, Zhenfu Wang, Satyen Kale, Alejandro Ribeiro, Amin Karbasi, Hamed Hassani</td>
    <td>Conference on Learning Theory</td>
    <td>2022</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2202.08833">What Functions Can Graph Neural Networks Generate?</a></td>
    <td>Mohammad Fereydounian, Hamed Hassani, Amin Karbasi</td>
    <td>arXiv preprint arXiv:2202.08833</td>
    <td>2022</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2103.06972">Federated Functional Gradient Boosting</a></td>
    <td>Zebang Shen, Hamed Hassani, Satyen Kale, Amin Karbasi</td>
    <td>International Conference on Artificial Intelligence and Statistics</td>
    <td>2022</td>
</tr>
<tr>
    <td><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/b8362385b08348d21162310c5b4e9541-Paper-Conference.pdf">Universal Rates for Interactive Learning</a></td>
    <td>Steve Hanneke, Shay Moran, Qian Zhang</td>
    <td>Advances in Neural Information Processing Systems 35</td>
    <td>2022</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2210.02713">On Optimal Learning Under Targeted Data Poisoning</a></td>
    <td>Steve Hanneke, Amin Karbasi, Mohammad Mahmoody, Idan Mehalel, Shay Moran</td>
    <td>Advances in Neural Information Processing Systems 35</td>
    <td>2022</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2104.02772">The Power of Subsampling in Submodular Maximization</a></td>
    <td>Christopher Harshaw, Ehsan Kazemi, Moran Feldman, Amin Karbasi</td>
    <td>Mathematics of Operations Research 47 (2)</td>
    <td>2022</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2203.09607">Learning Distributionally Robust Models at Scale via Composite Optimization</a></td>
    <td>Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, Amin Karbasi</td>
    <td>arXiv preprint arXiv:2203.09607</td>
    <td>2022</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2203.01491">Reinforcement Learning with Logarithmic Regret and Policy Switches</a></td>
    <td>Grigoris Velegkas, Zhuoran Yang, Amin Karbasi</td>
    <td>Advances in Neural Information Processing Systems 35</td>
    <td>2022</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2207.00486">Scalable MCMC Sampling for Nonsymmetric Determinantal Point Processes</a></td>
    <td>Insu Han, Mike Gartrell, Elvis Dohmatob, Amin Karbasi</td>
    <td>International Conference on Machine Learning</td>
    <td>2022</td>
</tr>
<tr>
    <td><a href="https://arxiv.org/pdf/2201.08417">Scalable Sampling for Nonsymmetric Determinantal Point Processes</a></td>
    <td>Insu Han, Mike Gartrell, Jennifer Gillenwater, Elvis Dohmatob, Amin Karbasi</td>
    <td>arXiv preprint arXiv:2201.08417</td>
    <td>2022</td>
</tr>
<tr>
    <td><a href="https://link.springer.com/chapter/10.1007/978-3-031-16431-6_37">Combining Multiple Atlases to Estimate Data-Driven Mappings Between Functional Connectomes Using Optimal Transport</a></td>
    <td>Javid Dadashkarimi, Amin Karbasi, Dustin Scheinost</td>
    <td>International Conference on Medical Image Computing and Computer-Assisted ‚Ä¶</td>
    <td>2022</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2210.15415">Exact Gradient Computation for Spiking Neural Networks Through Forward Propagation</a></td>
  <td>Jane H. Lee, Saeid Haghighatshoar, Amin Karbasi</td>
  <td>arXiv preprint</td>
  <td>2022</td>
</tr>
<tr>
  <td><a href="https://link.springer.com/chapter/10.1007/978-3-031-21083-9_12">Transforming Connectomes to ‚ÄúAny‚Äù Parcellation via Graph Matching</a></td>
  <td>Qinghao Liang, Javid Dadashkarimi, Wei Dai, Amin Karbasi, Joseph Chang, Harrison H. Zhou & Dustin Scheinost</td>
  <td>MICCAI Workshop</td>
  <td>2022</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2002.11080">The curious case of adversarially robust models: More data can help, double descend, or hurt generalization</a></td>
  <td>Yifei Min, Lin Chen, Amin Karbasi</td>
  <td>UAI</td>
  <td>2021</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1910.04959">Regret bounds for batched bandits</a></td>
  <td>Hossein Esfandiari, Amin Karbasi, Abbas Mehrabian, Vahab Mirrokni</td>
  <td>AAAI</td>
  <td>2021</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2008.01036">Multiple descent: Design your own generalization curve</a></td>
  <td>Lin Chen, Yifei Min, Mikhail Belkin, Amin Karbasi</td>
  <td>NeurIPS</td>
  <td>2021</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2002.03503">Regularized submodular maximization at scale</a></td>
  <td>Ehsan Kazemi, Shervin Minaee, Moran Feldman, Amin Karbasi</td>
  <td>ICML</td>
  <td>2021</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1911.03620">Adaptivity in adaptive submodularity</a></td>
  <td>Hossein Esfandiari, Amin Karbasi, Vahab Mirrokni</td>
  <td>COLT</td>
  <td>2021</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2106.01420">Parallelizing Thompson sampling</a></td>
  <td>Amin Karbasi, Vahab Mirrokni, Mohammad Shadravan</td>
  <td>NeurIPS</td>
  <td>2021</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2106.07724">An exponential improvement on the memorization capacity of deep threshold networks</a></td>
  <td>Shashank Rajput, Kartik Sreenivasan, Dimitris Papailiopoulos, Amin Karbasi</td>
  <td>NeurIPS</td>
  <td>2021</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/abs/2106.04769">Submodular+ concave</a></td>
  <td>Siddharth Mitra, Moran Feldman, Amin Karbasi</td>
  <td>NeurIPS</td>
  <td>2021</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2006.10921">Meta learning in the continuous time limit</a></td>
  <td>Ruitu Xu, Lin Chen, Amin Karbasi</td>
  <td>AISTATS</td>
  <td>2021</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2105.08709">Learning and certification under instance-targeted poisoning</a></td>
  <td>Ji Gao, Amin Karbasi, Mohammad Mahmoody</td>
  <td>UAI</td>
  <td>2021</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2107.01303">Data-driven mapping between functional connectomes using optimal transport</a></td>
  <td>Javid Dadashkarimi, Amin Karbasi, Dustin Scheinost</td>
  <td>MICCAI</td>
  <td>2021</td>
</tr>
<tr>
  <td><a href="https://pdf.sciencedirectassets.com/...">There is no single functional atlas even for a single individual: Functional parcel definitions change with task</a></td>
  <td>Mehraveh Salehi, Abigail S. Greene, Amin Karbasi, Xilin Shen, Dustin Scheinost, R. Todd Constable</td>
  <td>NeuroImage</td>
  <td>2020</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1804.09554">Stochastic conditional gradient methods: From convex minimization to submodular maximization</a></td>
  <td>Aryan Mokhtari, Hamed Hassani, Amin Karbasi</td>
  <td>JMLR</td>
  <td>2020</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1910.04322">One sample stochastic frank-wolfe</a></td>
  <td>Mingrui Zhang, Zebang Shen, Aryan Mokhtari, Hamed Hassani, Amin Karbasi</td>
  <td>AISTATS</td>
  <td>2020</td>
</tr>
<tr>
  <td><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC7216521/">Individualized functional networks reconfigure with cognitive state</a></td>
  <td>Mehraveh Salehi, Amin Karbasi, Daniel S Barron, Dustin Scheinost, R Todd Constable</td>
  <td>NeuroImage</td>
  <td>2020</td>
</tr>
<tr>
  <td><a href="https://proceedings.mlr.press/v119/chen20q/chen20q.pdf">More data can expand the generalization gap between adversarially robust and standard models</a></td>
  <td>Lin Chen, Yifei Min, Mingrui Zhang, Amin Karbasi</td>
  <td>ICML</td>
  <td>2020</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2006.13326">Safe learning under uncertain objectives and constraints</a></td>
  <td>Mohammad Fereydounian, Zebang Shen, Aryan Mokhtari, Amin Karbasi, Hamed Hassani</td>
  <td>arXiv preprint</td>
  <td>2020</td>
</tr>
<tr>
  <td><a href="https://proceedings.neurips.cc/...">Continuous submodular maximization: Beyond dr-submodularity</a></td>
  <td>Moran Feldman, Amin Karbasi</td>
  <td>NeurIPS</td>
  <td>2020</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1802.08183">Projection-free online optimization with stochastic gradient: From convexity to submodularity</a></td>
  <td>Lin Chen, Christopher Harshaw, Hamed Hassani, Amin Karbasi</td>
  <td>ICML</td>
  <td>2018</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1711.01660">Conditional gradient method for stochastic submodular maximization: Closing the gap</a></td>
  <td>Aryan Mokhtari, Hamed Hassani, Amin Karbasi</td>
  <td>AISTATS</td>
  <td>2018</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1802.07098">Do less, get more: Streaming submodular maximization with subsampling</a></td>
  <td>Moran Feldman, Amin Karbasi, Ehsan Kazemi</td>
  <td>NeurIPS</td>
  <td>2018</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1806.02815">Data summarization at scale: A two-stage submodular approach</a></td>
  <td>Marko Mitrovic, Ehsan Kazemi, Morteza Zadimoghaddam, Amin Karbasi</td>
  <td>ICML</td>
  <td>2018</td>
</tr>
<tr>
  <td><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5905726/">An exemplar-based approach to individualized parcellation reveals the need for sex-specific functional networks</a></td>
  <td>Mehraveh Salehi, Amin Karbasi, Xilin Shen, Dustin Scheinost, R Todd Constable</td>
  <td>Neuroimage 170</td>
  <td>2018</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1707.04347">Weakly submodular maximization beyond cardinality constraints: Does randomization help greedy?</a></td>
  <td>Lin Chen, Moran Feldman, Amin Karbasi</td>
  <td>ICML</td>
  <td>2018</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1802.03825">Decentralized submodular maximization: Bridging discrete and continuous settings</a></td>
  <td>Aryan Mokhtari, Hamed Hassani, Amin Karbasi</td>
  <td>ICML</td>
  <td>2018</td>
</tr>

<tr>
  <td><a href="https://arxiv.org/pdf/1802.09110">Submodularity on hypergraphs: From sets to sequences</a></td>
  <td>Marko Mitrovic, Moran Feldman, Andreas Krause, Amin Karbasi</td>
  <td>AISTATS</td>
  <td>2018</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1802.06942">Comparison based learning from weak oracles</a></td>
  <td>Ehsan Kazemi, Lin Chen, Sanjoy Dasgupta, Amin Karbasi</td>
  <td>AISTATS</td>
  <td>2018</td>
</tr>
<tr>
  <td><a href="https://www.biorxiv.org/content/10.1101/372110v1.full.pdf">State-specific individualized functional networks form a predictive signature of brain state</a></td>
  <td>Mehraveh Salehi, Amin Karbasi, Daniel S. Barron, Dustin Scheinost, R. Todd Constable</td>
  <td>BioRxiv</td>
  <td>2018</td>
</tr>
<tr>
  <td><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5851696/pdf/10827_2018_Article_678.pdf">Learning neural connectivity from firing activity: efficient algorithms with provable guarantees on topology</a></td>
  <td>Amin Karbasi, Amir Hesam Salavati, Martin Vetterli</td>
  <td>Journal of Computational Neuroscience 44</td>
  <td>2018</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1708.03949">Gradient methods for submodular maximization</a></td>
  <td>Hamed Hassani, Mahdi Soltanolkotabi, Amin Karbasi</td>
  <td>NeurIPS</td>
  <td>2017</td>
</tr>

<tr>
  <td><a href="https://arxiv.org/pdf/1703.02647">Streaming weak submodularity: Interpreting neural networks on the fly</a></td>
  <td>Ethan R. Elenberg, Alexandros G. Dimakis, Moran Feldman, Amin Karbasi</td>
  <td>NeurIPS</td>
  <td>2017</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1704.01652">Greed is good: Near-optimal submodular maximization via greedy optimization</a></td>
  <td>Moran Feldman, Christopher Harshaw, Amin Karbasi</td>
  <td>COLT</td>
  <td>2017</td>
</tr>
<tr>
  <td><a href="https://proceedings.mlr.press/v70/mirzasoleiman17a/mirzasoleiman17a.pdf">Deletion-robust submodular maximization: Data summarization with ‚Äúthe right to be forgotten‚Äù</a></td>
  <td>Baharan Mirzasoleiman, Amin Karbasi, Andreas Krause</td>
  <td>ICML</td>
  <td>2017</td>
</tr>
<tr>
  <td><a href="https://proceedings.mlr.press/v70/mitrovic17a/mitrovic17a.pdf">Differentially private submodular maximization: Data summarization in disguise</a></td>
  <td>Marko Mitrovic, Mark Bun, Andreas Krause, Amin Karbasi</td>
  <td>ICML</td>
  <td>2017</td>
</tr>
<tr>
  <td><a href="https://proceedings.mlr.press/v70/stan17a/stan17a.pdf">Probabilistic submodular maximization in sub-linear time</a></td>
  <td>Serban Stan, Morteza Zadimoghaddam, Andreas Krause, Amin Karbasi</td>
  <td>ICML</td>
  <td>2017</td>
</tr>
<tr>
  <td><a href="https://papers.nips.cc/paper_files/paper/2017/file/f0935e4cd5920aa6c7c996a5ee53a70f-Paper.pdf">Interactive submodular bandit</a></td>
  <td>Lin Chen, Andreas Krause, Amin Karbasi</td>
  <td>NeurIPS</td>
  <td>2017</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1603.03515">Near-optimal active learning of halfspaces via query synthesis in the noisy setting</a></td>
  <td>Lin Chen, Hamed Hassani, Amin Karbasi</td>
  <td>AAAI</td>
  <td>2017</td>
</tr>
<tr>
  <td><a href="https://link.springer.com/chapter/10.1007/978-3-319-66182-7_55">A submodular approach to create individualized parcellations of the human brain</a></td>
  <td>Mehraveh Salehi, Amin Karbasi, Dustin Scheinost & R. Todd Constable</td>
  <td>MICCAI</td>
  <td>2017</td>
</tr>
<tr>
  <td><a href="https://proceedings.mlr.press/v70/mitrovic17a/mitrovic17a.pdf">Differentially Private Submodular Maximization: Data Summarization in Disguise (Full version)</a></td>
  <td>Marko Mitrovic, Mark Bun, Andreas Krause, Amin Karbasi</td>
  <td>Venue/Journal not available</td>
  <td>2017</td>
</tr>
<tr>
  <td><a href="https://proceedings.mlr.press/v48/mirzasoleiman16.pdf">Fast constrained submodular maximization: Personalized data summarization</a></td>
  <td>Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi</td>
  <td>ICML</td>
  <td>2016</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1411.0541">Distributed submodular maximization</a></td>
  <td>Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, Andreas Krause</td>
  <td>Journal of Machine Learning Research</td>
  <td>2016</td>
</tr>
<tr>
  <td><a href="https://papers.nips.cc/paper_files/paper/2016/file/052335232b11864986bb2fa20fa38748-Paper.pdf">Fast distributed submodular cover: Public-private data summarization</a></td>
  <td>Baharan Mirzasoleiman, Morteza Zadimoghaddam, Amin Karbasi</td>
  <td>NeurIPS</td>
  <td>2016</td>
</tr>
<tr>
  <td><a href="https://papers.nips.cc/paper_files/paper/2016/file/8c00dee24c9878fea090ed070b44f1ab-Paper.pdf">Estimating the size of a large network and its communities from a random sample</a></td>
  <td>Lin Chen, Amin Karbasi, Forrest W. Crawford</td>
  <td>NeurIPS</td>
  <td>2016</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1511.04137">Seeing the unseen network: Inferring hidden social ties from respondent-driven sampling</a></td>
  <td>Lin Chen, Forrest W. Crawford, Amin Karbasi</td>
  <td>AAAI</td>
  <td>2016</td>
</tr>
<tr>
  <td><a href="https://ieeexplore.ieee.org/document/7471765">Learning network structures from firing patterns</a></td>
  <td>Amin Karbasi, Amir Hesam Salavati, Martin Vetterli</td>
  <td>IEEE International Conference on Acoustics</td>
  <td>2016</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1603.08616">Submodular variational inference for network reconstruction</a></td>
  <td>Lin Chen, Forrest W Crawford, Amin Karbasi</td>
  <td>arXiv preprint</td>
  <td>2016</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1603.03515">Dimension coupling: Optimal active learning of halfspaces via query synthesis</a></td>
  <td>Lin Chen, Hamed Hassani, Amin Karbasi</td>
  <td>arXiv preprint</td>
  <td>2016</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1409.7938">Lazier Than Lazy Greedy</a></td>
  <td>Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrak, Andreas Krause</td>
  <td>arXiv preprint</td>
  <td>2015</td>
</tr>
<tr>
  <td><a href="https://proceedings.mlr.press/v40/Chen15b.pdf">Sequential information maximization: When is greedy near-optimal?</a></td>
  <td>Yuxin Chen, S. Hamed Hassani, Amin Karbasi, Andreas Krause</td>
  <td>COLT</td>
  <td>2015</td>
</tr>
<tr>
  <td><a href="https://papers.nips.cc/paper_files/paper/2015/hash/c1fea270c48e8079d8ddf7d06d26ab52-Abstract.html">Distributed submodular cover: Succinctly summarizing massive data</a></td>
  <td>Baharan Mirzasoleiman, Amin Karbasi, Ashwinkumar Badanidiyuru, Andreas Krause</td>
  <td>NeurIPS</td>
  <td>2015</td>
</tr>
<tr>
  <td><a href="https://cdn.aaai.org/ojs/9694/9694-13-13222-1-2-20201228.pdf">Submodular surrogates for value of information</a></td>
  <td>Yuxin Chen, Shervin Javdani, Amin Karbasi, J. Bagnell, Siddhartha Srinivasa, Andreas Krause</td>
  <td>AAAI</td>
  <td>2015</td>
</tr>
<tr>
  <td><a href="https://www.ijcai.org/Proceedings/15/Papers/283.pdf">Non-Monotone Adaptive Submodular Maximization</a></td>
  <td>Alkis Gotovos, Amin Karbasi, Andreas Krause</td>
  <td>IJCAI</td>
  <td>2015</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1506.02194">Fast mixing for discrete point processes</a></td>
  <td>Patrick Rebeschini, Amin Karbasi</td>
  <td>COLT</td>
  <td>2015</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1605.00529">Tradeoffs for Space, Time, Data and Risk in Unsupervised Learning</a></td>
  <td>Mario Lucic, Mesrob I. Ohannessian, Amin Karbasi, Andreas Krause</td>
  <td>Venue/Journal not available</td>
  <td>2015</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1107.3059">From Small-World Networks to Comparison-Based Search</a></td>
  <td>Amin Karbasi, Stratis Ioannidis, Laurent Massoulie</td>
  <td>Transactions on Information Theory</td>
  <td>2015</td>
</tr>
<tr>
  <td><a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=082d26e93656fa9fcd5910f7b9a9922caa64835b">Normalization Phenomena in Asynchronous Networks</a></td>
  <td>Amin Karbasi, Johannes Lengler, Angelika Steger</td>
  <td>Venue/Journal not available</td>
  <td>2015</td>
</tr>
<tr>
  <td><a href="https://infoscience.epfl.ch/server/api/core/bitstreams/37b51b59-3d07-4822-8413-c0eaf039ea47/content">Asynchronous decoding of LDPC codes over BEC</a></td>
  <td>Saeid Haghighatshoar, Amin Karbasi, Amir Hesam Salavati</td>
  <td>ISIT</td>
  <td>2015</td>
</tr>
<tr>
  <td><a href="https://www.cs.cmu.edu/~sjavdani/papers/rss_2014_workshop_abstract.pdf">Decision Region Determination for Touch Based Localization</a></td>
  <td>Shervin Javdani, Yuxin Chen, Amin Karbasi, Andreas Krause, J. Andrew Bagnell, Siddhartha Srinivasa</td>
  <td>arXiv</td>
  <td>2015</td>
</tr>
<tr>
  <td><a href="https://las.inf.ethz.ch/files/badanidiyuru14streaming.pdf">Streaming submodular maximization: Massive data summarization on the fly</a></td>
  <td>Badanidiyuru, Ashwinkumar, Baharan Mirzasoleiman, Amin Karbasi, Andreas Krause</td>
  <td>ACM SIGKDD</td>
  <td>2014</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1402.2092">Near-optimally teaching the crowd to classify</a></td>
  <td>Adish Singla, Ilija Bogunovic, G√°bor Bart√≥k, Amin Karbasi, Andreas Krause</td>
  <td>ICML</td>
  <td>2014</td>
</tr>
<tr>
  <td><a href="https://proceedings.mlr.press/v33/javdani14.pdf">Near optimal bayesian active learning for decision making</a></td>
  <td>Shervin Javdani, Yuxin Chen, Amin Karbasi, Andreas Krause, Drew Bagnell, Siddhartha Srinivasa</td>
  <td>AISTATS</td>
  <td>2014</td>
</tr>
<tr>
  <td><a href="https://patentimages.storage.googleapis.com/f6/2d/c7/3371b471ebdb9e/US8695009.pdf">Allocating tasks to machines in computing clusters</a></td>
  <td>M Vojnovic, A Karbasi</td>
  <td>US Patent</td>
  <td>2014</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1403.3305">Noise facilitation in associative memories of exponential capacity</a></td>
  <td>Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav R. Varshney</td>
  <td>Neural Computation</td>
  <td>2014</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1407.6513">Convolutional neural associative memories: Massive capacity with noise tolerance</a></td>
  <td>Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi</td>
  <td>arXiv</td>
  <td>2014</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1301.1555">Coupled Neural Associative Memories</a></td>
  <td>Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi</td>
  <td>arXiv</td>
  <td>2014</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1411.0541">Distributed submodular maximization: Identifying representative elements in massive data</a></td>
  <td>Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, Andreas Krause</td>
  <td>NeurIPS</td>
  <td>2013</td>
</tr>
<tr>
  <td><a href="https://proceedings.mlr.press/v28/karbasi13.html">Iterative learning and denoising in convolutional neural associative memories</a></td>
  <td>Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi</td>
  <td>ICML</td>
  <td>2013</td>
</tr>
<tr>
  <td><a href="https://proceedings.mlr.press/v32/singla14.pdf">On actively teaching the crowd to classify</a></td>
  <td>Adish Singla, Ilija Bogunovic, G√°bor Bart√≥k, Amin Karbasi, Andreas Krause</td>
  <td>NIPS Workshop on Data Driven Education</td>
  <td>2013</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1012.4928">Calibration using matrix completion with application to ultrasound tomography</a></td>
  <td>Reza Parhizkar, Amin Karbasi, Sewoong Oh, Martin Vetterli</td>
  <td>IEEE Transactions on Signal Processing</td>
  <td>2013</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1403.3305">Noise-enhanced associative memories</a></td>
  <td>Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav R. Varshney</td>
  <td>NeurIPS</td>
  <td>2013</td>
</tr>
<tr>
  <td><a href="https://drops.dagstuhl.de/storage/00lipics/lipics-vol020-stacs2013/LIPIcs.STACS.2013.550/LIPIcs.STACS.2013.550.pdf">Constrained Binary Identification Problem</a></td>
  <td>Amin Karbasi, Morteza Zadimoghaddam</td>
  <td>STACS</td>
  <td>2013</td>
</tr>

<tr>
  <td><a href="https://arxiv.org/pdf/1001.1445">Graph-constrained group testing</a></td>
  <td>Mahdi Cheraghchi, Amin Karbasi, Soheil Mohajer, Venkatesh Saligrama</td>
  <td>IEEE Transactions on Information Theory</td>
  <td>2012</td>
</tr>
<tr>
  <td><a href="files/amin-12d.pdf">Sequential group testing with graph constraints</a></td>
  <td>Amin Karbasi, Morteza Zadimoghaddam</td>
  <td>IEEE Information Theory Workshop</td>
  <td>2012</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1110.3018">Robust localization from incomplete local information</a></td>
  <td>Amin Karbasi, Sewoong Oh</td>
  <td>IEEE/ACM Transactions on Networking</td>
  <td>2012</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1206.4674">Comparison-based learning with rank nets</a></td>
  <td>Amin Karbasi, Stratis Ioannidis, Laurent Massoulie</td>
  <td>arXiv</td>
  <td>2012</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1202.2770">Multi-level error-resilient neural networks</a></td>
  <td>Amir Hesam Salavati, Amin Karbasi</td>
  <td>IEEE ISIT Proceedings</td>
  <td>2012</td>
</tr>
<tr>
  <td><a href="https://ieeexplore.ieee.org/document/6017118">Low-Rank Matrix Approximation Using Point-Wise Operators</a></td>
  <td>Arash Amini, Amin Karbasi, Farokh Marvasti</td>
  <td>IEEE Transactions on Information Theory</td>
  <td>2012</td>
</tr>
<tr>
  <td><a href="files/amin-12c.pdf">Hot or Not: Interactive Content Search Using Comparisons</a></td>
  <td>Amin Karbasi, Stratis Ioannidis, Laurent Massouli√©</td>
  <td>Information Theory and Applications</td>
  <td>2012</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1202.2770">Multi-Level Error-Resilient Neural Networks with Learning</a></td>
  <td>Amir Hesam Salavati, Amin Karbasi</td>
  <td>arXiv</td>
  <td>2012</td>
</tr>
<tr>
  <td><a href="files/amin_thesis.pdf">Graph-Based Information Processing: Scaling Laws and Applications</a></td>
  <td>Amin Karbasi, Martin Vetterli, R√ºdiger Urbanke</td>
  <td>EPFL</td>
  <td>2012</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/1009.3186">Group testing with probabilistic tests: Theory, design, and application</a></td>
  <td>Mahdi Cheraghchi, Ali Hormati, Amin Karbasi, Martin Vetterli</td>
  <td>IEEE Transactions on Information Theory</td>
  <td>2011</td>
</tr>
<tr>
  <td><a href="https://link.springer.com/chapter/10.1007/978-3-642-22012-8_48">Content search through comparisons</a></td>
  <td>Amin Karbasi, Stratis Ioannidis, Laurent Massouli√©</td>
  <td>International Colloquium on Automata</td>
  <td>2011</td>
</tr>
<tr>
  <td><a href="files/amin-11c.pdf">Calibration in circular ultrasound tomography devices</a></td>
  <td>Reza Parhizkar, Amin Karbasi, Martin Vetterli</td>
  <td>IEEE ICASSP</td>
  <td>2011</td>
</tr>
<tr>
  <td><a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=a279445a4e0603b8b2768c1f1439e998c1072080">Adaptive content search through comparisons</a></td>
  <td>Amin Karbasi, Stratis Ioannidis, Laurent Massouli√©</td>
  <td>MIT Press</td>
  <td>2011</td>
</tr>
<tr>
  <td><a href="files/amin-11b.pdf">Compression with graphical constraints: An interactive browser</a></td>
  <td>Amin Karbasi, Morteza Zadimoghaddam</td>
  <td>IEEE ISIT Proceedings</td>
  <td>2011</td>
</tr>
<tr>
  <td><a href="https://infoscience.epfl.ch/entities/patent/8ce181cf-6797-41f3-9134-21970f143428">Greedy Scheduling for Distributed Computing Clusters</a></td>
  <td>Karbasi, Amin and Vojnovic, Milan</td>
  <td>Not Available</td>
  <td>2011</td>
</tr>
<tr>
  <td><a href="https://swoh.web.engr.illinois.edu/paper_positioning_central.pdf">Sensor network localization from local connectivity: Performance analysis for the mds-map algorithm</a></td>
  <td>Sewoong Oh, Andrea Montanari, Amin Karbasi</td>
  <td>2010 IEEE Information Theory Workshop</td>
  <td>2010</td>
</tr>
<tr>
  <td><a href="https://homes.cs.washington.edu/~sewoong/paper_positioning_distributed.pdf">Distributed sensor network localization from local connectivity: performance analysis for the hop-terrain algorithm</a></td>
  <td>Amin Karbasi, Sewoong Oh</td>
  <td>ACM SIGMETRICS International Conference</td>
  <td>2010</td>
</tr>
<tr>
  <td><a href="https://www.acoustics.asn.au/conference_proceedings/ICA2010/cdrom-ICA2010/papers/p539.pdf">Ultrasound tomography calibration using structured matrix completion</a></td>
  <td>Amin Karbasi, Sewoong Oh, Reza Parhizkar, Martin Vetterli</td>
  <td>20th International Congress on Acoustics</td>
  <td>2010</td>
</tr>
<tr>
  <td><a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=4bd4f520a6375f2702f2b595c9dc5f742bac5334">From centralized to distributed sensor localization</a></td>
  <td>Amin Karbasi</td>
  <td>ACM Workshop on Wireless of the Students</td>
  <td>2010</td>
</tr>
<tr>
  <td><a href="https://infoscience.epfl.ch/server/api/core/bitstreams/0d790e8c-e7a8-4501-a723-ab51552e99fc/content">Compressed sensing with probabilistic measurements: A group testing solution</a></td>
  <td>Mahdi Cheraghchi, Ali Hormati, Amin Karbasi, Martin Vetterli</td>
  <td>47th Annual Allerton Conference</td>
  <td>2009</td>
</tr>
<tr>
  <td><a href="https://ieeexplore.ieee.org/abstract/document/5206485">Support recovery in compressed sensing: An estimation theoretic approach</a></td>
  <td>Amin Karbasi, Ali Hormati, Soheil Mohajer, Martin Vetterli</td>
  <td>IEEE International Symposium on Information Theory</td>
  <td>2009</td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/0911.4880">An estimation theoretic approach for sparsity pattern recovery in the noisy setting</a></td>
  <td>Ali Hormati, Amin Karbasi, Soheil Mohajer, Martin Vetterli</td>
  <td>arXiv</td>
  <td>2009</td>
</tr>
<tr>
  <td><a href="https://www.eurasip.org/Proceedings/Eusipco/Eusipco2007/Papers/b1l-d05.pdf">A new DOA estimation method using a circular microphone array</a></td>
  <td>Amin Karbasi, Akihiko Sugiyama</td>
  <td>15th European Signal Processing Conference</td>
  <td>2007</td>
</tr>
<tr>
  <td><a href="https://www.eurasip.org/Proceedings/Eusipco/Eusipco2006/papers/1568981824.pdf">A DOA estimation method for an arbitrary triangular microphone arrangement</a></td>
  <td>Amin Karbasi, Akihiko Sugiyama</td>
  <td>14th European Signal Processing Conference</td>
  <td>2006</td>
</tr>



</tbody>
</table>

<hr>


</section>
<script src="js/birds.js"></script>
</body>
<script>
  'use strict';

  // Load the pygments stylesheet so that it's not render blocking.
  var head = document.head;
  var link = document.createElement('link');
  link.type = 'text/css';
  link.rel = 'stylesheet';
  link.href = '/css/pygments.css'
  head.appendChild(link);
</script>
</html>
